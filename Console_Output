Starting Training
layer weights:  Parameter containing:
tensor([[ 0.7987],
        [-0.4320],
        [-0.4384]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[0.6816],
        [0.8796],
        [0.1622]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(-0.0724, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.0148, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.0190, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.0940, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.0479, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.0579, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2604, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.0810, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.0968, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.4269, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.1141, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.1357, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 0 		  Q(s,a)=  tensor(0.5933, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 1 		  Q(s,a)=  tensor(0.1473, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 2 		  Q(s,a)=  tensor(0.1746, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 0 		  Q(s,a)=  tensor(0.7597, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 1 		  Q(s,a)=  tensor(0.1804, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 2 		  Q(s,a)=  tensor(0.2135, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[0.1664],
        [0.0331],
        [0.0389]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([-0.0724,  0.0148,  0.0190], dtype=torch.float64, requires_grad=True)
