Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1352, -0.0254,  0.1564,  0.1664, -0.0084,  0.0898, -0.0792, -0.1782,
         -0.1600, -0.1773, -0.1915,  0.1456, -0.0360, -0.1041, -0.0595,  0.1686,
          0.1807,  0.1572, -0.1624,  0.0813,  0.1097, -0.0551, -0.0911,  0.1930,
          0.0582],
        [-0.1695,  0.0146,  0.1635,  0.0555, -0.1384, -0.1417, -0.0556, -0.1828,
          0.0313,  0.0723, -0.1324,  0.1617,  0.0922, -0.0373,  0.0364, -0.1630,
         -0.0398,  0.1988, -0.1184, -0.0668, -0.1061, -0.0906, -0.1809, -0.0895,
          0.1868],
        [-0.1904,  0.0631,  0.1485,  0.0070, -0.0749, -0.1969, -0.1967, -0.0497,
          0.1708,  0.0773,  0.1341, -0.0988, -0.1448, -0.0853,  0.0746,  0.1377,
         -0.1000, -0.0453, -0.1785, -0.1754,  0.1122, -0.1047, -0.1142, -0.0995,
          0.1872],
        [ 0.0953,  0.0202,  0.0849,  0.0328,  0.1211, -0.1258, -0.0452,  0.0177,
         -0.1659, -0.0665, -0.1497, -0.1627,  0.1481,  0.0555, -0.0451,  0.0071,
         -0.0910,  0.0871, -0.1501, -0.0043, -0.1651,  0.0302,  0.1617,  0.1020,
         -0.0635],
        [-0.1722,  0.0436,  0.0665,  0.1258, -0.0401,  0.1680, -0.1362, -0.1073,
          0.1481, -0.1713,  0.1932,  0.0927, -0.1305, -0.1030,  0.0497, -0.1775,
          0.1982,  0.1777, -0.0699, -0.1276,  0.0058, -0.1012, -0.1543,  0.0838,
          0.0579]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1164,  0.0613, -0.1033,  0.0992, -0.0208, -0.0763,  0.1082,  0.0213,
         -0.0476, -0.1986, -0.1504, -0.1345,  0.0801,  0.1459,  0.1568, -0.1431,
         -0.0500, -0.1404, -0.1538, -0.1946, -0.0236, -0.1765,  0.1044,  0.0738,
         -0.1552],
        [-0.1351, -0.0062, -0.1160,  0.0718, -0.1117,  0.1092,  0.0564, -0.0728,
         -0.1947, -0.1271, -0.0713, -0.0379,  0.1486,  0.0895,  0.0282, -0.1847,
         -0.0115, -0.0250,  0.1682, -0.1812,  0.1907,  0.1858,  0.1406,  0.0498,
          0.0701],
        [ 0.1696,  0.1677,  0.1366,  0.0290,  0.0913, -0.1407, -0.0974, -0.0445,
          0.0221, -0.0441,  0.0981,  0.1255,  0.1430, -0.0601, -0.0587,  0.1698,
          0.0802,  0.1008,  0.0733,  0.0217,  0.0042, -0.0681,  0.0069,  0.1644,
          0.1113],
        [ 0.1514,  0.0911, -0.0926,  0.0247,  0.1464, -0.1686,  0.0948,  0.1227,
          0.0296,  0.0950,  0.1277, -0.0855,  0.0474,  0.1962, -0.0170, -0.0619,
         -0.1261, -0.0403, -0.1925,  0.0165, -0.1381, -0.1704,  0.1413,  0.0873,
         -0.0299],
        [-0.1305,  0.0683, -0.1656,  0.0535,  0.1139, -0.0136,  0.0324,  0.0311,
         -0.0990,  0.1299,  0.0056,  0.1827,  0.0328, -0.1580, -0.1613, -0.0509,
          0.0609, -0.1296,  0.0673, -0.1462, -0.0171, -0.0203, -0.0766, -0.0146,
          0.1278]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(2.5954, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0004, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(3.4959, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7371, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3670, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(3.3081, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6341, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(2.0382, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.2514, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.6482, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(3.1988, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(3.6390, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.7259, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(3.3288, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(2.3196, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(4.1301, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(3.8401, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.9218, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(3.7749, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.6856, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.08596078431372549  Action:  2  Reward Received:  0
State:  0.8817254901960784  Action:  1  Reward Received:  2.0
State:  0.5022745098039215  Action:  0  Reward Received:  0.1
State:  0.08596078431372549  Action:  2  Reward Received:  0
State:  0.8817254901960784  Action:  1  Reward Received:  2.0

Total Reward Received:  4.1
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1859,  0.0572,  0.0562, -0.1691,  0.0945, -0.0809, -0.1452, -0.1566,
          0.1519, -0.0452,  0.0035, -0.0095,  0.1113,  0.0868, -0.1781, -0.0540,
          0.1620,  0.0550,  0.1637,  0.0467,  0.0141,  0.1596, -0.0905, -0.1537,
          0.1701],
        [-0.0760, -0.1016,  0.0613, -0.0399,  0.0323,  0.0923,  0.0679,  0.0251,
         -0.0194,  0.1130,  0.0989, -0.0410,  0.0546,  0.0303, -0.1726,  0.0160,
          0.1182, -0.0732, -0.0293,  0.1033,  0.0502, -0.1349, -0.1584,  0.0912,
         -0.0052],
        [-0.1803, -0.1479,  0.1627,  0.0661,  0.1568,  0.0275,  0.0393,  0.1263,
          0.1682, -0.1787,  0.0321, -0.1460,  0.1657, -0.1617,  0.1323,  0.0235,
          0.0277,  0.1821, -0.1147,  0.0524,  0.0085, -0.0372, -0.1872, -0.1423,
         -0.1349],
        [-0.0210,  0.1956, -0.1838, -0.0698,  0.0844, -0.1686,  0.1555, -0.1685,
          0.0909,  0.0823,  0.1271, -0.1122, -0.1200, -0.0422, -0.0295,  0.1038,
         -0.1505, -0.0156,  0.0284, -0.0512,  0.0676,  0.1826,  0.1440, -0.0875,
         -0.0929],
        [ 0.1818,  0.0645, -0.1592, -0.0733, -0.0374, -0.1077, -0.1264, -0.0162,
         -0.0906,  0.0004,  0.1923,  0.1327, -0.0249,  0.0720,  0.1706, -0.1404,
         -0.1548,  0.0630,  0.0306, -0.0974, -0.1174, -0.1468, -0.1979,  0.0479,
         -0.0271]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1668, -0.1421, -0.0163, -0.1794,  0.0990, -0.0913,  0.0761, -0.0887,
          0.1097, -0.0873, -0.1512,  0.1250, -0.1093,  0.0925, -0.1520, -0.0097,
         -0.0957, -0.1082, -0.0117, -0.1219,  0.1932,  0.1077,  0.0245, -0.0664,
          0.1482],
        [ 0.0242, -0.1673,  0.1722, -0.0157,  0.1615, -0.1510,  0.1486, -0.1975,
          0.1927,  0.0198,  0.0094, -0.0668, -0.1436, -0.0875, -0.1266,  0.0885,
          0.1033,  0.0864,  0.0517, -0.1891, -0.0172, -0.1457,  0.1303, -0.1875,
         -0.1518],
        [ 0.1724,  0.1733, -0.1598, -0.0273,  0.0598, -0.1434, -0.0932, -0.1180,
          0.0132, -0.1697,  0.0058,  0.0562,  0.0520,  0.1456,  0.1507, -0.1054,
          0.0337,  0.1187, -0.1329, -0.1687, -0.0337, -0.0125, -0.1626, -0.0120,
          0.1107],
        [-0.0485, -0.1067,  0.0780, -0.0173,  0.0815,  0.1489,  0.0172,  0.0671,
         -0.0852, -0.0359, -0.1645,  0.0376,  0.0424, -0.1363,  0.0435, -0.0078,
          0.1528,  0.0678, -0.0498, -0.0043,  0.1150,  0.0206,  0.0607, -0.0921,
          0.0388],
        [ 0.1811, -0.1541, -0.1790, -0.1830, -0.1275,  0.1813,  0.0147, -0.1342,
         -0.0744,  0.0492,  0.0138, -0.1778, -0.1792, -0.1894, -0.0011, -0.0188,
         -0.1268,  0.0890,  0.1736,  0.0839, -0.1210,  0.1120,  0.1424, -0.1627,
         -0.0232]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(2.6240, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.7796, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(3.5065, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6438, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5669, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(3.2871, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6648, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(3.1294, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.0383, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.7259, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(3.1065, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(3.5669, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(3.4843, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(3.2240, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(2.2599, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(4.1573, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(3.7252, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(3.5471, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.8144, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.4400, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.0923921568627451  Action:  2  Reward Received:  0
State:  0.8933333333333333  Action:  1  Reward Received:  2.0
State:  0.4988235294117647  Action:  0  Reward Received:  0.1
State:  0.0923921568627451  Action:  2  Reward Received:  0
State:  0.8933333333333333  Action:  1  Reward Received:  2.0

Total Reward Received:  4.1
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1533, -0.1146,  0.0328, -0.0669, -0.1242, -0.1831, -0.1042, -0.1320,
         -0.0171,  0.0939,  0.1374, -0.0075,  0.1321,  0.0759,  0.0229,  0.0446,
          0.1204,  0.0863, -0.1465,  0.0010, -0.1830,  0.0922, -0.1889, -0.0416,
         -0.0863],
        [ 0.0048,  0.0413, -0.1226, -0.0251,  0.1317,  0.0293,  0.0386, -0.1482,
          0.0736, -0.0350,  0.0739,  0.0845,  0.0546,  0.0295,  0.1045, -0.1105,
         -0.0268, -0.0827,  0.0330, -0.1994, -0.1525,  0.0554, -0.1921,  0.1334,
          0.1533],
        [-0.1332, -0.1930, -0.1832,  0.1297, -0.0066, -0.1195,  0.1445, -0.1110,
          0.1620, -0.0780, -0.0715,  0.1697,  0.1714, -0.1457,  0.0477, -0.0772,
         -0.0740,  0.1522, -0.0784,  0.1476,  0.1575, -0.1617,  0.1432, -0.0452,
          0.0235],
        [-0.0792,  0.0652,  0.1867,  0.1754,  0.1301, -0.0936, -0.0321,  0.0715,
          0.0108,  0.0272, -0.1941, -0.0848, -0.1820, -0.0950, -0.1442,  0.0567,
          0.0963, -0.1145, -0.1835,  0.1819, -0.1365,  0.0970,  0.0855, -0.0891,
         -0.0486],
        [ 0.1717,  0.1053,  0.0358, -0.0675,  0.0686, -0.1983, -0.0972,  0.0987,
         -0.0024, -0.1376, -0.0470,  0.1706,  0.0035,  0.0418,  0.1523,  0.1756,
          0.1187, -0.1095,  0.1719, -0.0630, -0.0836, -0.0837, -0.0120, -0.1239,
         -0.0396]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1638,  0.0308,  0.0135, -0.1343,  0.1856,  0.0479,  0.0202,  0.1442,
          0.1404, -0.1883,  0.0962,  0.0428, -0.1189, -0.1012,  0.0514, -0.1497,
         -0.1889,  0.0842, -0.1870, -0.0722,  0.1440, -0.1387, -0.1015, -0.1426,
         -0.1362],
        [ 0.0317,  0.0669,  0.1914, -0.0897,  0.0949, -0.0641,  0.1933, -0.0229,
          0.1798,  0.0846, -0.0401, -0.0306,  0.0508,  0.1301, -0.0975, -0.1552,
         -0.0525, -0.1361, -0.1412,  0.0515,  0.1008, -0.1989, -0.0006, -0.0583,
         -0.0312],
        [-0.0844, -0.1028, -0.0704,  0.1113, -0.0364,  0.0702,  0.1541,  0.1673,
         -0.1767, -0.1640,  0.1311,  0.0157, -0.1534,  0.0549,  0.1037,  0.1371,
         -0.1665, -0.1247, -0.0141,  0.0962, -0.0629, -0.1944, -0.1452,  0.1324,
          0.1241],
        [ 0.1544,  0.1655,  0.0712, -0.1336,  0.1006,  0.1598,  0.1212,  0.0265,
         -0.0729,  0.1881,  0.1324, -0.1794, -0.1514,  0.0020,  0.0060, -0.1549,
          0.0537,  0.0045, -0.0167,  0.0163,  0.1668,  0.0579,  0.1660,  0.1323,
          0.0196],
        [-0.0498, -0.1725,  0.0546, -0.0784,  0.1574,  0.0296, -0.0899, -0.0452,
         -0.0127, -0.0179,  0.1451, -0.0131, -0.1890,  0.1240,  0.0313, -0.1295,
         -0.1904, -0.0314,  0.1062,  0.0651, -0.0114, -0.0952, -0.0265, -0.1521,
          0.1171]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(3.0088, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.9615, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(3.3593, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6922, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.7279, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(3.3146, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.9645, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.8167, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.8524, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3844, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(2.9589, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(3.7827, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.9372, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.4078, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.4576, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(3.3029, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(4.5023, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.0392, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.2464, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.3037, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.09411764705882353  Action:  2  Reward Received:  0
State:  0.8843921568627451  Action:  1  Reward Received:  2.0
State:  0.4741960784313725  Action:  0  Reward Received:  0.1
State:  0.09411764705882353  Action:  2  Reward Received:  0
State:  0.8843921568627451  Action:  1  Reward Received:  2.0

Total Reward Received:  4.1
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1711, -0.1632, -0.0374, -0.1177,  0.0338,  0.1667,  0.1287,  0.0309,
         -0.1115, -0.0925,  0.0301, -0.1425, -0.1702, -0.0207, -0.0072,  0.1804,
         -0.0495, -0.1762,  0.1446, -0.0675, -0.0506,  0.0925, -0.0311, -0.1958,
          0.0842],
        [-0.0834, -0.1486,  0.1951,  0.0717,  0.1482,  0.0799,  0.0260,  0.1692,
         -0.0386, -0.1397, -0.1340,  0.0991,  0.0254, -0.0568, -0.0163, -0.0569,
          0.1749, -0.0907,  0.1717,  0.0219, -0.1404,  0.0101, -0.0017, -0.0216,
          0.0720],
        [-0.1102,  0.0120,  0.0796,  0.1348,  0.1210, -0.0432,  0.1140, -0.1647,
          0.0084, -0.1310,  0.1501, -0.1130, -0.1122, -0.0979,  0.1011,  0.0940,
          0.0575, -0.0567,  0.0190,  0.1449, -0.0370,  0.1103,  0.0188, -0.1970,
         -0.0100],
        [-0.0124, -0.1601, -0.0908, -0.1759,  0.1388,  0.1596, -0.1167, -0.1623,
          0.0358,  0.1263, -0.0126, -0.0199, -0.1629,  0.0179,  0.1448,  0.1477,
          0.1568, -0.0338, -0.0161, -0.0754,  0.1521,  0.0342,  0.1072,  0.1827,
         -0.0969],
        [-0.0427,  0.1270, -0.0961,  0.0715,  0.1713, -0.0626, -0.1008, -0.0541,
          0.0244, -0.0841,  0.0837, -0.1418,  0.0653, -0.0026,  0.0765,  0.0033,
         -0.0950, -0.1197,  0.0240,  0.0930,  0.0458,  0.0682,  0.0732,  0.1721,
          0.0148]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0827, -0.1587,  0.0476, -0.0136,  0.0190, -0.0982,  0.1390,  0.0082,
         -0.0072, -0.0721,  0.0444, -0.1123,  0.1085, -0.0236,  0.0357,  0.1216,
         -0.0611,  0.1904, -0.0929,  0.1612, -0.0761, -0.1579, -0.1499, -0.0168,
         -0.1355],
        [-0.0150,  0.0825,  0.0810,  0.1543,  0.0266, -0.0630,  0.0445,  0.1373,
         -0.1024, -0.1713,  0.0938,  0.1566, -0.1147, -0.1996,  0.0207,  0.1509,
          0.0868, -0.0539, -0.0036, -0.1351,  0.0710, -0.0947, -0.1347,  0.0093,
         -0.0032],
        [ 0.1644, -0.1437,  0.0109,  0.0242,  0.0855,  0.0736, -0.0064,  0.1704,
         -0.0455, -0.1059,  0.0915,  0.1984,  0.1556,  0.1327,  0.1372,  0.1773,
         -0.1625,  0.0601, -0.1504,  0.1130, -0.1688, -0.0491, -0.0450, -0.1912,
          0.1066],
        [ 0.0736,  0.0689,  0.0229, -0.0059,  0.1145, -0.0105, -0.1668,  0.0859,
          0.0907,  0.1871,  0.0804, -0.0170,  0.0503, -0.0328,  0.1703,  0.0455,
         -0.1283,  0.0689, -0.0870,  0.0201, -0.1149, -0.0712,  0.1051,  0.1480,
         -0.1416],
        [-0.0282,  0.1521, -0.0270, -0.1753,  0.1187,  0.0765,  0.0202, -0.1035,
         -0.1482,  0.0048,  0.1584, -0.1054,  0.0677,  0.0274, -0.1219, -0.0867,
          0.1286,  0.1680, -0.0113,  0.1151,  0.0410,  0.0971,  0.0676,  0.1564,
         -0.0743]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(2.7184, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1232, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(3.3910, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5126, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3610, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(3.0549, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.0600, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(2.3296, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.0143, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.7573, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(3.0331, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(3.5610, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.9153, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.7384, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3527, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(3.5711, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(3.7028, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.4372, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(3.1181, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(-0.0114, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.06619607843137255  Action:  2  Reward Received:  0
State:  0.879686274509804  Action:  1  Reward Received:  2.0
State:  0.48752941176470593  Action:  0  Reward Received:  0.1
State:  0.06619607843137255  Action:  2  Reward Received:  0
State:  0.879686274509804  Action:  1  Reward Received:  2.0

Total Reward Received:  4.1
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0031,  0.0572, -0.1809, -0.1608, -0.0354,  0.0132, -0.1957,  0.0967,
          0.0660, -0.1835, -0.1189, -0.0283,  0.1287,  0.1948, -0.1115,  0.1850,
         -0.0277,  0.1442,  0.0908, -0.0127, -0.0900, -0.1113,  0.1644, -0.0046,
         -0.0279],
        [-0.1258, -0.1623, -0.1168, -0.1325,  0.0070, -0.0759,  0.1081,  0.0993,
          0.0127,  0.0784,  0.1009,  0.0808, -0.1128, -0.1012,  0.0580,  0.1145,
          0.0648,  0.0827,  0.0478, -0.0137, -0.0617,  0.1253,  0.1884, -0.0875,
          0.0817],
        [-0.1479, -0.1077,  0.1329,  0.0568,  0.1118, -0.1927,  0.1767, -0.0484,
         -0.1971, -0.0607, -0.1019,  0.1558, -0.0554, -0.0824,  0.0960, -0.0108,
          0.0856,  0.0310,  0.0101,  0.1982, -0.0028, -0.0294,  0.0750,  0.1699,
         -0.1958],
        [-0.0404, -0.1892,  0.1029,  0.0012, -0.0338, -0.0105, -0.1856,  0.1950,
          0.0060, -0.1386, -0.0483, -0.1806,  0.0323,  0.1798,  0.0812,  0.1610,
         -0.0376, -0.1143, -0.0384, -0.1093, -0.1570, -0.0925,  0.0976,  0.0887,
          0.1726],
        [-0.1498, -0.0963,  0.0667, -0.1974,  0.1282,  0.1563, -0.0469, -0.1610,
          0.1703, -0.1979,  0.0625,  0.1796, -0.1208,  0.1608,  0.0652, -0.1688,
         -0.0674,  0.0745,  0.1729, -0.1624, -0.1076, -0.0315,  0.0461,  0.0955,
          0.1315]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1697,  0.0533, -0.0675,  0.1496, -0.1763,  0.0822,  0.0318, -0.1834,
          0.1353, -0.0654, -0.1571, -0.1030, -0.0252, -0.1276, -0.0143,  0.1551,
         -0.1309,  0.0981, -0.1050, -0.1608,  0.0936,  0.1610,  0.1939,  0.0595,
         -0.1434],
        [ 0.1368,  0.0769, -0.1319,  0.0702,  0.1511,  0.1826,  0.1370,  0.0396,
         -0.0592, -0.1997, -0.0891,  0.1734, -0.1177,  0.1954, -0.0869, -0.1786,
         -0.0770, -0.0237,  0.0132,  0.1055,  0.0573,  0.0152,  0.0918,  0.1295,
          0.0700],
        [-0.1347, -0.0280,  0.1969, -0.0401, -0.0440, -0.1055, -0.1799,  0.1940,
         -0.0709,  0.0558, -0.0779,  0.0978, -0.0460, -0.0004,  0.0182,  0.1879,
         -0.1797, -0.1351,  0.1859,  0.1130, -0.0335,  0.1494,  0.0268,  0.1845,
         -0.0520],
        [-0.0930,  0.0786,  0.1222,  0.1297,  0.0901,  0.1116, -0.1213,  0.0318,
         -0.1458,  0.1413, -0.1580, -0.0422,  0.0464, -0.1060, -0.0591,  0.0294,
          0.0947, -0.0931, -0.0033, -0.1299, -0.1238, -0.1105,  0.1319,  0.0921,
          0.1190],
        [ 0.1910, -0.0982, -0.0704,  0.0068,  0.0795, -0.1017, -0.0164,  0.1319,
         -0.1738, -0.0756, -0.1608,  0.1877, -0.1019, -0.0050, -0.0128,  0.1417,
         -0.1808,  0.0725, -0.0606, -0.0656,  0.0868, -0.1818, -0.0095,  0.0896,
         -0.1843]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(2.9908, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7346, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(3.6738, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9663, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6823, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(3.7509, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.9143, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(2.7119, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.0847, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4454, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(3.4231, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(3.5160, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(2.8478, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.9514, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.7791, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(4.3136, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(3.2696, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(2.3649, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.8326, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.6669, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.0972549019607843  Action:  2  Reward Received:  0
State:  0.8992941176470589  Action:  1  Reward Received:  2.0
State:  0.48125490196078435  Action:  0  Reward Received:  0.1
State:  0.0972549019607843  Action:  2  Reward Received:  0
State:  0.8992941176470589  Action:  1  Reward Received:  2.0

Total Reward Received:  4.1
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0077, -0.1093,  0.1589,  0.0989,  0.0424, -0.1096, -0.1084, -0.1736,
          0.0207,  0.0629, -0.1820,  0.1616, -0.1254,  0.0229,  0.0768, -0.1179,
          0.0765,  0.1585,  0.1163, -0.0168,  0.1620, -0.1021, -0.1382, -0.0051,
         -0.0696],
        [-0.1564,  0.0246, -0.1952, -0.1604, -0.0770,  0.0490, -0.0800, -0.1905,
          0.1648,  0.1035,  0.0284,  0.1174, -0.1012,  0.1355, -0.1709, -0.1442,
         -0.1820,  0.0826, -0.1248, -0.0552, -0.1991, -0.0731,  0.1979, -0.0188,
          0.1464],
        [-0.0142,  0.1182,  0.0507,  0.0022, -0.0109, -0.0408, -0.1738, -0.1484,
         -0.1931,  0.1416,  0.1914, -0.0311,  0.0438, -0.1327,  0.1108, -0.1339,
         -0.0873, -0.1914,  0.1612, -0.1863, -0.0204,  0.1901,  0.1251,  0.1272,
          0.1767],
        [ 0.0206, -0.0029, -0.0298,  0.0540,  0.0507, -0.1567, -0.1225, -0.0980,
          0.0982,  0.0246,  0.1780, -0.1845,  0.1533,  0.1148,  0.0877, -0.0823,
          0.1963, -0.0397,  0.1899, -0.1547, -0.0351, -0.0926, -0.1398, -0.0814,
          0.0651],
        [ 0.1196, -0.0926,  0.0384,  0.1926,  0.0302,  0.0549,  0.0928,  0.0579,
          0.1692,  0.1020, -0.1354, -0.0875, -0.1072,  0.1205, -0.0014,  0.0572,
         -0.1229,  0.0052, -0.1005,  0.1911, -0.0964,  0.0885,  0.0761,  0.1141,
         -0.1813]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0551, -0.1998,  0.0845, -0.1284, -0.0495, -0.0844, -0.1547,  0.1433,
         -0.0801,  0.0913,  0.0282, -0.0832, -0.1399, -0.1426,  0.1492, -0.1020,
         -0.0957,  0.1690,  0.0219,  0.0141, -0.1507,  0.1463, -0.1249,  0.0263,
          0.1356],
        [-0.0584, -0.0908, -0.0122, -0.1750,  0.1066,  0.1681, -0.0503,  0.1932,
         -0.0651, -0.0071, -0.0854, -0.0540, -0.1145, -0.1279, -0.0832, -0.1251,
          0.1002, -0.0330, -0.1171,  0.0818,  0.0148, -0.1956,  0.0912,  0.1281,
          0.0339],
        [ 0.1043,  0.0616, -0.1476,  0.0266,  0.1694,  0.1433,  0.1007, -0.0250,
         -0.0207,  0.1907,  0.0287,  0.1897,  0.1399,  0.1548, -0.0557,  0.0886,
         -0.1589,  0.1028,  0.1775,  0.1046,  0.0562, -0.0976,  0.1522, -0.1099,
          0.1184],
        [-0.0790,  0.1600, -0.0684,  0.1456,  0.0422, -0.0154,  0.0528, -0.1072,
         -0.0168,  0.1136,  0.1562, -0.0038,  0.1510,  0.0263,  0.1731, -0.0073,
         -0.1935, -0.0123,  0.1338, -0.0208, -0.0209, -0.1812, -0.0658,  0.0461,
         -0.1666],
        [ 0.0192, -0.1551, -0.1362,  0.1591, -0.0043, -0.0358, -0.0045,  0.1137,
         -0.1491, -0.0478,  0.0631, -0.1278,  0.1368, -0.1660, -0.1827,  0.1444,
          0.0005,  0.0625,  0.0158,  0.0262, -0.0723, -0.1819,  0.0617,  0.1385,
          0.1845]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(2.5080, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.9848, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(3.5409, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6330, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4484, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(3.2648, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6183, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(2.9681, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.0819, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.6872, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(3.1260, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(3.4803, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(2.9265, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.7685, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(2.1311, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(3.8672, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(3.4387, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(2.7172, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.8521, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.4646, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.09050980392156864  Action:  2  Reward Received:  0
State:  0.8654117647058824  Action:  1  Reward Received:  2.0
State:  0.500078431372549  Action:  0  Reward Received:  0.1
State:  0.09050980392156864  Action:  2  Reward Received:  0
State:  0.8654117647058824  Action:  1  Reward Received:  2.0

Total Reward Received:  4.1
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1912,  0.0324,  0.1597,  0.1710,  0.0915,  0.0445, -0.0174,  0.0574,
         -0.0513,  0.1225,  0.1436, -0.0980,  0.0751,  0.1735, -0.0438,  0.1485,
         -0.0663, -0.0671, -0.0650,  0.1094,  0.0048,  0.1471, -0.1270,  0.0155,
          0.0569],
        [ 0.0507, -0.0790,  0.0397,  0.0430,  0.0751,  0.0931,  0.1816,  0.0024,
         -0.1119, -0.1558,  0.1664, -0.0398,  0.0723,  0.0990,  0.0498,  0.0973,
          0.0946,  0.0899, -0.1038, -0.0384,  0.0650,  0.0783, -0.1470,  0.1252,
          0.1709],
        [ 0.1968, -0.1689, -0.1471, -0.1848,  0.1594,  0.0627,  0.1407,  0.1828,
          0.0177, -0.1597, -0.0691, -0.1386, -0.1429, -0.0760,  0.0641, -0.1515,
          0.0590, -0.1631,  0.1803,  0.0486,  0.1285,  0.0390,  0.0955, -0.0927,
         -0.0114],
        [ 0.1858,  0.1097,  0.1873, -0.0959,  0.1129,  0.0671, -0.0856,  0.1544,
         -0.0613,  0.1891, -0.0305,  0.0954,  0.1539, -0.1388, -0.0857,  0.0268,
          0.0093, -0.1599, -0.1164, -0.0175, -0.0404,  0.0451,  0.1250,  0.0235,
         -0.0561],
        [-0.0190,  0.0191,  0.1652, -0.0816, -0.1706,  0.1739,  0.0702, -0.1481,
         -0.0118, -0.0304,  0.1576, -0.0258, -0.0672,  0.1940, -0.0349,  0.1936,
          0.0110, -0.1355, -0.1570,  0.0140, -0.0129, -0.0741,  0.0569, -0.0394,
          0.0297]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0894, -0.0140,  0.0183, -0.0688, -0.0859,  0.1608, -0.0939,  0.0494,
          0.1297,  0.1019, -0.1692,  0.0455,  0.1942, -0.1732, -0.1788, -0.1620,
         -0.0690, -0.0117,  0.1750,  0.1582, -0.0286, -0.0306, -0.0773, -0.0003,
          0.1444],
        [-0.1251,  0.1460, -0.0249, -0.1879, -0.0837, -0.1891, -0.0402, -0.0422,
          0.0956, -0.1523, -0.1654,  0.0945, -0.0943, -0.0570, -0.1145, -0.1012,
          0.1910,  0.1165,  0.1123,  0.1593,  0.0651,  0.0095,  0.0784,  0.1277,
         -0.0832],
        [-0.0251, -0.0662,  0.1047, -0.0600,  0.1338, -0.0433,  0.1167, -0.0459,
         -0.1798, -0.1425,  0.0638, -0.0026, -0.0460,  0.0227,  0.1872, -0.1695,
          0.1777, -0.0826,  0.1035,  0.1337, -0.0308,  0.0020,  0.1103,  0.0346,
          0.1959],
        [ 0.0670,  0.1391,  0.0891,  0.0124,  0.1330, -0.1664, -0.1776, -0.0400,
         -0.0135,  0.0290, -0.1038, -0.0650, -0.0578,  0.0665, -0.0138,  0.1588,
         -0.0323,  0.1467, -0.0062,  0.0379, -0.1462, -0.0567, -0.1622, -0.0096,
          0.1002],
        [ 0.1531,  0.0027, -0.0899,  0.0469, -0.0896, -0.1952, -0.0332, -0.1260,
          0.0910, -0.0278, -0.0680,  0.0886, -0.1957,  0.1968,  0.0672, -0.1966,
          0.0020, -0.1388, -0.1265, -0.1658,  0.0624, -0.1840, -0.0469, -0.0123,
          0.0940]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(2.8221, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5443, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(3.0956, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3612, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4705, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(3.2883, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.4069, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.9182, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.3352, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.7066, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(2.9681, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(3.4695, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0638, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.6390, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(2.2631, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(3.7018, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(3.4391, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2652, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.3351, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.5990, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.10776470588235293  Action:  2  Reward Received:  0
State:  0.9044705882352941  Action:  1  Reward Received:  2.0
State:  0.5030588235294118  Action:  0  Reward Received:  0.1
State:  0.10776470588235293  Action:  2  Reward Received:  0
State:  0.9044705882352941  Action:  1  Reward Received:  2.0

Total Reward Received:  4.1
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1176,  0.0264, -0.1529, -0.1149, -0.0010, -0.1460,  0.0345, -0.1333,
          0.1996, -0.1653, -0.0806,  0.1459,  0.1719, -0.0112,  0.1027, -0.1440,
         -0.0189,  0.1145,  0.0918,  0.0628,  0.1646,  0.1575,  0.0872, -0.0776,
          0.0026],
        [-0.1494, -0.1994,  0.0526,  0.0016,  0.1508,  0.1106, -0.0765,  0.0059,
         -0.1567, -0.0704, -0.1498,  0.0793,  0.1498, -0.1541, -0.0274, -0.1491,
          0.0145, -0.1056,  0.1588, -0.0090,  0.1224, -0.1047,  0.0158,  0.1372,
          0.1581],
        [ 0.1952,  0.0152, -0.1959, -0.1971, -0.1587,  0.0151,  0.1431, -0.1207,
         -0.0312, -0.1158,  0.0848,  0.0972, -0.0817, -0.0360, -0.1943, -0.0368,
          0.0846,  0.1426, -0.1027, -0.0617,  0.0382,  0.0140,  0.1008,  0.1481,
          0.0466],
        [ 0.0790, -0.1141,  0.1255,  0.1762,  0.1465, -0.0658, -0.0151, -0.0048,
         -0.0220, -0.0392,  0.1672, -0.1507,  0.1265, -0.1838, -0.0499,  0.0141,
         -0.0682, -0.1090, -0.1426,  0.1913,  0.1030, -0.0112, -0.1574, -0.0119,
         -0.1237],
        [ 0.1176, -0.1983,  0.0966, -0.1566, -0.1538, -0.1788, -0.0036, -0.1213,
          0.0180, -0.0716, -0.1374,  0.1631, -0.1584, -0.1657, -0.1955,  0.0957,
          0.1497,  0.1481,  0.0913,  0.1060, -0.1575, -0.0710,  0.0534, -0.0728,
         -0.0406]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0338, -0.0133,  0.0542, -0.1571, -0.1518, -0.1122,  0.1224,  0.1715,
         -0.1482,  0.1950,  0.1256,  0.0858, -0.1847, -0.1519,  0.1438,  0.1659,
         -0.0863,  0.1300,  0.0492,  0.0237,  0.0438,  0.1411,  0.0089, -0.1807,
          0.0667],
        [-0.1602, -0.1899, -0.0197, -0.0910, -0.1823,  0.1643, -0.0924, -0.1716,
         -0.0611,  0.1403, -0.0480,  0.0899, -0.0902,  0.0007, -0.0304,  0.0303,
          0.0981,  0.0177, -0.0159,  0.0748, -0.1386, -0.0820,  0.0744,  0.1100,
          0.1633],
        [-0.1922,  0.1260, -0.0354, -0.1902,  0.0757, -0.0323, -0.0084, -0.0647,
         -0.1007,  0.1041,  0.0768, -0.0687, -0.1738, -0.1578,  0.0931, -0.1865,
          0.0835,  0.0512, -0.1158, -0.1282, -0.1958,  0.0438,  0.0089, -0.1593,
          0.0618],
        [-0.0111,  0.0773, -0.1839, -0.0088, -0.0583, -0.0951, -0.0400,  0.0504,
         -0.1258, -0.0168, -0.1873, -0.1361, -0.0586, -0.0068,  0.1759,  0.0820,
          0.0103, -0.1465, -0.0401, -0.1397, -0.1569,  0.0254, -0.1759, -0.0548,
          0.0968],
        [-0.1780,  0.1262,  0.1030, -0.0591,  0.1147, -0.0170, -0.1725, -0.0788,
         -0.0441, -0.0065, -0.1922, -0.0124, -0.0819,  0.1674, -0.0791,  0.1861,
         -0.1235,  0.0147,  0.1100, -0.1292,  0.1045, -0.0254, -0.1559,  0.1475,
         -0.0395]], dtype=torch.float64, requires_grad=True)
Traceback (most recent call last):
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/Atari DQN Project/Linear.py", line 429, in <module>
    main()
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/Atari DQN Project/Linear.py", line 418, in main
    model.train()
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/Atari DQN Project/Q_Learning.py", line 93, in train
    self.train_on_minibatch(self.replay_buffer.sample_minibatch(), self.t)
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/Atari DQN Project/Linear.py", line 111, in train_on_minibatch
    self.approx_network.optimizer.step()
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 124, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/env/lib/python3.11/site-packages/torch/optim/optimizer.py", line 485, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/env/lib/python3.11/site-packages/torch/optim/optimizer.py", line 79, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/env/lib/python3.11/site-packages/torch/optim/adam.py", line 246, in step
    adam(
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/env/lib/python3.11/site-packages/torch/optim/optimizer.py", line 147, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/env/lib/python3.11/site-packages/torch/optim/adam.py", line 933, in adam
    func(
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/env/lib/python3.11/site-packages/torch/optim/adam.py", line 525, in _single_tensor_adam
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
