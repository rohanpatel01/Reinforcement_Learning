[I 2025-07-07 21:29:23,197] A new study created in RDB with name: Test_Env_Testing_More_Params
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7497],
        [ 0.6742],
        [-0.5187],
        [ 0.5173],
        [-0.0798]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.4843],
        [ 0.4028],
        [-0.7137],
        [-0.2452],
        [-0.1574]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3847, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5849, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.7130, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6459, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.3956, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4687, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5646, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6253, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6907, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4203, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5527, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5443, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5375, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.7355, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4450, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6367, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5240, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4498, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7803, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4698, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  3  Reward Received:  1.0
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0840],
        [-0.0203],
        [-0.0878],
        [ 0.0448],
        [ 0.0247]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3847, 0.5849, 0.7130, 0.6459, 0.3956], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:29:28,761] Trial 0 finished with value: 0.7000000000000001 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.8, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 0 with value: 0.7000000000000001.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7782],
        [ 0.7066],
        [ 0.9168],
        [-0.8993],
        [-0.4024]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7462],
        [ 0.1043],
        [ 0.8712],
        [-0.7403],
        [-0.5156]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7994, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6290, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9308, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8432, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7934, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6625, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.8004, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.8447, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.8776, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.7496, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5256, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.9718, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7587, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.9119, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.7058, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3888, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.1433, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6726, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.9463, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.6621, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1369],
        [ 0.1714],
        [-0.0861],
        [ 0.0344],
        [-0.0438]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7994, 0.6290, 0.9308, 0.8432, 0.7934], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:29:34,393] Trial 1 finished with value: 1.5999999999999999 and parameters: {'nsteps_train': 12000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 1.4, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.0470],
        [-0.4618],
        [ 0.4963],
        [ 0.3133],
        [ 0.9626]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.4841],
        [-0.8547],
        [-0.1733],
        [-0.9485],
        [-0.1653]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6452, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5398, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5715, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6147, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6423, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5119, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5509, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5137, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5934, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5290, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3787, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5619, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4558, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5720, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4157, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2455, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5730, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3980, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5506, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3024, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1332],
        [ 0.0110],
        [-0.0579],
        [-0.0214],
        [-0.1133]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6452, 0.5398, 0.5715, 0.6147, 0.6423], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:29:40,647] Trial 2 finished with value: 0.5 and parameters: {'nsteps_train': 14000, 'lr_begin': 0.005, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.9, 'minibatch_size': 32}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9020],
        [ 0.9341],
        [ 0.9376],
        [ 0.5371],
        [-0.7750]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7779],
        [ 0.1505],
        [ 0.3581],
        [ 0.8409],
        [-0.7370]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.8900, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6905, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6844, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6402, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8527, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6766, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7125, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6254, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6390, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6913, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4632, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7345, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5664, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6377, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5298, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2498, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7565, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5074, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6365, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3684, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2134],
        [ 0.0220],
        [-0.0590],
        [-0.0013],
        [-0.1614]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.8900, 0.6905, 0.6844, 0.6402, 0.8527], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:29:47,553] Trial 3 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.05, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.6, 'minibatch_size': 64}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4765],
        [-0.1476],
        [-0.1117],
        [ 0.8355],
        [ 0.0143]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6399],
        [ 0.9578],
        [ 0.0017],
        [ 0.9512],
        [-0.7004]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7447, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6449, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.7309, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7484, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7650, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6217, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7311, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6620, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.7327, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6968, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4986, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.8173, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5932, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.7171, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6285, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3755, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.9034, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5243, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7014, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5603, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1231],
        [ 0.0862],
        [-0.0689],
        [-0.0156],
        [-0.0683]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7447, 0.6449, 0.7309, 0.7484, 0.7650], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:29:52,261] Trial 4 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.9, 'minibatch_size': 32}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4595],
        [-0.4722],
        [-0.3732],
        [ 0.1718],
        [-0.4801]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8256],
        [-0.1775],
        [ 0.3237],
        [ 0.4904],
        [ 0.0507]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6031, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4253, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5077, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5942, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6360, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4899, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4497, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5001, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5832, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5643, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3767, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4741, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4924, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5722, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4926, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2635, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4985, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4848, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5611, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4210, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1132],
        [ 0.0244],
        [-0.0076],
        [-0.0110],
        [-0.0717]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6031, 0.4253, 0.5077, 0.5942, 0.6360], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:30:03,642] Trial 5 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.05, 'epsilon_decay_percentage': 0.8, 'lr_decay_percentage': 0.7, 'minibatch_size': 128}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.3281],
        [ 0.3698],
        [-0.7523],
        [ 0.2710],
        [-0.8184]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6917],
        [ 0.5684],
        [-0.0251],
        [ 0.9222],
        [ 0.0755]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.9444, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.9767, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.0332, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4924, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.9972, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0578, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7470, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.8582, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.9017, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4456, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.1711, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.5173, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.6831, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.3111, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.8940, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.2845, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(3.2876, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.5080, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.7204, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.3423, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.1133],
        [ 0.7703],
        [-0.1751],
        [ 0.4093],
        [ 0.4484]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.9444, 0.9767, 2.0332, 1.4924, 0.9972], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:30:31,546] Trial 6 finished with value: 1.5999999999999999 and parameters: {'nsteps_train': 14000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 1, 'minibatch_size': 256}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.6490],
        [-0.4478],
        [-0.0178],
        [ 0.5556],
        [-0.8316]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.7831],
        [-0.2763],
        [-0.3515],
        [ 0.3141],
        [-0.1323]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6055, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4212, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4918, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4934, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5828, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4375, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4945, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4397, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5358, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4867, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2695, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5678, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3875, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5783, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3907, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1015, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6411, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3354, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6207, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2947, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1680],
        [ 0.0733],
        [-0.0521],
        [ 0.0424],
        [-0.0960]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6055, 0.4212, 0.4918, 0.4934, 0.5828], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:30:55,572] Trial 7 finished with value: 0.5 and parameters: {'nsteps_train': 12000, 'lr_begin': 0.05, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.5, 'minibatch_size': 256}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2280],
        [-0.3908],
        [-0.1781],
        [ 0.3761],
        [ 0.9301]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.4764],
        [ 0.4329],
        [ 0.4468],
        [-0.1070],
        [ 0.7297]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6326, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.1591, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5021, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4171, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.2944, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5215, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2342, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4209, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4086, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.2860, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4105, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3093, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3397, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4002, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2776, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2994, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3844, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2585, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.3918, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2692, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1110],
        [ 0.0751],
        [-0.0812],
        [-0.0084],
        [-0.0084]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6326, 0.1591, 0.5021, 0.4171, 0.2944], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:31:00,300] Trial 8 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.5, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5883],
        [ 0.9024],
        [ 0.1696],
        [ 0.8791],
        [ 0.1370]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1128],
        [-0.8471],
        [ 0.4511],
        [-0.8764],
        [-0.8522]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5638, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4063, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4785, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4905, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5808, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4834, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4699, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4477, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5293, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4915, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4030, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5336, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4169, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5682, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4022, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3226, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5972, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3861, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6071, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3129, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0804],
        [ 0.0636],
        [-0.0308],
        [ 0.0389],
        [-0.0893]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5638, 0.4063, 0.4785, 0.4905, 0.5808], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:31:20,446] Trial 9 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.6, 'minibatch_size': 256}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1511],
        [ 0.0058],
        [ 0.3190],
        [-0.2849],
        [-0.6158]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0771],
        [ 0.1771],
        [ 0.7918],
        [-0.2959],
        [-0.1549]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3418, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0893, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6821, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4713, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2283, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.1764, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.4381, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4871, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5077, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2495, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.0109, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.7869, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2922, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.5440, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.2707, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.8455, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.1357, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.0973, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.5804, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.2919, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1654],
        [ 0.3488],
        [-0.1949],
        [ 0.0363],
        [ 0.0212]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3418, 1.0893, 1.6821, 1.4713, 1.2283], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:31:28,562] Trial 10 finished with value: 1.5999999999999999 and parameters: {'nsteps_train': 12000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 1.4, 'lr_decay_percentage': 0.8, 'minibatch_size': 64}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.4816],
        [ 0.2233],
        [-0.7657],
        [ 0.2705],
        [ 0.9777]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9122],
        [ 0.6139],
        [-0.5015],
        [-0.7795],
        [-0.2205]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.4212, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.8072, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9624, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.9066, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5646, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4852, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.9167, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.8693, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.8244, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6271, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5492, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.0262, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7761, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.7423, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6896, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6132, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.1357, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6830, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6602, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.7522, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0640],
        [ 0.1095],
        [-0.0931],
        [-0.0821],
        [ 0.0625]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.4212, 0.8072, 0.9624, 0.9066, 0.5646], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:31:50,594] Trial 11 finished with value: 1.4000000000000001 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 1, 'minibatch_size': 256}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.3093],
        [-0.1451],
        [-0.6172],
        [ 0.2047],
        [-0.4082]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2458],
        [ 0.5228],
        [-0.3689],
        [ 0.1271],
        [-0.6222]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5441, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4303, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5122, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5108, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5290, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4155, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4979, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4926, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5453, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4671, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2869, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5656, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4729, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5798, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4052, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1583, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6332, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4532, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6143, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3433, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1286],
        [ 0.0677],
        [-0.0197],
        [ 0.0345],
        [-0.0619]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5441, 0.4303, 0.5122, 0.5108, 0.5290], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:32:06,879] Trial 12 finished with value: 0.5 and parameters: {'nsteps_train': 15000, 'lr_begin': 0.005, 'epsilon_decay_percentage': 1.3, 'lr_decay_percentage': 1, 'minibatch_size': 128}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4154],
        [-0.1426],
        [ 0.8777],
        [ 0.6183],
        [-0.3620]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2515],
        [-0.3193],
        [ 0.5037],
        [ 0.4001],
        [ 0.1515]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5258, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3300, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4056, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.3894, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5358, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4245, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3858, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3764, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4104, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4287, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3231, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4416, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3472, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4314, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3216, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2217, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4974, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3181, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4524, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2146, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1014],
        [ 0.0558],
        [-0.0292],
        [ 0.0210],
        [-0.1071]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5258, 0.3300, 0.4056, 0.3894, 0.5358], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:32:32,852] Trial 13 finished with value: 0.5 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.4, 'lr_decay_percentage': 1, 'minibatch_size': 256}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.2563],
        [ 0.6183],
        [ 0.7693],
        [ 0.0700],
        [-0.1906]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7654],
        [-0.2382],
        [-0.8485],
        [-0.3993],
        [-0.4451]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5493, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4103, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4761, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4309, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5650, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4694, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4524, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4522, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4341, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4530, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3895, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4944, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4283, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4374, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3411, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3097, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5365, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4045, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4406, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2291, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0799],
        [ 0.0421],
        [-0.0239],
        [ 0.0032],
        [-0.1120]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5493, 0.4103, 0.4761, 0.4309, 0.5650], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:32:39,043] Trial 14 finished with value: 0.5 and parameters: {'nsteps_train': 14000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 1.2, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9559],
        [-0.8765],
        [-0.9523],
        [-0.3604],
        [ 0.3518]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9186],
        [ 0.8524],
        [ 0.8798],
        [ 0.0356],
        [-0.7436]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5589, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5236, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4701, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.3218, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.3984, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5162, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4692, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4318, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.3612, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.3512, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4735, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4148, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3935, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4007, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3039, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.4309, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3603, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3552, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4402, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2567, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0427],
        [-0.0544],
        [-0.0383],
        [ 0.0395],
        [-0.0472]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5589, 0.5236, 0.4701, 0.3218, 0.3984], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:33:07,143] Trial 15 finished with value: 0.5 and parameters: {'nsteps_train': 14000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.8, 'minibatch_size': 256}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1858],
        [-0.0491],
        [ 0.5326],
        [ 0.5769],
        [-0.6672]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.3688],
        [-0.2543],
        [-0.4113],
        [-0.3249],
        [ 0.0706]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6312, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2484, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8438, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5702, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5551, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.4187, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6972, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.5896, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5968, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.5248, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.2062, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.1459, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.3355, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.6234, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.4946, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.9936, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.5947, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.0813, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.6499, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.4644, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2125],
        [ 0.4488],
        [-0.2542],
        [ 0.0266],
        [-0.0302]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6312, 1.2484, 1.8438, 1.5702, 1.5551], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:33:15,220] Trial 16 finished with value: 1.4000000000000001 and parameters: {'nsteps_train': 12000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 1, 'lr_decay_percentage': 0.5, 'minibatch_size': 64}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8922],
        [ 0.6538],
        [ 0.0455],
        [ 0.0524],
        [-0.9775]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7327],
        [-0.4961],
        [ 0.6912],
        [-0.3823],
        [-0.6526]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7654, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6525, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9142, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8196, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6945, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6265, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.8774, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.8637, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.8918, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6959, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4875, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.1024, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.8131, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.9641, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6973, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3486, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.3273, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7625, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.0363, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.6987, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1389],
        [ 0.2250],
        [-0.0506],
        [ 0.0722],
        [ 0.0014]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7654, 0.6525, 0.9142, 0.8196, 0.6945], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:33:27,482] Trial 17 finished with value: 1.5999999999999999 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.005, 'epsilon_decay_percentage': 0.7, 'lr_decay_percentage': 1, 'minibatch_size': 128}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9677],
        [ 0.2051],
        [ 0.4303],
        [-0.2877],
        [-0.3618]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2486],
        [-0.6541],
        [ 0.3018],
        [ 0.4581],
        [ 0.4820]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5928, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5317, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6481, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6049, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5939, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4305, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6404, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5917, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6153, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5356, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2681, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7492, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5353, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6258, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4772, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1058, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.8579, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4788, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6362, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4189, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1623],
        [ 0.1087],
        [-0.0564],
        [ 0.0104],
        [-0.0583]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5928, 0.5317, 0.6481, 0.6049, 0.5939], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:33:33,301] Trial 18 finished with value: 1.4000000000000001 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.6014],
        [-0.4264],
        [ 0.9579],
        [ 0.2446],
        [-0.6578]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0488],
        [-0.8687],
        [ 0.6642],
        [ 0.9433],
        [ 0.1651]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6315, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4918, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5630, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5179, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6305, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6331, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4722, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5217, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5254, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5265, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6346, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4525, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4804, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5329, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4225, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6362, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4329, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4392, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5403, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3185, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0016],
        [-0.0196],
        [-0.0413],
        [ 0.0075],
        [-0.1040]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6315, 0.4918, 0.5630, 0.5179, 0.6305], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:34:03,091] Trial 19 finished with value: 0.5 and parameters: {'nsteps_train': 15000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.4, 'lr_decay_percentage': 1, 'minibatch_size': 256}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.2766],
        [ 0.2827],
        [-0.7971],
        [-0.9163],
        [ 0.9866]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6592],
        [-0.7155],
        [-0.8077],
        [ 0.4534],
        [ 0.6583]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6070, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5020, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6098, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6136, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6123, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4899, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6109, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5099, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5790, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4820, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3729, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7197, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4099, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5444, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3517, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2558, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.8286, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3100, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5099, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2214, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.8999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1170],
        [ 0.1089],
        [-0.0999],
        [-0.0346],
        [-0.1303]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6070, 0.5020, 0.6098, 0.6136, 0.6123], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:34:09,281] Trial 20 finished with value: -0.8999999999999999 and parameters: {'nsteps_train': 14000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2370],
        [ 0.7619],
        [-0.9497],
        [-0.4854],
        [ 0.1126]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6177],
        [ 0.6124],
        [ 0.7836],
        [ 0.8574],
        [ 0.2017]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6646, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5933, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6600, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6671, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6613, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5237, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6349, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5733, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6701, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5655, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3828, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6764, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4867, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6731, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4696, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2419, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7180, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4001, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6761, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3738, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1409],
        [ 0.0416],
        [-0.0866],
        [ 0.0030],
        [-0.0959]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6646, 0.5933, 0.6600, 0.6671, 0.6613], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:34:17,372] Trial 21 finished with value: -0.7000000000000001 and parameters: {'nsteps_train': 12000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 1.4, 'lr_decay_percentage': 0.8, 'minibatch_size': 64}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7257],
        [ 0.1290],
        [ 0.7979],
        [-0.0020],
        [ 0.9354]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6005],
        [ 0.0722],
        [ 0.7775],
        [ 0.4588],
        [-0.3961]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6225, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4851, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5723, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5753, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6159, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5111, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5397, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5458, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5545, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5187, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3996, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5944, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5192, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5337, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4215, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2882, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6491, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4926, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5129, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3243, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1114],
        [ 0.0547],
        [-0.0266],
        [-0.0208],
        [-0.0972]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6225, 0.4851, 0.5723, 0.5753, 0.6159], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:34:25,551] Trial 22 finished with value: 0.5 and parameters: {'nsteps_train': 12000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 1.4, 'lr_decay_percentage': 0.8, 'minibatch_size': 64}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.3766],
        [-0.3974],
        [ 0.6015],
        [-0.1223],
        [-0.5045]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3040],
        [ 0.7153],
        [ 0.4798],
        [ 0.6595],
        [-0.2708]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7230, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6183, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.8949, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8322, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7172, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6057, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.8394, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.8440, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.8865, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6979, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4884, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.0605, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7931, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.9408, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6786, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3711, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.2816, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7422, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.9952, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.6593, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1173],
        [ 0.2211],
        [-0.0509],
        [ 0.0543],
        [-0.0193]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7230, 0.6183, 0.8949, 0.8322, 0.7172], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:34:33,622] Trial 23 finished with value: 1.5999999999999999 and parameters: {'nsteps_train': 12000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 1.4, 'lr_decay_percentage': 0.8, 'minibatch_size': 64}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0842],
        [-0.1244],
        [-0.1039],
        [-0.0798],
        [ 0.0801]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.3744],
        [-0.0741],
        [-0.9850],
        [ 0.6151],
        [ 0.2311]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2184, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0554, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3693, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.0501, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0648, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0523, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.3097, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2102, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.1182, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0766, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.8863, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.5641, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0511, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.1863, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0883, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7202, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.8184, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.8920, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.2544, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.1000, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1661],
        [ 0.2543],
        [-0.1591],
        [ 0.0681],
        [ 0.0117]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2184, 1.0554, 1.3693, 1.0501, 1.0648], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:34:41,740] Trial 24 finished with value: 1.4000000000000001 and parameters: {'nsteps_train': 12000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 0.7, 'lr_decay_percentage': 0.8, 'minibatch_size': 64}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.0136],
        [ 0.1059],
        [-0.1505],
        [ 0.4937],
        [ 0.7003]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9207],
        [-0.1199],
        [-0.4884],
        [ 0.7402],
        [ 0.4761]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6226, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4885, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5818, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5928, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6290, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4818, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5695, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5359, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6392, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5409, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3410, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6506, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4900, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6857, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4528, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2002, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7316, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4442, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7321, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3647, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1408],
        [ 0.0810],
        [-0.0459],
        [ 0.0464],
        [-0.0881]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6226, 0.4885, 0.5818, 0.5928, 0.6290], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:34:49,829] Trial 25 finished with value: 0.5 and parameters: {'nsteps_train': 12000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 1.3, 'lr_decay_percentage': 0.6, 'minibatch_size': 64}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.7415],
        [ 0.8254],
        [ 0.0128],
        [-0.0466],
        [ 0.7321]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9903],
        [ 0.3655],
        [ 0.1669],
        [ 0.3624],
        [ 0.7998]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5503, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4479, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4135, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4472, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5424, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4174, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4364, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3720, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4371, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4220, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2844, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4250, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3304, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4270, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3016, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1514, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4135, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2888, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4169, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1811, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1330],
        [-0.0115],
        [-0.0416],
        [-0.0101],
        [-0.1204]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5503, 0.4479, 0.4135, 0.4472, 0.5424], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:35:17,808] Trial 26 finished with value: 0.5 and parameters: {'nsteps_train': 14000, 'lr_begin': 0.005, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.9, 'minibatch_size': 256}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.3583],
        [ 0.0071],
        [-0.5412],
        [ 0.0888],
        [ 0.5088]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6348],
        [ 0.8732],
        [-0.4833],
        [-0.0860],
        [ 0.2163]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.9258, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.7350, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.8587, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8134, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8891, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7736, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.8500, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.7549, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.7850, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.7746, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6214, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.9649, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6511, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.7565, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6601, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.4692, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.0799, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5474, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7281, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5455, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1522],
        [ 0.1149],
        [-0.1038],
        [-0.0284],
        [-0.1145]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.9258, 0.7350, 0.8587, 0.8134, 0.8891], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:35:31,099] Trial 27 finished with value: 0.5 and parameters: {'nsteps_train': 12000, 'lr_begin': 0.05, 'epsilon_decay_percentage': 0.5, 'lr_decay_percentage': 0.8, 'minibatch_size': 128}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.3955],
        [-0.3745],
        [ 0.2058],
        [-0.3550],
        [ 0.6857]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6233],
        [-0.6627],
        [ 0.2422],
        [ 0.0608],
        [-0.4096]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5735, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4479, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4362, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.3897, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5591, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4551, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5039, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3968, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.3923, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4132, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3367, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5600, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3575, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.3948, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2673, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2183, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6160, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3182, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.3973, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1214, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1184],
        [ 0.0560],
        [-0.0393],
        [ 0.0025],
        [-0.1459]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5735, 0.4479, 0.4362, 0.3897, 0.5591], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:35:37,760] Trial 28 finished with value: 0.5 and parameters: {'nsteps_train': 15000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 1 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5574],
        [ 0.4767],
        [ 0.4967],
        [ 0.7437],
        [-0.4082]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1688],
        [-0.4062],
        [ 0.7342],
        [ 0.2818],
        [-0.2522]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(2.0823, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2427, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(3.0403, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.0461, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1062, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.8946, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.6119, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(2.6617, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.1453, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.8997, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.7069, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(3.9811, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(2.2830, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(3.2445, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(2.6931, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.5191, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(5.3503, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.9044, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(4.3437, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(3.4866, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1877],
        [ 1.3692],
        [-0.3787],
        [ 1.0992],
        [ 0.7935]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([2.0823, 1.2427, 3.0403, 1.0461, 1.1062], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:36:03,715] Trial 29 finished with value: 4.0 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.2, 'lr_decay_percentage': 1, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5978],
        [ 0.7617],
        [ 0.7090],
        [ 0.3764],
        [-0.4484]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2910],
        [-0.7977],
        [-0.7983],
        [ 0.2167],
        [ 0.4458]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.8312, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6510, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.8639, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8976, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8065, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7397, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7445, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.7822, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.8391, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.7076, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6482, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.8380, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7005, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.7805, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6086, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.5567, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.9315, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6188, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7219, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5097, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0915],
        [ 0.0935],
        [-0.0817],
        [-0.0586],
        [-0.0989]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.8312, 0.6510, 0.8639, 0.8976, 0.8065], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:36:29,713] Trial 30 finished with value: -0.7000000000000001 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.2, 'lr_decay_percentage': 1, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9001],
        [ 0.2315],
        [-0.4671],
        [-0.3371],
        [ 0.7017]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.3532],
        [-0.5265],
        [-0.8171],
        [-0.6871],
        [-0.9280]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7155, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5776, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.8978, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6879, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7796, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5553, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.9037, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.8619, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.8288, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.7360, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3951, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.2298, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.8260, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.9696, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6924, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2349, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.5559, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7902, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.1105, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.6487, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1602],
        [ 0.3261],
        [-0.0359],
        [ 0.1409],
        [-0.0436]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7155, 0.5776, 0.8978, 0.6879, 0.7796], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:36:55,570] Trial 31 finished with value: 1.4000000000000001 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.2, 'lr_decay_percentage': 1, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.7137],
        [-0.7236],
        [ 0.8874],
        [-0.9855],
        [ 0.5139]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6783],
        [-0.9655],
        [-0.4769],
        [-0.8919],
        [-0.2144]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3563, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2984, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.2446, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4562, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.4972, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.1584, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3530, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3270, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5722, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4136, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(-0.0395, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4076, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4093, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6882, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3299, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(-0.2373, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4622, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4917, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.8042, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2462, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1979],
        [ 0.0546],
        [ 0.0823],
        [ 0.1160],
        [-0.0837]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3563, 0.2984, 0.2446, 0.4562, 0.4972], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:37:21,395] Trial 32 finished with value: 0.5 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.8, 'lr_decay_percentage': 1, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2518],
        [ 0.0592],
        [-0.3425],
        [ 0.7678],
        [ 0.2738]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.3842],
        [ 0.3329],
        [-0.1784],
        [-0.5586],
        [-0.1418]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3415, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3346, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5718, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5190, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.4140, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3665, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5447, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5563, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5802, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4199, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3915, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7547, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5408, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6414, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4258, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.4165, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.9648, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5252, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7025, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4317, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0250],
        [ 0.2101],
        [-0.0155],
        [ 0.0612],
        [ 0.0059]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3415, 0.3346, 0.5718, 0.5190, 0.4140], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:37:27,574] Trial 33 finished with value: 1.5999999999999999 and parameters: {'nsteps_train': 14000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.2, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.3955],
        [-0.1713],
        [-0.1690],
        [-0.3118],
        [-0.6173]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.3256],
        [ 0.5842],
        [-0.3360],
        [-0.8494],
        [ 0.9247]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4988, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1410, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.3680, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8314, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7449, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.4395, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.9811, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(2.0761, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.1665, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4610, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.3802, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.8211, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.7842, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.5016, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(2.1770, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.3210, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(3.6611, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.4923, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.8367, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.8930, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0593],
        [ 0.8400],
        [-0.2919],
        [ 0.3351],
        [ 0.7160]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4988, 1.1410, 2.3680, 1.8314, 0.7449], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:37:35,056] Trial 34 finished with value: 1.5999999999999999 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.4, 'lr_decay_percentage': 0.9, 'minibatch_size': 64}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9973],
        [ 0.3101],
        [ 0.5367],
        [-0.2366],
        [ 0.4070]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5888],
        [ 0.8352],
        [ 0.8062],
        [ 0.8272],
        [ 0.9843]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.8575, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.7098, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9308, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8447, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7630, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7065, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.8723, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.8360, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.8929, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.7305, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5556, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.0348, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7412, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.9411, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6981, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.4047, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.1974, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6463, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.9893, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.6656, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1509],
        [ 0.1625],
        [-0.0948],
        [ 0.0482],
        [-0.0325]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.8575, 0.7098, 0.9308, 0.8447, 0.7630], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:38:01,232] Trial 35 finished with value: 1.5999999999999999 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.05, 'epsilon_decay_percentage': 1.4, 'lr_decay_percentage': 1, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2964],
        [-0.6048],
        [ 0.8635],
        [-0.7938],
        [-0.5455]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3185],
        [-0.4483],
        [ 0.2008],
        [ 0.5791],
        [-0.7271]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6524, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5871, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5932, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6007, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6393, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5003, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6112, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5489, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6731, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5330, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3482, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6352, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5045, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.7454, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4268, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1961, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6593, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4601, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.8178, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3205, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1521],
        [ 0.0241],
        [-0.0444],
        [ 0.0724],
        [-0.1062]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6524, 0.5871, 0.5932, 0.6007, 0.6393], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:38:06,680] Trial 36 finished with value: 0.5 and parameters: {'nsteps_train': 12000, 'lr_begin': 0.005, 'epsilon_decay_percentage': 1.2, 'lr_decay_percentage': 0.6, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4742],
        [ 0.1568],
        [-0.7136],
        [-0.0737],
        [ 0.6337]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2785],
        [-0.1404],
        [-0.0900],
        [ 0.4974],
        [ 0.1782]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4321, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0226, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5511, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3769, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2802, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.2701, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.4476, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3380, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.3564, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3187, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.1080, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.8727, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.1248, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.3360, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.3572, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.9460, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.2977, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.9117, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.3155, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.3957, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1620],
        [ 0.4251],
        [-0.2131],
        [-0.0204],
        [ 0.0385]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4321, 1.0226, 1.5511, 1.3769, 1.2802], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:38:34,512] Trial 37 finished with value: 1.4000000000000001 and parameters: {'nsteps_train': 14000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 0.8, 'lr_decay_percentage': 0.7, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9723],
        [-0.2746],
        [ 0.2116],
        [ 0.4011],
        [ 0.9468]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2810],
        [-0.8934],
        [ 0.2286],
        [-0.6248],
        [ 0.8503]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5978, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4844, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6067, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6280, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6052, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4622, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5223, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5699, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6348, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4771, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3265, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5602, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5331, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6416, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3489, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1908, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5982, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4963, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6485, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2207, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1357],
        [ 0.0379],
        [-0.0368],
        [ 0.0068],
        [-0.1282]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5978, 0.4844, 0.6067, 0.6280, 0.6052], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:38:47,800] Trial 38 finished with value: -0.5 and parameters: {'nsteps_train': 12000, 'lr_begin': 0.05, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.9, 'minibatch_size': 128}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5113],
        [-0.5531],
        [-0.6679],
        [ 0.5189],
        [ 0.5437]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8431],
        [ 0.2485],
        [-0.5272],
        [-0.3567],
        [-0.2398]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.2162, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2198, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4746, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.2577, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.0867, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.1291, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5340, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5421, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4637, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.2938, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.0421, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.8482, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6096, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6697, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5008, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(-0.0450, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.1624, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6771, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.8756, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.7079, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0870],
        [ 0.3142],
        [ 0.0675],
        [ 0.2060],
        [ 0.2071]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.2162, 0.2198, 0.4746, 0.2577, 0.0867], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:38:52,665] Trial 39 finished with value: 4.0 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8987],
        [-0.9006],
        [ 0.6607],
        [ 0.7805],
        [-0.5530]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2554],
        [ 0.6093],
        [ 0.9152],
        [-0.0552],
        [-0.0530]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5870, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.8059, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.0575, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8910, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1298, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7245, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.0039, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(2.0812, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.8624, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.5782, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.8619, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(3.2020, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(2.1050, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.8338, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(2.0267, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.9994, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(4.4000, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(2.1287, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(3.8052, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.4751, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[0.1374],
        [1.1980],
        [0.0237],
        [0.9714],
        [0.4484]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5870, 0.8059, 2.0575, 0.8910, 1.1298], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:38:57,260] Trial 40 finished with value: 4.0 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.3858],
        [ 0.2647],
        [-0.8951],
        [-0.5921],
        [ 0.9634]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0501],
        [-0.4443],
        [-0.9949],
        [-0.8343],
        [-0.9437]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7194, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3891, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6903, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4701, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7108, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5740, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6026, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5220, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5532, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5184, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4286, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.8161, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3537, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6363, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3260, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2832, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.0296, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.1854, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7194, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1335, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1454],
        [ 0.2135],
        [-0.1683],
        [ 0.0831],
        [-0.1924]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7194, 0.3891, 0.6903, 0.4701, 0.7108], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:39:01,858] Trial 41 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4132],
        [-0.5905],
        [-0.8528],
        [ 0.5759],
        [ 0.2795]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9838],
        [ 0.3796],
        [ 0.7513],
        [ 0.7847],
        [-0.1383]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3606, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3015, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6434, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3103, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6867, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6081, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6387, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.5656, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5023, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.9999, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.8557, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.9758, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.4878, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.6942, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.3132, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.1032, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.3130, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.4100, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.8862, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.6264, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.2475],
        [ 0.3372],
        [-0.0778],
        [ 0.1920],
        [ 0.3132]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3606, 1.3015, 1.6434, 1.3103, 0.6867], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:39:06,483] Trial 42 finished with value: 1.4000000000000001 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4114],
        [ 0.2406],
        [-0.1704],
        [-0.3456],
        [ 0.7101]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5717],
        [ 0.9042],
        [ 0.8090],
        [ 0.6890],
        [ 0.6973]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.2795, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5126, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6491, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.9081, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7596, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5033, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5674, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6983, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.8884, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.7225, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7271, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6222, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7475, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.8687, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6854, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.9509, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6770, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7966, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.8490, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.6483, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.2238],
        [ 0.0548],
        [ 0.0492],
        [-0.0197],
        [-0.0371]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.2795, 0.5126, 0.6491, 0.9081, 0.7596], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:39:11,076] Trial 43 finished with value: -0.1 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7359],
        [ 0.5179],
        [ 0.6719],
        [-0.4176],
        [-0.2865]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.8527],
        [-0.7160],
        [-0.1438],
        [ 0.8769],
        [ 0.1653]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7625, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0354, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5280, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6215, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5597, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9326, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.2439, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3388, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.0031, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.9465, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.1026, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.4523, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.1495, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.3847, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.3332, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.2727, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.6607, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.9603, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.7663, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.7200, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.1700],
        [ 0.2084],
        [-0.1892],
        [ 0.3816],
        [ 0.3868]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7625, 1.0354, 1.5280, 0.6215, 0.5597], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:39:15,673] Trial 44 finished with value: 4.0 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1627],
        [-0.1783],
        [ 0.1032],
        [ 0.3288],
        [-0.8796]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9469],
        [-0.6182],
        [ 0.4285],
        [-0.3185],
        [ 0.5058]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.4738, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2113, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4106, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.2911, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.3136, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.2648, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2890, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4168, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4394, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.3133, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.0558, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3668, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4231, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5877, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3131, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(-0.1532, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4445, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4293, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7359, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3128, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2090],
        [ 0.0777],
        [ 0.0062],
        [ 0.1483],
        [-0.0003]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.4738, 0.2113, 0.4106, 0.2911, 0.3136], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:39:20,284] Trial 45 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4223],
        [ 0.0086],
        [ 0.4777],
        [ 0.4710],
        [ 0.5927]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1184],
        [ 0.4505],
        [-0.2259],
        [-0.0607],
        [ 0.9085]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.8973, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6122, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5907, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7602, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5241, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7951, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5881, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.5813, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5566, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0881, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6928, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.5640, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.5719, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.3529, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.6522, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.5906, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(3.5399, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.5626, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(3.1493, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.2162, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1022],
        [ 0.9759],
        [-0.0094],
        [ 0.7964],
        [ 0.5641]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.8973, 0.6122, 1.5907, 0.7602, 0.5241], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:39:25,057] Trial 46 finished with value: 1.4000000000000001 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1468],
        [-0.7529],
        [ 0.7473],
        [-0.1624],
        [ 0.9420]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2344],
        [-0.5066],
        [-0.5843],
        [-0.8498],
        [ 0.0029]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.2517, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2977, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4573, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4688, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.4473, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.2665, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3658, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4244, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4213, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.3777, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2814, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4339, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3915, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.3739, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3081, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2962, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5020, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3586, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.3265, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2385, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  1.7
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0148],
        [ 0.0681],
        [-0.0329],
        [-0.0474],
        [-0.0696]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.2517, 0.2977, 0.4573, 0.4688, 0.4473], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:39:29,829] Trial 47 finished with value: 1.7 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5719],
        [ 0.1132],
        [ 0.7816],
        [ 0.8371],
        [-0.6519]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3712],
        [ 0.6967],
        [-0.4204],
        [ 0.6007],
        [ 0.2313]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3729, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.0797, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.0206, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7339, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5353, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5294, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2512, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.2670, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6989, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5606, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6860, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4226, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5133, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6639, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5859, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.8425, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5940, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7597, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6289, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.6111, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.1565],
        [ 0.1714],
        [ 0.2463],
        [-0.0350],
        [ 0.0253]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3729, 0.0797, 0.0206, 0.7339, 0.5353], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:39:34,558] Trial 48 finished with value: -0.1 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5109],
        [ 0.4301],
        [ 0.9532],
        [-0.6699],
        [-0.7318]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1662],
        [ 0.6997],
        [-0.7507],
        [ 0.6327],
        [-0.0855]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6204, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3106, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4737, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4828, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6036, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5101, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3276, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4519, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4111, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4798, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3997, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3445, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4300, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.3394, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3560, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2894, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3615, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4082, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.2677, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2322, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1103],
        [ 0.0170],
        [-0.0218],
        [-0.0717],
        [-0.1238]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6204, 0.3106, 0.4737, 0.4828, 0.6036], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:39:39,257] Trial 49 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1886],
        [-0.3950],
        [-0.5288],
        [ 0.2447],
        [-0.8658]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5915],
        [ 0.9051],
        [-0.6736],
        [-0.5088],
        [ 0.5799]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2888, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.7948, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7953, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7922, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8799, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.1124, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6686, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.7050, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.3631, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2437, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.9359, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.5424, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.6146, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.9341, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.6075, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7594, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(3.4162, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.5243, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.5050, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.9712, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1765],
        [ 0.8738],
        [-0.0903],
        [ 0.5709],
        [ 0.3638]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2888, 0.7948, 1.7953, 0.7922, 0.8799], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:39:43,982] Trial 50 finished with value: 4.0 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5549],
        [-0.0982],
        [-0.5830],
        [-0.0883],
        [ 0.6704]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3512],
        [ 0.7177],
        [ 0.8298],
        [ 0.7504],
        [ 0.3986]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.0254, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3746, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5802, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4583, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.2167, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0291, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7703, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4640, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5281, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6547, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.0329, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.1660, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.3478, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.5979, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0927, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.0367, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.5616, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.2316, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.6676, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.5307, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  3  Reward Received:  1.0
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0038],
        [ 0.3957],
        [-0.1162],
        [ 0.0698],
        [ 0.4380]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.0254, 0.3746, 1.5802, 1.4583, 0.2167], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:39:48,700] Trial 51 finished with value: 0.7000000000000001 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[0.5139],
        [0.3431],
        [0.6831],
        [0.0705],
        [0.2258]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0434],
        [-0.0285],
        [-0.5053],
        [ 0.8060],
        [-0.9511]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.1824, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2800, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1050, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.0843, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1582, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0063, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6538, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.0874, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.1124, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.9689, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.8302, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.0276, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0698, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.1406, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.7796, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6541, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.4014, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.0522, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.1687, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5903, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1761],
        [ 0.3738],
        [-0.0176],
        [ 0.0282],
        [-0.1893]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.1824, 0.2800, 1.1050, 1.0843, 1.1582], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:39:53,579] Trial 52 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7192],
        [ 0.8074],
        [ 0.3073],
        [-0.9173],
        [-0.8762]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0169],
        [ 0.2549],
        [-0.9509],
        [-0.7818],
        [-0.3848]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6678, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3866, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6548, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6405, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5836, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4677, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5088, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5252, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6306, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5231, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2677, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6311, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3956, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6208, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4627, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.0676, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7533, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2660, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6110, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4023, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2001],
        [ 0.1222],
        [-0.1296],
        [-0.0098],
        [-0.0604]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6678, 0.3866, 0.6548, 0.6405, 0.5836], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:39:58,169] Trial 53 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1310],
        [-0.8890],
        [-0.3098],
        [ 0.6479],
        [ 0.4963]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2708],
        [-0.1520],
        [ 0.0672],
        [-0.9808],
        [ 0.6006]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.2282, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.1741, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6697, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8892, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5925, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.2633, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5284, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6566, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.9156, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6745, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2984, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.8826, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6435, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.9420, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.7566, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3335, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.2369, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6305, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.9684, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.8386, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0351],
        [ 0.3543],
        [-0.0131],
        [ 0.0264],
        [ 0.0820]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.2282, 0.1741, 0.6697, 0.8892, 0.5925], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:40:02,878] Trial 54 finished with value: -0.7000000000000001 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.5, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.3493],
        [ 0.1528],
        [-0.3367],
        [ 0.6967],
        [ 0.5528]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1168],
        [-0.9787],
        [-0.0587],
        [ 0.3202],
        [-0.8408]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.8925, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2524, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.7904, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7529, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8930, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6903, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3210, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6915, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.7533, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6958, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4882, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3897, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5926, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.7536, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4986, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2860, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4584, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4938, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7540, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3015, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2022],
        [ 0.0687],
        [-0.0989],
        [ 0.0004],
        [-0.1972]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.8925, 0.2524, 0.7904, 0.7529, 0.8930], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:40:07,593] Trial 55 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.3, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.0023],
        [ 0.4426],
        [ 0.7085],
        [-0.8666],
        [-0.3333]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2687],
        [-0.4067],
        [ 0.1549],
        [ 0.7879],
        [ 0.6175]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.4871, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5632, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6542, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.3619, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.1986, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4442, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6300, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.7077, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5830, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4363, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4014, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6968, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7612, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.8041, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6740, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3585, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7636, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.8146, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.0251, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.9117, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  3  Reward Received:  1.0
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0428],
        [ 0.0668],
        [ 0.0535],
        [ 0.2211],
        [ 0.2377]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.4871, 0.5632, 0.6542, 0.3619, 0.1986], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:40:12,290] Trial 56 finished with value: 0.7000000000000001 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.5432],
        [ 0.4839],
        [ 0.7356],
        [-0.9143],
        [ 0.5886]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5235],
        [ 0.9860],
        [-0.7714],
        [-0.4824],
        [-0.6540]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.8175, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.8686, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5353, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1618, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5546, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8092, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.3416, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4253, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4072, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.7813, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.8008, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.8147, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.3152, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.6525, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0079, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7925, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.2877, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.2052, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.8978, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.2345, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0083],
        [ 0.4730],
        [-0.1100],
        [ 0.2453],
        [ 0.2266]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.8175, 0.8686, 1.5353, 1.1618, 0.5546], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:40:17,015] Trial 57 finished with value: 4.0 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.6, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1343],
        [-0.7745],
        [ 0.3444],
        [ 0.3627],
        [-0.3331]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7787],
        [ 0.1209],
        [-0.2367],
        [ 0.1666],
        [-0.1318]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5539, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5220, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4688, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6807, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6932, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6156, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5510, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.5806, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4234, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.1530, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6773, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.5800, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.6923, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.1660, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.6128, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7390, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(3.6091, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.8041, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.9086, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.0726, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[0.0617],
        [1.0290],
        [0.1118],
        [0.7426],
        [0.4598]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5539, 0.5220, 1.4688, 0.6807, 0.6932], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:40:22,273] Trial 58 finished with value: 4.0 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.6, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.5650],
        [ 0.4259],
        [-0.8936],
        [-0.9209],
        [-0.1984]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9630],
        [-0.1811],
        [-0.9245],
        [-0.0832],
        [-0.3854]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6396, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4617, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5829, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4443, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6379, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5429, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4702, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4970, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4588, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4657, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4462, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4787, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4111, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4733, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2935, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3495, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4872, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3253, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4879, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1212, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0967],
        [ 0.0085],
        [-0.0859],
        [ 0.0145],
        [-0.1722]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6396, 0.4617, 0.5829, 0.4443, 0.6379], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:40:26,982] Trial 59 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.005, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.6, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.2656],
        [ 0.5879],
        [-0.6102],
        [-0.9553],
        [ 0.6273]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0690],
        [-0.4117],
        [ 0.8643],
        [-0.3477],
        [-0.6869]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4492, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.8943, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7495, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2263, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8058, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.1928, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6006, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.5801, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5910, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0662, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.9365, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.3068, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.4107, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.9557, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.3266, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6801, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(3.0131, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.2413, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.3204, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.5869, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2564],
        [ 0.7063],
        [-0.1694],
        [ 0.3647],
        [ 0.2604]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4492, 0.8943, 1.7495, 1.2263, 0.8058], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:40:43,892] Trial 60 finished with value: 1.4000000000000001 and parameters: {'nsteps_train': 15000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.6, 'minibatch_size': 128}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1450],
        [ 0.6109],
        [-0.7057],
        [ 0.7591],
        [-0.0991]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0467],
        [-0.9597],
        [ 0.4156],
        [ 0.5141],
        [-0.4902]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6275, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4455, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4062, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4135, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.4977, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5133, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4485, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3823, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.3813, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.3586, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3992, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4515, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3585, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.3491, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2195, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2851, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4544, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3347, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.3170, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.0804, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1141],
        [ 0.0030],
        [-0.0238],
        [-0.0322],
        [-0.1391]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6275, 0.4455, 0.4062, 0.4135, 0.4977], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:40:49,976] Trial 61 finished with value: 0.5 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.6, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.4062],
        [-0.4813],
        [-0.4459],
        [-0.9206],
        [ 0.4482]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0271],
        [ 0.7959],
        [-0.0624],
        [ 0.7324],
        [ 0.0351]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5550, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3493, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5718, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4374, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5534, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4356, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6185, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6760, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6813, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5660, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3163, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.8878, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7801, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.9253, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5786, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1969, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.1570, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.8843, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.1693, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5912, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  3  Reward Received:  1.0
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1193],
        [ 0.2692],
        [ 0.1041],
        [ 0.2440],
        [ 0.0126]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5550, 0.3493, 0.5718, 0.4374, 0.5534], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:40:55,261] Trial 62 finished with value: 0.7000000000000001 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.6, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8259],
        [ 0.0229],
        [-0.9057],
        [ 0.0973],
        [ 0.3417]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5273],
        [ 0.1390],
        [-0.7938],
        [ 0.6671],
        [-0.5490]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.4709, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.1775, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.2638, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6136, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.1999, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5363, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2646, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3790, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5667, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.3037, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6017, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3516, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4943, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5199, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4076, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6670, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4387, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6095, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4730, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5114, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0654],
        [ 0.0870],
        [ 0.1152],
        [-0.0469],
        [ 0.1039]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.4709, 0.1775, 0.2638, 0.6136, 0.1999], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:41:00,322] Trial 63 finished with value: -0.1 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.7, 'lr_decay_percentage': 0.6, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.9896],
        [-0.4771],
        [ 0.4934],
        [ 0.0507],
        [ 0.9202]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9944],
        [ 0.6764],
        [ 0.2896],
        [ 0.3219],
        [ 0.2311]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3295, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4868, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.2513, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5529, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.4029, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3366, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4746, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3333, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5394, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.3734, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3437, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4623, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4152, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5258, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3438, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3508, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4501, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4972, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5123, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3143, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0071],
        [-0.0122],
        [ 0.0820],
        [-0.0135],
        [-0.0296]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3295, 0.4868, 0.2513, 0.5529, 0.4029], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:41:06,915] Trial 64 finished with value: -0.5 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.6, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.4847],
        [-0.4500],
        [-0.4170],
        [-0.7350],
        [ 0.9020]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9710],
        [ 0.7261],
        [ 0.5538],
        [-0.9256],
        [ 0.8060]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6166, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0778, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.2409, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7104, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8418, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9493, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.0932, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(2.1279, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.2222, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4839, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.2819, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(3.1087, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(2.0150, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.7339, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(2.1260, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.6146, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(4.1242, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.9021, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.2457, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.7681, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.3327],
        [ 1.0155],
        [-0.1129],
        [ 0.5118],
        [ 0.6421]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6166, 1.0778, 2.2409, 0.7104, 0.8418], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:41:11,896] Trial 65 finished with value: 4.0 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.4024],
        [-0.2248],
        [ 0.1675],
        [ 0.9719],
        [ 0.8904]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[0.9215],
        [0.3604],
        [0.0511],
        [0.8409],
        [0.8700]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.8413, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.7208, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.8510, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.9650, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8042, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7118, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7997, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.8342, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.9410, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.7873, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5822, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.8786, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.8174, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.9170, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.7703, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.4527, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.9576, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.8005, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.8930, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.7534, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1296],
        [ 0.0789],
        [-0.0168],
        [-0.0240],
        [-0.0169]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.8413, 0.7208, 0.8510, 0.9650, 0.8042], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:41:16,772] Trial 66 finished with value: -0.7000000000000001 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.05, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.6, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.3360],
        [ 0.6629],
        [-0.1262],
        [-0.5040],
        [-0.3119]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6500],
        [-0.6365],
        [-0.7367],
        [-0.7108],
        [ 0.0232]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.1231, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3169, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5021, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4633, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.3617, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.1451, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3835, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4553, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4773, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.3414, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.1670, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4501, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4085, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4914, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3211, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1889, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5166, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3616, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5054, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3008, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  3  Reward Received:  1.0
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0219],
        [ 0.0666],
        [-0.0468],
        [ 0.0140],
        [-0.0203]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.1231, 0.3169, 0.5021, 0.4633, 0.3617], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:41:22,079] Trial 67 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.6, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1769],
        [-0.6777],
        [-0.7295],
        [ 0.7695],
        [ 0.3439]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9251],
        [-0.3153],
        [ 0.2039],
        [-0.3098],
        [-0.2516]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3911, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3498, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4189, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4250, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5325, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3179, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3600, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3841, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4266, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4168, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2447, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3702, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3493, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4281, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3011, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1715, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3804, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3145, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4297, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1855, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0732],
        [ 0.0102],
        [-0.0348],
        [ 0.0016],
        [-0.1157]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3911, 0.3498, 0.4189, 0.4250, 0.5325], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:41:37,012] Trial 68 finished with value: 0.5 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.2, 'lr_decay_percentage': 1, 'minibatch_size': 128}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.4397],
        [-0.8817],
        [ 0.7909],
        [ 0.6207],
        [-0.3251]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.3471],
        [-0.0942],
        [ 0.2860],
        [-0.1620],
        [ 0.0501]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3068, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0716, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.9767, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4185, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6885, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.3425, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6242, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.6442, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6340, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.6082, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.3783, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.1767, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.3118, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.8494, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.5279, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.4140, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.7293, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.9793, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.0649, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.4477, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0357],
        [ 0.5526],
        [-0.3325],
        [ 0.2154],
        [-0.0803]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3068, 1.0716, 1.9767, 1.4185, 1.6885], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:42:07,882] Trial 69 finished with value: 4.0 and parameters: {'nsteps_train': 15000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.9, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.3363],
        [-0.6107],
        [-0.1682],
        [-0.5380],
        [-0.5945]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6400],
        [ 0.6969],
        [ 0.5438],
        [ 0.5440],
        [-0.6564]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6760, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5334, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6498, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5965, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6862, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5381, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6060, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5968, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6291, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5647, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4002, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6786, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5439, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6617, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4433, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2623, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7512, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4910, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6943, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3218, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1379],
        [ 0.0726],
        [-0.0529],
        [ 0.0326],
        [-0.1215]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6760, 0.5334, 0.6498, 0.5965, 0.6862], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:42:12,943] Trial 70 finished with value: 0.5 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.005, 'epsilon_decay_percentage': 0.8, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4076],
        [ 0.2468],
        [-0.7598],
        [ 0.8403],
        [ 0.6154]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1501],
        [-0.8578],
        [ 0.3786],
        [-0.1975],
        [-0.4585]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6095, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4455, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.2711, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5313, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6298, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5173, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4251, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3343, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5659, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5088, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4252, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4048, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3976, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6004, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3878, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3330, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3844, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4609, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6349, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2669, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0922],
        [-0.0204],
        [ 0.0633],
        [ 0.0345],
        [-0.1210]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6095, 0.4455, 0.2711, 0.5313, 0.6298], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:42:18,030] Trial 71 finished with value: 0.5 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1877],
        [-0.6906],
        [-0.5047],
        [ 0.8616],
        [ 0.0221]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.5060],
        [-0.7654],
        [ 0.6200],
        [-0.0529],
        [-0.2627]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5142, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2810, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3707, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.3835, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.3818, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4379, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2817, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3050, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.3086, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.2842, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3615, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.2824, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.2392, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.2337, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.1865, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2852, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.2831, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.1735, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.1588, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.0888, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0764],
        [ 0.0007],
        [-0.0657],
        [-0.0749],
        [-0.0977]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5142, 0.2810, 0.3707, 0.3835, 0.3818], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:42:23,366] Trial 72 finished with value: 0.5 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.3062],
        [-0.0773],
        [ 0.3151],
        [-0.3023],
        [ 0.7527]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[0.4619],
        [0.1109],
        [0.1808],
        [0.9608],
        [0.8524]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5486, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.8934, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6249, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5223, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6352, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7006, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.4058, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4346, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.0139, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0568, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.8525, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.9182, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2442, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.5056, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.4784, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.0045, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.4306, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.0538, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.9973, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.9000, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.1520],
        [ 0.5124],
        [-0.1904],
        [ 0.4917],
        [ 0.4216]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5486, 0.8934, 1.6249, 0.5223, 0.6352], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:42:28,435] Trial 73 finished with value: 4.0 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0813],
        [-0.4117],
        [ 0.7227],
        [ 0.5281],
        [-0.2051]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9474],
        [ 0.6888],
        [ 0.8577],
        [-0.3848],
        [-0.3413]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7881, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5412, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.7249, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7148, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8044, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5071, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.8016, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.8125, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.9932, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6667, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2262, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.0620, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.9001, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.2716, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5291, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(-0.0547, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.3224, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.9877, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.5500, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3915, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2809],
        [ 0.2604],
        [ 0.0876],
        [ 0.2784],
        [-0.1376]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7881, 0.5412, 0.7249, 0.7148, 0.8044], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:42:34,466] Trial 74 finished with value: 0.5 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.2980],
        [ 0.2446],
        [ 0.5746],
        [-0.1787],
        [ 0.7526]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1945],
        [ 0.6382],
        [ 0.8604],
        [-0.8653],
        [ 0.6087]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5002, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6223, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2688, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5046, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7249, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5790, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.3021, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3025, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6921, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.9285, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6577, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.9820, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.3362, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.8795, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.1321, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7364, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.6618, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.3699, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.0669, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.3358, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[0.0787],
        [0.6798],
        [0.0337],
        [0.1874],
        [0.2036]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5002, 0.6223, 1.2688, 0.5046, 0.7249], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:42:39,191] Trial 75 finished with value: 4.0 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.5, 'lr_decay_percentage': 1, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.2906],
        [-0.9843],
        [ 0.7715],
        [-0.0433],
        [ 0.5046]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7880],
        [-0.2780],
        [ 0.1033],
        [-0.3025],
        [-0.1791]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3979, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2750, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4188, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.3893, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5067, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4106, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3304, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3982, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4397, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4274, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4233, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3858, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3776, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4901, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3482, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.4360, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4412, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3570, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5405, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2689, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0127],
        [ 0.0554],
        [-0.0206],
        [ 0.0504],
        [-0.0793]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3979, 0.2750, 0.4188, 0.3893, 0.5067], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:43:06,055] Trial 76 finished with value: 0.5 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1, 'lr_decay_percentage': 0.6, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.9942],
        [-0.6632],
        [ 0.1212],
        [-0.8363],
        [ 0.9956]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5672],
        [-0.9291],
        [-0.0805],
        [ 0.1186],
        [ 0.5267]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.8712, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5461, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6914, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7192, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7834, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7528, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5869, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6350, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.7399, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6869, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6344, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6276, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5786, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.7606, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5905, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.5160, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6683, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5222, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7813, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4941, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1184],
        [ 0.0407],
        [-0.0564],
        [ 0.0207],
        [-0.0964]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.8712, 0.5461, 0.6914, 0.7192, 0.7834], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:43:10,763] Trial 77 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.05, 'epsilon_decay_percentage': 1.2, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.6852],
        [-0.0377],
        [-0.7630],
        [-0.2169],
        [-0.8814]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6014],
        [-0.5941],
        [-0.4999],
        [-0.1511],
        [-0.1770]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5754, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2172, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4346, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5373, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5021, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4297, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4503, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4475, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6045, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4377, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2840, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6835, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4605, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6716, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3734, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1383, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.9166, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4735, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7388, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3090, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1457],
        [ 0.2331],
        [ 0.0130],
        [ 0.0672],
        [-0.0644]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5754, 0.2172, 0.4346, 0.5373, 0.5021], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:43:15,871] Trial 78 finished with value: 0.5 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.3, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.5042],
        [-0.3233],
        [ 0.5324],
        [-0.4796],
        [ 0.6438]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1678],
        [-0.5923],
        [-0.0203],
        [-0.1151],
        [ 0.7890]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3591, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6898, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4844, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6157, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7101, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4362, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6304, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4971, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5736, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5982, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5132, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5710, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5099, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5315, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4863, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.5903, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5116, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5226, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4895, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3744, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0770],
        [-0.0594],
        [ 0.0127],
        [-0.0421],
        [-0.1119]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3591, 0.6898, 0.4844, 0.6157, 0.7101], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:43:23,524] Trial 79 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 1, 'minibatch_size': 64}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8468],
        [-0.9504],
        [ 0.1299],
        [ 0.8733],
        [ 0.5172]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.7442],
        [ 0.7101],
        [-0.6978],
        [ 0.3135],
        [-0.7160]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.9947, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.9715, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.7100, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1589, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2763, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.2453, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.2366, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(2.2463, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.2176, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.7349, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.4958, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(3.5017, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.7826, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(3.2763, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(2.1934, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.7464, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(4.7668, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.3189, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(4.3349, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.6520, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.2506],
        [ 1.2651],
        [-0.4637],
        [ 1.0587],
        [ 0.4586]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.9947, 0.9715, 2.7100, 1.1589, 1.2763], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:43:50,539] Trial 80 finished with value: 4.0 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.7, 'lr_decay_percentage': 0.8, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.6773],
        [ 0.9636],
        [ 0.1195],
        [ 0.7719],
        [ 0.6216]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.8663],
        [ 0.8440],
        [ 0.5881],
        [ 0.8388],
        [-0.2045]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.4849, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2603, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3623, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.3222, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5296, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3646, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2699, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.2879, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.2936, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.3310, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2444, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.2794, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.2135, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.2650, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.1324, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1242, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.2889, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.1392, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.2364, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(-0.0661, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1202],
        [ 0.0095],
        [-0.0744],
        [-0.0286],
        [-0.1986]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.4849, 0.2603, 0.3623, 0.3222, 0.5296], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:44:21,112] Trial 81 finished with value: 0.5 and parameters: {'nsteps_train': 15000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.9, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.7582],
        [-0.5702],
        [-0.7383],
        [ 0.9248],
        [ 0.7000]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9712],
        [ 0.6265],
        [-0.5269],
        [-0.5386],
        [-0.6622]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5457, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5296, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4060, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5427, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5030, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4080, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6160, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4888, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6112, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4878, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2703, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7024, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5716, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6798, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4726, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1325, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7888, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6544, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7483, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4574, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1377],
        [ 0.0864],
        [ 0.0828],
        [ 0.0686],
        [-0.0152]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5457, 0.5296, 0.4060, 0.5427, 0.5030], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:44:51,728] Trial 82 finished with value: 0.5 and parameters: {'nsteps_train': 15000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.9, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.3336],
        [-0.6618],
        [ 0.5845],
        [ 0.7503],
        [ 0.8667]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.7295],
        [ 0.0281],
        [-0.5719],
        [-0.0590],
        [ 0.9888]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6173, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3723, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9901, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1050, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8131, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5359, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5730, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.9024, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.0466, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.7604, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4546, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7738, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.8146, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.9882, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.7077, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3732, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.9746, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7269, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.9299, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.6550, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0814],
        [ 0.2008],
        [-0.0877],
        [-0.0584],
        [-0.0527]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6173, 0.3723, 0.9901, 1.1050, 0.8131], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:45:22,509] Trial 83 finished with value: -0.7000000000000001 and parameters: {'nsteps_train': 15000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.9, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.0828],
        [ 0.2420],
        [-0.8118],
        [-0.9961],
        [ 0.1027]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2398],
        [ 0.9075],
        [ 0.1955],
        [-0.7307],
        [ 0.1006]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5278, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4102, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3562, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.2315, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.4573, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4475, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3890, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3263, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.2228, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.3387, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3672, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3677, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.2963, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.2141, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2200, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2869, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3465, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2664, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.2054, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1014, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0803],
        [-0.0212],
        [-0.0299],
        [-0.0087],
        [-0.1186]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5278, 0.4102, 0.3562, 0.2315, 0.4573], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:45:53,118] Trial 84 finished with value: 0.5 and parameters: {'nsteps_train': 15000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.9, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.4669],
        [ 0.7397],
        [-0.4167],
        [ 0.7641],
        [ 0.1825]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1443],
        [-0.5176],
        [ 0.5517],
        [-0.1589],
        [ 0.5394]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3202, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2778, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3311, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.2731, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.4895, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3059, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2884, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.2900, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.2710, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.3551, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2916, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.2991, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.2490, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.2689, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2207, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2774, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3097, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2080, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.2669, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.0863, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0143],
        [ 0.0106],
        [-0.0410],
        [-0.0021],
        [-0.1344]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3202, 0.2778, 0.3311, 0.2731, 0.4895], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:46:13,725] Trial 85 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.9, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8235],
        [-0.5859],
        [-0.9052],
        [-0.7119],
        [ 0.9850]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6214],
        [ 0.1876],
        [ 0.7458],
        [ 0.8240],
        [-0.0457]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.4948, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5377, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3273, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5587, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.4585, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4499, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4952, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3942, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5488, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4118, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4050, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4527, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4610, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5389, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3650, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3601, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4102, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5278, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5290, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3183, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0449],
        [-0.0425],
        [ 0.0668],
        [-0.0099],
        [-0.0468]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.4948, 0.5377, 0.3273, 0.5587, 0.4585], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:46:21,109] Trial 86 finished with value: -0.5 and parameters: {'nsteps_train': 15000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.2, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2789],
        [ 0.5890],
        [ 0.4906],
        [ 0.0160],
        [-0.7878]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2059],
        [ 0.4731],
        [-0.5179],
        [-0.1839],
        [ 0.1027]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5650, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4012, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4079, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4349, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5846, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3942, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4309, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3680, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4461, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4562, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2234, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4605, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3282, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4573, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3277, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.0525, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4902, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2883, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4685, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1993, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1708],
        [ 0.0297],
        [-0.0399],
        [ 0.0112],
        [-0.1284]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5650, 0.4012, 0.4079, 0.4349, 0.5846], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:46:37,053] Trial 87 finished with value: 0.5 and parameters: {'nsteps_train': 14000, 'lr_begin': 0.005, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.5, 'minibatch_size': 128}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.7864],
        [-0.7359],
        [-0.7549],
        [ 0.5237],
        [ 0.1639]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2001],
        [ 0.1070],
        [ 0.6305],
        [ 0.3087],
        [ 0.1133]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5418, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3762, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3756, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.3570, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.3062, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4390, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3484, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3447, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.3590, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.2659, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3362, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3206, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3137, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.3611, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2256, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2334, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.2929, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2828, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.3631, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1852, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1028],
        [-0.0278],
        [-0.0309],
        [ 0.0020],
        [-0.0403]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5418, 0.3762, 0.3756, 0.3570, 0.3062], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:46:41,755] Trial 88 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.3362],
        [ 0.6387],
        [ 0.9438],
        [-0.4057],
        [ 0.7322]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8806],
        [-0.1523],
        [ 0.7130],
        [ 0.8419],
        [-0.5416]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.0305, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.9653, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6260, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.0815, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.3251, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9423, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.8950, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6322, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.9048, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4431, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.8542, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.8246, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6383, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.7281, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5610, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7660, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7543, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6445, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5515, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.6790, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0882],
        [-0.0704],
        [ 0.0062],
        [-0.1767],
        [ 0.1180]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.0305, 0.9653, 0.6260, 1.0815, 0.3251], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:46:46,465] Trial 89 finished with value: -0.1 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.6, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4059],
        [ 0.5282],
        [ 0.6149],
        [-0.7368],
        [ 0.9122]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0446],
        [ 0.2931],
        [-0.3764],
        [-0.7249],
        [ 0.6920]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.8085, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3312, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.4396, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6677, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2583, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.1436, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.0446, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(2.0782, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.1699, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.6734, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.4786, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.7581, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.7168, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.6721, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(2.0885, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.8136, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(3.4716, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.3554, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(3.1743, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.5036, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.3350],
        [ 0.7135],
        [-0.3614],
        [ 0.5022],
        [ 0.4151]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.8085, 1.3312, 2.4396, 1.6677, 1.2583], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:47:09,474] Trial 90 finished with value: 1.5999999999999999 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.8, 'lr_decay_percentage': 1, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.0977],
        [-0.1503],
        [-0.2197],
        [-0.7961],
        [-0.5437]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.5063],
        [-0.1409],
        [ 0.2450],
        [ 0.6584],
        [-0.1644]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3687, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3716, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4853, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5231, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5389, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3317, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5106, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5227, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5923, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5664, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2947, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6497, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5601, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6616, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5940, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2577, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7887, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5975, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7309, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.6216, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0370],
        [ 0.1390],
        [ 0.0374],
        [ 0.0693],
        [ 0.0276]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3687, 0.3716, 0.4853, 0.5231, 0.5389], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:47:14,540] Trial 91 finished with value: 0.5 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.5441],
        [-0.9270],
        [-0.7869],
        [-0.0645],
        [-0.2644]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[0.9938],
        [0.1194],
        [0.6587],
        [0.0677],
        [0.5144]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.0465, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3698, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4662, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.3824, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5207, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.2111, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3487, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4403, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4299, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4456, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3758, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3275, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4145, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4773, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3706, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.5404, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3063, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3886, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5248, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2955, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.1646],
        [-0.0212],
        [-0.0259],
        [ 0.0475],
        [-0.0751]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.0465, 0.3698, 0.4662, 0.3824, 0.5207], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:47:20,337] Trial 92 finished with value: 0.5 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4339],
        [ 0.4944],
        [-0.6959],
        [-0.9846],
        [-0.0721]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2392],
        [ 0.5634],
        [ 0.4086],
        [-0.8069],
        [-0.1940]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.2946, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3473, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.2840, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.3149, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5636, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3083, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5929, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4910, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5034, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5504, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3219, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.8384, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6981, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6919, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5372, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3355, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.0839, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.9051, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.8804, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5240, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0136],
        [ 0.2455],
        [ 0.2070],
        [ 0.1885],
        [-0.0132]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.2946, 0.3473, 0.2840, 0.3149, 0.5636], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:47:25,439] Trial 93 finished with value: 0.5 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9511],
        [ 0.2555],
        [-0.8749],
        [-0.4356],
        [-0.0651]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5299],
        [ 0.3013],
        [ 0.7196],
        [ 0.8467],
        [-0.5153]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.8850, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.1722, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3354, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8882, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6260, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8010, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3470, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4562, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.7877, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6200, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7169, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5218, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5770, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6872, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6141, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6328, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6966, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6978, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5867, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.6081, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  0  Reward Received:  -1.0
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  -1.2000000000000002
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0841],
        [ 0.1748],
        [ 0.1208],
        [-0.1005],
        [-0.0059]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.8850, 0.1722, 0.3354, 0.8882, 0.6260], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:47:30,736] Trial 94 finished with value: -1.2000000000000002 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5009],
        [ 0.5946],
        [ 0.2560],
        [-0.8832],
        [ 0.9597]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.3740],
        [-0.4409],
        [ 0.9709],
        [-0.6906],
        [ 0.7407]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.4497, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.0446, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.1118, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.0106, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.1771, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.2556, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.1563, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.1808, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.1926, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.1928, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.0615, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.2681, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.2499, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.3747, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2085, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(-0.1327, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3798, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3190, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5567, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2242, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1941],
        [ 0.1117],
        [ 0.0691],
        [ 0.1821],
        [ 0.0157]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.4497, 0.0446, 0.1118, 0.0106, 0.1771], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:47:36,046] Trial 95 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5182],
        [ 0.0916],
        [-0.9365],
        [-0.7218],
        [-0.7792]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2603],
        [-0.9066],
        [ 0.8593],
        [ 0.6818],
        [ 0.8162]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2796, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1384, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5225, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2638, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2977, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.1262, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.3646, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3802, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.2136, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2601, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.9729, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.5908, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2378, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.1634, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.2224, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.8195, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.8170, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.0954, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.1132, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.1848, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1534],
        [ 0.2262],
        [-0.1424],
        [-0.0502],
        [-0.0376]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2796, 1.1384, 1.5225, 1.2638, 1.2977], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:47:41,886] Trial 96 finished with value: 4.0 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.05, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1772],
        [-0.9761],
        [ 0.9733],
        [-0.0827],
        [-0.7976]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0447],
        [ 0.1378],
        [ 0.1320],
        [-0.0787],
        [-0.1930]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.2628, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.1000, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.1512, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.2724, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(-0.0458, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.1981, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.1337, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.1740, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.2660, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.0269, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.1334, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.1674, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.1967, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.2596, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.0996, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.0686, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.2012, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2194, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.2532, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1723, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0647],
        [ 0.0337],
        [ 0.0227],
        [-0.0064],
        [ 0.0727]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([ 0.2628,  0.1000,  0.1512,  0.2724, -0.0458], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:47:49,543] Trial 97 finished with value: -0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.5, 'lr_decay_percentage': 0.9, 'minibatch_size': 64}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.6307],
        [ 0.0797],
        [-0.6553],
        [ 0.8575],
        [-0.0980]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6106],
        [-0.2939],
        [-0.6871],
        [ 0.7627],
        [-0.2837]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.9218, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6660, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0960, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1493, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1267, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8541, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7052, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.9516, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.0339, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.9500, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7864, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7445, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.8072, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.9184, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.7733, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7187, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7837, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6628, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.8030, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5965, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0677],
        [ 0.0392],
        [-0.1444],
        [-0.1155],
        [-0.1767]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.9218, 0.6660, 1.0960, 1.1493, 1.1267], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:47:54,702] Trial 98 finished with value: -0.5 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.2, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1683],
        [-0.3097],
        [ 0.3393],
        [ 0.8119],
        [ 0.5890]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.8297],
        [-0.6462],
        [-0.5430],
        [-0.5320],
        [ 0.6734]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.7671, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3989, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.1159, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6205, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6091, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.5249, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.8516, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.8221, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.7693, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.6943, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.2828, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.3043, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.5283, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.9181, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.7795, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.0406, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.7571, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.2345, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.0669, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.8647, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2421],
        [ 0.4527],
        [-0.2938],
        [ 0.1488],
        [ 0.0852]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.7671, 1.3989, 2.1159, 1.6205, 1.6091], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:48:01,322] Trial 99 finished with value: 1.4000000000000001 and parameters: {'nsteps_train': 15000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 1, 'lr_decay_percentage': 0.6, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.4010],
        [ 0.4521],
        [ 0.8122],
        [-0.7072],
        [ 0.9154]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1397],
        [-0.2641],
        [ 0.6587],
        [-0.3502],
        [-0.0468]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6360, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3430, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6860, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.3007, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.0477, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4249, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5707, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6654, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5736, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.3355, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2139, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7984, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6449, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.8466, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6234, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.0028, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.0261, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6243, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.1195, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.9113, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  3  Reward Received:  1.0
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2110],
        [ 0.2277],
        [-0.0206],
        [ 0.2730],
        [ 0.2879]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6360, 0.3430, 0.6860, 0.3007, 0.0477], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:48:06,755] Trial 100 finished with value: 0.7000000000000001 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.3, 'lr_decay_percentage': 0.8, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4061],
        [ 0.2102],
        [-0.5419],
        [-0.9477],
        [ 0.8053]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7360],
        [-0.6696],
        [-0.2000],
        [ 0.2391],
        [ 0.0200]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3017, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3433, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3505, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5230, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.4984, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3037, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3428, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3623, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4900, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.3884, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3057, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3423, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3740, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4569, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2784, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3078, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3419, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3858, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4239, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1684, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0020],
        [-0.0005],
        [ 0.0118],
        [-0.0330],
        [-0.1100]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3017, 0.3433, 0.3505, 0.5230, 0.4984], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:48:11,490] Trial 101 finished with value: -0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.5, 'lr_decay_percentage': 1, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0775],
        [-0.7435],
        [-0.4654],
        [-0.3753],
        [ 0.2899]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.4854],
        [ 0.7288],
        [-0.8202],
        [ 0.8000],
        [-0.4922]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7665, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2272, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4100, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7842, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7947, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7485, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3491, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4984, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6650, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6970, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7305, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4711, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5868, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5459, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5992, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7125, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5931, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6752, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4267, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5015, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0180],
        [ 0.1220],
        [ 0.0884],
        [-0.1192],
        [-0.0977]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7665, 0.2272, 0.4100, 0.7842, 0.7947], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:48:16,190] Trial 102 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.5, 'lr_decay_percentage': 1, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8397],
        [-0.7496],
        [ 0.2618],
        [-0.0034],
        [-0.4751]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9488],
        [ 0.8182],
        [ 0.5017],
        [-0.4752],
        [-0.6221]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.2271, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.0734, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5411, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6564, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7199, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3553, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2696, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5065, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6163, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5950, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4835, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4657, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4720, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5762, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4700, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6117, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6619, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4374, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5362, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3450, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.1282],
        [ 0.1962],
        [-0.0346],
        [-0.0401],
        [-0.1250]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.2271, 0.0734, 0.5411, 0.6564, 0.7199], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:48:21,571] Trial 103 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.5, 'lr_decay_percentage': 1, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2527],
        [ 0.5140],
        [ 0.4126],
        [ 0.0423],
        [-0.2257]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.8968],
        [-0.8570],
        [ 0.7259],
        [ 0.9418],
        [-0.3069]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5584, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.7959, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.7786, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.1485, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7157, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6012, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7692, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.8769, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5836, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.7446, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6440, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7425, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.9752, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.0187, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.7734, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6868, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7158, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.0735, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.4538, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.8023, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  3  Reward Received:  1.0
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  0.6000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0428],
        [-0.0267],
        [ 0.0983],
        [ 0.4351],
        [ 0.0289]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5584, 0.7959, 0.7786, 0.1485, 0.7157], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:48:26,449] Trial 104 finished with value: 0.6000000000000001 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 1, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7656],
        [ 0.4027],
        [ 0.4661],
        [-0.0884],
        [-0.6807]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2384],
        [-0.0987],
        [ 0.1740],
        [ 0.4658],
        [ 0.6455]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.9964, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.8318, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4234, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8438, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8491, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8753, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.3302, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3196, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.1575, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0163, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7542, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.8286, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2158, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.4712, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.1835, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6332, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.3269, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.1119, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.7849, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.3508, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1211],
        [ 0.4984],
        [-0.1038],
        [ 0.3137],
        [ 0.1672]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.9964, 0.8318, 1.4234, 0.8438, 0.8491], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:48:31,183] Trial 105 finished with value: 1.4000000000000001 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.5, 'lr_decay_percentage': 1, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.2107],
        [-0.8809],
        [-0.4806],
        [-0.5450],
        [-0.6222]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1830],
        [ 0.4689],
        [-0.5586],
        [-0.1078],
        [-0.4932]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5295, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3691, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.1255, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.2899, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5260, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4459, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3535, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.1935, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.3212, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.3935, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3622, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3380, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.2616, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.3526, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2610, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2785, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3225, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3296, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.3840, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1285, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0837],
        [-0.0155],
        [ 0.0681],
        [ 0.0314],
        [-0.1325]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5295, 0.3691, 0.1255, 0.2899, 0.5260], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:49:00,015] Trial 106 finished with value: 0.5 and parameters: {'nsteps_train': 14000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.5, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.6006],
        [ 0.0146],
        [-0.8932],
        [ 0.0317],
        [-0.8714]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1800],
        [-0.2866],
        [-0.1310],
        [ 0.8427],
        [-0.6772]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.4506, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5334, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5297, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8097, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8194, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5064, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5809, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5804, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6965, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.7290, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5621, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6283, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6311, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5834, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6385, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6179, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6758, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6817, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4702, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5480, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0558],
        [ 0.0475],
        [ 0.0507],
        [-0.1131],
        [-0.0905]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.4506, 0.5334, 0.5297, 0.8097, 0.8194], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:49:05,671] Trial 107 finished with value: 0.5 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.6, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.6080],
        [-0.7500],
        [ 0.0733],
        [ 0.8247],
        [ 0.7482]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9970],
        [-0.3140],
        [ 0.9935],
        [-0.7667],
        [-0.1486]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5985, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3360, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9319, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.9460, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7159, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4424, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5098, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.8206, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.9237, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.7283, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2864, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6837, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7093, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.9014, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.7407, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1303, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.8575, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5979, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.8791, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.7531, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1560],
        [ 0.1739],
        [-0.1113],
        [-0.0223],
        [ 0.0124]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5985, 0.3360, 0.9319, 0.9460, 0.7159], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:49:20,852] Trial 108 finished with value: -0.5 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.5, 'minibatch_size': 128}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.7118],
        [ 0.2036],
        [ 0.4555],
        [ 0.1214],
        [-0.3959]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0561],
        [-0.5760],
        [-0.9256],
        [ 0.2777],
        [ 0.9222]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5038, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2608, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8620, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5806, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4619, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.3763, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6796, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.6357, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6562, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.5198, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.2488, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.0984, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.4094, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.7317, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.5776, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.1213, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.5172, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.1831, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.8073, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.6354, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1275],
        [ 0.4188],
        [-0.2263],
        [ 0.0755],
        [ 0.0578]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5038, 1.2608, 1.8620, 1.5806, 1.4619], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:49:25,554] Trial 109 finished with value: 1.4000000000000001 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.005, 'epsilon_decay_percentage': 0.7, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9037],
        [-0.7726],
        [-0.7783],
        [ 0.1664],
        [ 0.9029]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.7507],
        [-0.5621],
        [ 0.0401],
        [-0.8523],
        [-0.0323]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.4036, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.0203, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.2458, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.2543, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.2944, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.2898, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.1331, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.2312, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.2679, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.2489, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.1761, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.2459, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.2166, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.2815, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2034, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.0623, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3586, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2020, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.2951, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1579, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1138],
        [ 0.1128],
        [-0.0146],
        [ 0.0136],
        [-0.0455]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.4036, 0.0203, 0.2458, 0.2543, 0.2944], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:49:30,260] Trial 110 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.4, 'lr_decay_percentage': 1, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0329],
        [-0.9641],
        [ 0.4420],
        [ 0.8649],
        [ 0.7185]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8379],
        [ 0.5113],
        [ 0.0355],
        [-0.0801],
        [-0.6302]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(2.0926, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2154, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.4184, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7012, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4324, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.7593, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.9541, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.9954, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.0316, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.7268, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.4260, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.6928, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.5723, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.3619, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(2.0211, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.0927, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(3.4315, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.1492, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.6923, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.3155, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3333],
        [ 0.7387],
        [-0.4231],
        [ 0.3303],
        [ 0.2943]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([2.0926, 1.2154, 2.4184, 1.7012, 1.4324], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:49:57,240] Trial 111 finished with value: 1.5999999999999999 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.7, 'lr_decay_percentage': 0.8, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5388],
        [-0.1801],
        [-0.9850],
        [ 0.5781],
        [ 0.6879]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2657],
        [-0.4215],
        [-0.5769],
        [-0.6299],
        [-0.6071]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.0989, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5513, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5135, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.9205, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2231, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9382, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.3560, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3987, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.3126, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3253, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7776, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.1606, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2839, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.7046, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.4274, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6169, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.9653, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.1691, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.0967, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.5295, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1607],
        [ 0.8047],
        [-0.1148],
        [ 0.3921],
        [ 0.1021]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.0989, 0.5513, 1.5135, 0.9205, 1.2231], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:50:24,270] Trial 112 finished with value: 4.0 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.7, 'lr_decay_percentage': 0.8, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1639],
        [-0.6980],
        [ 0.8139],
        [-0.7140],
        [-0.9634]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9027],
        [-0.1315],
        [ 0.6199],
        [ 0.4415],
        [-0.1819]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6364, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5603, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6058, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6018, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6462, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5828, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5266, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5363, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6044, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5158, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5292, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4928, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4669, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6071, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3854, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.4756, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4591, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3974, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6097, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2550, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0536],
        [-0.0337],
        [-0.0694],
        [ 0.0026],
        [-0.1304]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6364, 0.5603, 0.6058, 0.6018, 0.6462], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:50:51,375] Trial 113 finished with value: 0.5 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.8, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2011],
        [-0.7493],
        [-0.2607],
        [-0.9187],
        [-0.4090]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9378],
        [ 0.0454],
        [ 0.6075],
        [-0.7171],
        [ 0.9695]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.4407, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5131, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.7601, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7005, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6620, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4833, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7349, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.7331, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.7356, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6746, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5258, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.9567, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7060, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.7707, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6871, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.5684, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.1785, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6790, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.8057, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.6996, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0426],
        [ 0.2218],
        [-0.0270],
        [ 0.0351],
        [ 0.0125]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.4407, 0.5131, 0.7601, 0.7005, 0.6620], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:51:17,746] Trial 114 finished with value: 1.5999999999999999 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.7, 'lr_decay_percentage': 0.8, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1586],
        [-0.5105],
        [ 0.0491],
        [-0.2043],
        [ 0.1957]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2587],
        [ 0.7128],
        [-0.3761],
        [ 0.8867],
        [ 0.5920]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6511, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5374, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.7146, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7466, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6727, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5543, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7619, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6953, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.7861, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6579, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4574, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.9864, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6759, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.8257, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6430, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3606, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.2109, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6566, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.8652, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.6281, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0969],
        [ 0.2245],
        [-0.0193],
        [ 0.0395],
        [-0.0149]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6511, 0.5374, 0.7146, 0.7466, 0.6727], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:51:42,593] Trial 115 finished with value: -0.7000000000000001 and parameters: {'nsteps_train': 12000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.7, 'lr_decay_percentage': 0.5, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.7572],
        [-0.9683],
        [-0.3611],
        [ 0.0159],
        [ 0.6302]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6201],
        [-0.5118],
        [-0.0895],
        [ 0.4105],
        [ 0.6308]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6569, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5081, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6062, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5414, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6516, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4778, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5688, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5699, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5185, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5450, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2988, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6295, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5336, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4956, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4383, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1198, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6902, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4973, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4726, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3316, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1790],
        [ 0.0607],
        [-0.0363],
        [-0.0229],
        [-0.1067]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6569, 0.5081, 0.6062, 0.5414, 0.6516], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:52:09,305] Trial 116 finished with value: 0.5 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.05, 'epsilon_decay_percentage': 1.2, 'lr_decay_percentage': 0.8, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9134],
        [-0.2541],
        [-0.8678],
        [ 0.9193],
        [ 0.4601]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9188],
        [ 0.1112],
        [ 0.6530],
        [-0.5671],
        [ 0.2780]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.9379, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5644, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.7695, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8554, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.4160, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6311, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6489, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6869, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.7774, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5523, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3243, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7334, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6044, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6994, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6885, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.0174, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.8180, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5218, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6213, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.8248, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3068],
        [ 0.0845],
        [-0.0825],
        [-0.0780],
        [ 0.1363]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.9379, 0.5644, 0.7695, 0.8554, 0.4160], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:52:14,376] Trial 117 finished with value: 0.5 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.9, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8491],
        [ 0.9889],
        [ 0.7080],
        [ 0.9374],
        [-0.6819]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9530],
        [-0.7049],
        [ 0.4713],
        [ 0.3296],
        [ 0.4145]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7202, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0101, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8782, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4842, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1302, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8284, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6031, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.6784, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6943, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3323, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.9367, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.1961, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.4786, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.9043, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.5345, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.0449, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.7891, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.2789, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.1143, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.7366, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.1082],
        [ 0.5930],
        [-0.1998],
        [ 0.2100],
        [ 0.2021]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7202, 1.0101, 1.8782, 1.4842, 1.1302], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:52:24,998] Trial 118 finished with value: 1.5999999999999999 and parameters: {'nsteps_train': 15000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.6, 'minibatch_size': 64}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.7404],
        [ 0.9477],
        [ 0.3545],
        [-0.6396],
        [-0.4280]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8249],
        [ 0.7107],
        [ 0.5882],
        [ 0.1514],
        [-0.3291]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7141, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4290, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4545, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7046, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.4513, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5182, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5255, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5588, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.7277, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5649, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3223, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6219, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6630, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.7508, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6786, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1264, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7184, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7673, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7739, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.7922, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1959],
        [ 0.0965],
        [ 0.1043],
        [ 0.0231],
        [ 0.1137]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7141, 0.4290, 0.4545, 0.7046, 0.4513], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:52:29,773] Trial 119 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9034],
        [ 0.3060],
        [-0.2167],
        [-0.4590],
        [-0.4702]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9465],
        [-0.2603],
        [ 0.2578],
        [-0.3188],
        [ 0.1598]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5412, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4802, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5373, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5049, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5857, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4253, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5402, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4971, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5099, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5046, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3093, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6001, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4570, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5148, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4234, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1934, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6600, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4169, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5197, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3422, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1159],
        [ 0.0599],
        [-0.0401],
        [ 0.0049],
        [-0.0812]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5412, 0.4802, 0.5373, 0.5049, 0.5857], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:52:50,840] Trial 120 finished with value: 0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.01, 'epsilon_decay_percentage': 0.8, 'lr_decay_percentage': 0.7, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1992],
        [-0.1820],
        [-0.6756],
        [ 0.7244],
        [-0.3329]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.7565],
        [ 0.7223],
        [ 0.6361],
        [-0.2119],
        [ 0.4750]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2224, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.9215, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3408, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2589, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0478, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0540, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.2520, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1233, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.2186, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.8804, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.8855, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.5825, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.9059, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.1784, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.7130, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7171, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.9130, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6884, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.1382, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5456, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1684],
        [ 0.3305],
        [-0.2175],
        [-0.0402],
        [-0.1674]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2224, 0.9215, 1.3408, 1.2589, 1.0478], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:52:56,695] Trial 121 finished with value: 1.4000000000000001 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.05, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8025],
        [ 0.7531],
        [ 0.1220],
        [ 0.5398],
        [-0.6326]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7104],
        [-0.4216],
        [-0.4360],
        [ 0.9477],
        [ 0.2611]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.8042, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5588, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6773, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7403, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7368, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6295, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6597, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6184, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.7151, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6979, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4547, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7606, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5596, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6898, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6589, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2800, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.8615, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5007, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6646, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.6200, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1747],
        [ 0.1009],
        [-0.0589],
        [-0.0252],
        [-0.0390]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.8042, 0.5588, 0.6773, 0.7403, 0.7368], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:53:02,690] Trial 122 finished with value: 0.5 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.05, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5130],
        [-0.4573],
        [-0.3330],
        [ 0.6309],
        [ 0.7395]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6851],
        [-0.8251],
        [-0.6432],
        [ 0.3988],
        [-0.4404]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7327, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6802, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.7730, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7325, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7733, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5787, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7838, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.7056, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.7124, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6861, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4247, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.8875, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6382, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6923, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5990, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2707, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.9911, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5708, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6722, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5118, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1540],
        [ 0.1037],
        [-0.0674],
        [-0.0201],
        [-0.0872]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7327, 0.6802, 0.7730, 0.7325, 0.7733], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:53:09,338] Trial 123 finished with value: 0.5 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.05, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.0355],
        [ 0.9671],
        [-0.2498],
        [ 0.6979],
        [-0.8087]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9403],
        [-0.6750],
        [-0.4713],
        [-0.9703],
        [-0.7518]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3718, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3890, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.7319, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5938, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6350, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.2227, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7397, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.7267, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.7763, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5618, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.0736, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.0904, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7215, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.9587, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4885, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(-0.0755, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.4410, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7163, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.1412, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4153, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1491],
        [ 0.3507],
        [-0.0052],
        [ 0.1825],
        [-0.0732]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3718, 0.3890, 0.7319, 0.5938, 0.6350], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:53:15,154] Trial 124 finished with value: 1.5999999999999999 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7142],
        [-0.8276],
        [-0.3459],
        [ 0.5103],
        [ 0.1916]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6233],
        [ 0.5583],
        [ 0.1463],
        [ 0.8137],
        [-0.4829]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7320, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6615, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.7158, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7686, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7228, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6098, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7549, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6996, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.7603, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6447, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4877, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.8484, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6834, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.7519, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5666, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3655, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.9419, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6672, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7435, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4886, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1222],
        [ 0.0935],
        [-0.0162],
        [-0.0084],
        [-0.0781]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7320, 0.6615, 0.7158, 0.7686, 0.7228], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:53:21,681] Trial 125 finished with value: -0.7000000000000001 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.05, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.7, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.0925],
        [-0.7559],
        [ 0.7198],
        [ 0.3694],
        [ 0.9525]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8000],
        [ 0.9171],
        [-0.3606],
        [ 0.4209],
        [-0.4218]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.0809, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.9014, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1312, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1284, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0917, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8652, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.0541, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.9785, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.1653, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0096, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6495, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.2068, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.8258, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.2021, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.9274, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.4338, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.3595, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6732, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.2389, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.8453, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2157],
        [ 0.1527],
        [-0.1527],
        [ 0.0368],
        [-0.0822]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.0809, 0.9014, 1.1312, 1.1284, 1.0917], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:53:27,013] Trial 126 finished with value: 1.5999999999999999 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.05, 'epsilon_decay_percentage': 0.9, 'lr_decay_percentage': 0.6, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1904],
        [-0.7219],
        [-0.6239],
        [ 0.3336],
        [ 0.4447]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.3933],
        [ 0.2195],
        [ 0.6766],
        [-0.1122],
        [-0.9933]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6548, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.0834, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9589, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.0048, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7489, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6135, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3657, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.8646, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.9693, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6638, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5721, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6480, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7703, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.9338, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5786, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.5307, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.9303, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6760, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.8983, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4935, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0414],
        [ 0.2823],
        [-0.0943],
        [-0.0355],
        [-0.0852]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6548, 0.0834, 0.9589, 1.0048, 0.7489], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:53:31,742] Trial 127 finished with value: -0.7000000000000001 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.7410],
        [ 0.2307],
        [ 0.2765],
        [ 0.4443],
        [-0.8978]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.4736],
        [-0.4550],
        [ 0.6481],
        [ 0.1475],
        [-0.3947]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.8059, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2499, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2938, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1333, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8911, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7177, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5991, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1786, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.2171, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.9751, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6295, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.9482, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0633, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.3009, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0591, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.5413, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.2974, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.9481, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.3846, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.1432, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  3  Reward Received:  1.0
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0882],
        [ 0.3492],
        [-0.1153],
        [ 0.0838],
        [ 0.0840]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.8059, 0.2499, 1.2938, 1.1333, 0.8911], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:53:37,507] Trial 128 finished with value: 0.7000000000000001 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1, 'lr_decay_percentage': 1, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.3919],
        [ 0.0151],
        [-0.2461],
        [-0.8230],
        [-0.7080]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1148],
        [-0.7252],
        [ 0.4393],
        [-0.3686],
        [-0.3652]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5410, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3232, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3741, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.3809, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5423, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4401, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3658, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3421, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.3963, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4152, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3391, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4083, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3101, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4117, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2881, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2381, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4508, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2780, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4272, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1611, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1010],
        [ 0.0425],
        [-0.0320],
        [ 0.0154],
        [-0.1271]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5410, 0.3232, 0.3741, 0.3809, 0.5423], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:53:54,520] Trial 129 finished with value: 0.5 and parameters: {'nsteps_train': 15000, 'lr_begin': 0.005, 'epsilon_decay_percentage': 1.3, 'lr_decay_percentage': 0.9, 'minibatch_size': 128}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.6941],
        [ 0.9392],
        [ 0.1544],
        [-0.4347],
        [-0.0816]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4371],
        [-0.3842],
        [ 0.0215],
        [ 0.6600],
        [ 0.9440]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.0291, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.8623, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.0056, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.9255, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.9862, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0221, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7999, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.8408, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5563, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3581, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.0150, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.7375, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.6760, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.1870, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.7299, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.0079, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(3.6751, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.5112, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.8178, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.1018, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0071],
        [ 0.9376],
        [-0.1648],
        [ 0.6308],
        [ 0.3718]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.0291, 0.8623, 2.0056, 0.9255, 0.9862], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:54:00,933] Trial 130 finished with value: 4.0 and parameters: {'nsteps_train': 14000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.2, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.0473],
        [ 0.0859],
        [ 0.9389],
        [ 0.5394],
        [-0.1497]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0883],
        [ 0.5229],
        [-0.7509],
        [-0.9609],
        [ 0.5989]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6380, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5219, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6148, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4752, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6439, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5592, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5037, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5340, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4219, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5009, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4803, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4855, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4532, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.3685, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3578, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.4015, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4674, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3725, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.3152, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2147, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0789],
        [-0.0182],
        [-0.0808],
        [-0.0533],
        [-0.1431]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6380, 0.5219, 0.6148, 0.4752, 0.6439], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:54:27,685] Trial 131 finished with value: 0.5 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.7, 'lr_decay_percentage': 0.8, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.3554],
        [ 0.0361],
        [ 0.9831],
        [ 0.8601],
        [ 0.8808]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 7.3004e-04],
        [ 5.1911e-02],
        [-9.3166e-01],
        [-7.9109e-01],
        [-2.0925e-01]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5031, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3163, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4646, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.2948, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.4126, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4114, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3461, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3850, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.3238, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.3162, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3197, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3759, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3055, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.3529, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2199, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2280, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4058, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2259, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.3819, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1235, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0917],
        [ 0.0298],
        [-0.0795],
        [ 0.0290],
        [-0.0964]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5031, 0.3163, 0.4646, 0.2948, 0.4126], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:54:54,706] Trial 132 finished with value: 0.5 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.7, 'lr_decay_percentage': 0.8, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.5610],
        [-0.0700],
        [ 0.9872],
        [ 0.4808],
        [-0.5252]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.8623],
        [ 0.0782],
        [-0.3889],
        [-0.6845],
        [ 0.7467]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3933, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.1851, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5359, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4961, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.4857, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4317, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2732, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4724, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5148, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4331, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4701, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3613, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4089, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5335, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3805, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.5085, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4493, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3454, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5522, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3278, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  3  Reward Received:  1.0
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0384],
        [ 0.0881],
        [-0.0635],
        [ 0.0187],
        [-0.0526]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3933, 0.1851, 0.5359, 0.4961, 0.4857], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:55:21,699] Trial 133 finished with value: 0.7000000000000001 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.7, 'lr_decay_percentage': 0.8, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[0.2172],
        [0.8520],
        [0.1560],
        [0.5062],
        [0.0600]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2866],
        [ 0.4520],
        [-0.5614],
        [ 0.6086],
        [-0.7642]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6401, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4973, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6433, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5500, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6515, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5111, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5243, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5380, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5246, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4772, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3821, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5513, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4328, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4992, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3028, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2530, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5782, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3276, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4738, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1284, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1290],
        [ 0.0270],
        [-0.1052],
        [-0.0254],
        [-0.1744]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6401, 0.4973, 0.6433, 0.5500, 0.6515], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:55:47,872] Trial 134 finished with value: 0.5 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.7, 'lr_decay_percentage': 0.8, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.0657],
        [-0.8883],
        [-0.6942],
        [ 0.3975],
        [-0.9018]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4180],
        [ 0.0824],
        [ 0.3353],
        [-0.7490],
        [-0.5693]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(2.0453, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2691, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(3.0481, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9028, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1172, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.9385, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.5117, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(2.5472, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.7341, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.9142, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.8318, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(3.7543, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(2.0463, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(3.5654, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(2.7113, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.7250, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(4.9968, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.5455, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(4.3968, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(3.5084, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1067],
        [ 1.2426],
        [-0.5009],
        [ 0.8313],
        [ 0.7971]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([2.0453, 1.2691, 3.0481, 1.9028, 1.1172], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:56:14,797] Trial 135 finished with value: 1.5999999999999999 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.5, 'lr_decay_percentage': 0.8, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1236],
        [-0.6403],
        [ 0.0649],
        [ 0.4360],
        [ 0.5461]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8296],
        [ 0.8389],
        [-0.1804],
        [ 0.9524],
        [ 0.4846]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6463, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.7551, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.0380, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8189, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8188, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6204, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7510, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.2752, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.7963, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6918, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5944, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7468, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5124, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.7736, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5648, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.5684, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7426, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7495, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7509, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4379, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0260],
        [-0.0042],
        [ 0.2372],
        [-0.0227],
        [-0.1270]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6463, 0.7551, 0.0380, 0.8189, 0.8188], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:56:35,853] Trial 136 finished with value: -0.5 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 0.7, 'minibatch_size': 256}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8810],
        [ 0.5778],
        [-0.2588],
        [ 0.7957],
        [-0.9441]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0438],
        [ 0.3340],
        [-0.2071],
        [ 0.6208],
        [ 0.5430]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.1511, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6836, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6701, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7190, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6121, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3712, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6535, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6397, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6911, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5760, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5913, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6235, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6092, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6632, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5399, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.8114, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5935, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5788, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6353, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5038, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.2201],
        [-0.0300],
        [-0.0304],
        [-0.0279],
        [-0.0361]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.1511, 0.6836, 0.6701, 0.7190, 0.6121], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:56:40,536] Trial 137 finished with value: -0.1 and parameters: {'nsteps_train': 10000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.6, 'lr_decay_percentage': 0.6, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4313],
        [ 0.3114],
        [ 0.7486],
        [ 0.3280],
        [ 0.1874]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.5518],
        [-0.8087],
        [ 0.9451],
        [ 0.1781],
        [-0.2742]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6331, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5826, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4066, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8199, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7928, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5833, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6706, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5053, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.8095, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6977, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5335, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7586, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6041, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.7991, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6026, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.4837, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.8466, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7029, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.7887, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5074, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0498],
        [ 0.0880],
        [ 0.0988],
        [-0.0104],
        [-0.0951]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6331, 0.5826, 0.4066, 0.8199, 0.7928], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:56:45,622] Trial 138 finished with value: -0.7000000000000001 and parameters: {'nsteps_train': 11000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.1, 'lr_decay_percentage': 1, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.2065],
        [ 0.9571],
        [-0.9831],
        [ 0.9659],
        [ 0.7522]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6859],
        [ 0.3860],
        [-0.8415],
        [ 0.5402],
        [ 0.1872]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3015, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.1322, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3056, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5163, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6298, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4305, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2294, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3744, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5326, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5326, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5595, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3265, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4432, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5490, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4354, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6884, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4237, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5120, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5653, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3382, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.1290],
        [ 0.0972],
        [ 0.0688],
        [ 0.0163],
        [-0.0972]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3015, 0.1322, 0.3056, 0.5163, 0.6298], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-07 21:56:52,381] Trial 139 finished with value: 0.5 and parameters: {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 0.7, 'lr_decay_percentage': 0.5, 'minibatch_size': 32}. Best is trial 29 with value: 4.0.
Best Params:  {'nsteps_train': 13000, 'lr_begin': 0.001, 'epsilon_decay_percentage': 1.2, 'lr_decay_percentage': 1, 'minibatch_size': 256}
Optimization complete. Data saved to: sqlite:///db.sqlite3
