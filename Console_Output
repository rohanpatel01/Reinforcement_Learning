[I 2025-07-10 00:30:07,428] A new study created in RDB with name: State rand int rep and normalization
Starting Training
layer weights:  Parameter containing:
tensor([[-0.6958],
        [-0.7622],
        [-0.4912],
        [-0.3310],
        [ 0.5414]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.8442],
        [-0.2156],
        [-0.6178],
        [-0.7376],
        [-0.8201]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4371, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1850, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4838, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5954, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4310, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2673, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3251, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4146, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6361, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3920, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9990, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5465, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3054, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7006, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3305, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7811, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7264, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2167, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7530, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2806, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0771], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9714], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9714], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9714], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9714], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.7336],
        [ 0.6054],
        [-0.2987],
        [ 0.1762],
        [-0.1682]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4937, 1.1382, 1.5068, 1.5818, 1.4439], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:30:21,554] Trial 0 finished with value: -0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 350, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 0 with value: -0.5.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.6040],
        [-0.7657],
        [-0.2972],
        [-0.7905],
        [ 0.6914]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3079],
        [ 0.1511],
        [ 0.9698],
        [ 0.8043],
        [-0.2158]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2775, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2193, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2592, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4098, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2596, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0419, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3388, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1457, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4168, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1247, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8360, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4432, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0466, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4229, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0069, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6122, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5567, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.9388, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4295, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.8789, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0160], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3600], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4920], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3600], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4920], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4950],
        [ 0.2510],
        [-0.2384],
        [ 0.0147],
        [-0.2832]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2854, 1.2153, 1.2630, 1.4096, 1.2641], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:30:26,790] Trial 1 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 300, 'high': 250, 'minibatch_size': 32, 'replay_buffer_size': 50000000}. Best is trial 0 with value: -0.5.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4610],
        [-0.1171],
        [ 0.9416],
        [-0.9424],
        [-0.3274]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6936],
        [ 0.5141],
        [-0.4136],
        [ 0.7087],
        [-0.6760]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.1074, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2520, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1886, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3443, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0741, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9428, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4035, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0830, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4266, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0046, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7521, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5791, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9606, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5220, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.9242, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5213, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7916, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.8124, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6375, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.8267, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2680], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4280], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2680], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4280], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.5017],
        [ 0.4620],
        [-0.3221],
        [ 0.2510],
        [-0.2118]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.1575, 1.2058, 1.2209, 1.3192, 1.0953], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:30:34,890] Trial 2 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 300, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 0 with value: -0.5.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.6246],
        [-0.5703],
        [-0.6049],
        [ 0.2335],
        [-0.2864]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6218],
        [-0.8150],
        [ 0.0131],
        [-0.2187],
        [-0.8657]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.9920, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.8924, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0409, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1065, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.9576, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8635, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0453, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9612, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1816, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.9064, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7290, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2051, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.8778, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2600, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8529, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.5581, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4083, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.7718, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3598, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7848, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7600], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2956], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7600], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2956], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.6575],
        [ 0.7816],
        [-0.4078],
        [ 0.3839],
        [-0.2618]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.0578, 0.8143, 1.0817, 1.0681, 0.9838], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:30:41,096] Trial 3 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 0 with value: -0.5.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8525],
        [-0.0820],
        [ 0.7050],
        [ 0.8165],
        [ 0.9998]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6159],
        [-0.8060],
        [ 0.3424],
        [-0.7876],
        [-0.9620]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.1589, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0424, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2371, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3411, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1156, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0526, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1677, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1870, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4022, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1007, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9409, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2994, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1344, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4663, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0850, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7793, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4899, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0583, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5591, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0623, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0691], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.5818], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.5818], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.5818], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.5818], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.7404],
        [ 0.8728],
        [-0.3487],
        [ 0.4253],
        [-0.1039]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2101, 0.9820, 1.2612, 1.3117, 1.1228], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:30:45,542] Trial 4 finished with value: -0.5 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 550, 'minibatch_size': 32, 'replay_buffer_size': 1000}. Best is trial 0 with value: -0.5.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.3932],
        [ 0.4329],
        [ 0.2683],
        [ 0.6407],
        [-0.9974]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1751],
        [ 0.7545],
        [-0.6612],
        [-0.2091],
        [-0.9917]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.1546, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2196, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2953, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2344, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2174, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9672, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3390, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2058, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2537, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0872, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8624, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4057, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1557, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2645, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0144, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6327, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5521, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0461, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2882, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8547, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0978], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.4511], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.3244], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3244], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3244], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8269],
        [ 0.5269],
        [-0.3949],
        [ 0.0852],
        [-0.5748]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2354, 1.1681, 1.3339, 1.2261, 1.2736], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:30:56,976] Trial 5 finished with value: 1.4000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 5 with value: 1.4000000000000001.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2563],
        [-0.5251],
        [-0.3838],
        [-0.3094],
        [-0.1631]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.5483],
        [-0.2995],
        [ 0.8057],
        [ 0.6201],
        [ 0.7072]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3752, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2668, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3192, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3064, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4645, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1617, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3041, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1899, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2160, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2750, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9764, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3365, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0776, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1376, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1104, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7798, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.3708, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.9586, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.0543, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.9359, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0280], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0280], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0280], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0280], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0280], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4681],
        [ 0.0817],
        [-0.2835],
        [-0.1981],
        [-0.4155]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3883, 1.2646, 1.3271, 1.3119, 1.4761], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:31:00,612] Trial 6 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 300, 'high': 250, 'minibatch_size': 32, 'replay_buffer_size': 1000}. Best is trial 5 with value: 1.4000000000000001.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0195],
        [ 0.6303],
        [ 0.5505],
        [-0.5081],
        [ 0.4518]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0929],
        [ 0.2353],
        [-0.7904],
        [ 0.3477],
        [ 0.6849]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5292, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4861, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5220, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5759, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4976, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3624, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6628, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4545, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5440, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4503, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0882, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9532, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3436, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4914, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3726, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8506, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.2050, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2474, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4459, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3053, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0818], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6218], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2145], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2145], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2145], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.8999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.2566],
        [ 1.3313],
        [-0.5084],
        [-0.2408],
        [-0.3562]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6320, 1.3772, 1.5636, 1.5956, 1.5268], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:31:11,883] Trial 7 finished with value: -0.8999999999999999 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 300, 'high': 550, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 5 with value: 1.4000000000000001.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.9645],
        [-0.8534],
        [ 0.2863],
        [ 0.0443],
        [-0.7018]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0152],
        [ 0.9157],
        [-0.2639],
        [-0.6334],
        [-0.1215]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6256, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6636, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8368, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7625, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6564, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3610, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9569, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6664, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7845, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5141, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0839, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.2641, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4880, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8075, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3649, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9201, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.4457, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3825, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.8211, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2768, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1000], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.9600], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.5200], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5200], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5200], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.6298],
        [ 0.6983],
        [-0.4056],
        [ 0.0523],
        [-0.3390]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6885, 1.5938, 1.8773, 1.7572, 1.6903], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:31:17,812] Trial 8 finished with value: 1.4000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 5 with value: 1.4000000000000001.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.9763],
        [-0.1510],
        [ 0.9818],
        [ 0.7912],
        [ 0.3229]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.5036],
        [-0.5443],
        [ 0.4738],
        [ 0.0577],
        [-0.9017]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2838, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2656, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4854, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6455, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3240, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0763, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5401, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3596, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7243, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3695, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9061, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7652, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2565, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7889, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4067, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7193, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.0123, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1434, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8599, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4477, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1286], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9057], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4143], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9057], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4143], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.7264],
        [ 0.9607],
        [-0.4401],
        [ 0.2759],
        [ 0.1591]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3772, 1.1421, 1.5419, 1.6100, 1.3036], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:31:21,501] Trial 9 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 350, 'minibatch_size': 32, 'replay_buffer_size': 1000}. Best is trial 5 with value: 1.4000000000000001.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7453],
        [-0.2087],
        [ 0.5718],
        [ 0.7217],
        [ 0.8546]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.7419],
        [ 0.5013],
        [-0.2577],
        [ 0.5045],
        [-0.0291]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3241, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2122, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4255, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4767, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3256, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1053, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4346, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3219, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5361, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2497, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8947, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6487, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2223, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5933, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1766, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7440, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8019, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1510, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6342, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1244, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0533], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6778], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2889], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6778], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2889], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.9290],
        [ 0.9443],
        [-0.4396],
        [ 0.2523],
        [-0.3222]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3737, 1.1618, 1.4489, 1.4632, 1.3428], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:31:41,889] Trial 10 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 450, 'minibatch_size': 256, 'replay_buffer_size': 50000000}. Best is trial 5 with value: 1.4000000000000001.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.6921],
        [-0.0292],
        [-0.7000],
        [ 0.0476],
        [ 0.0685]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1040],
        [ 0.7301],
        [ 0.4806],
        [-0.5840],
        [ 0.3527]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3710, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3572, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3737, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4656, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5765, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0669, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4732, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3282, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5282, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4091, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7143, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6077, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2755, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6007, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2149, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4070, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.7250, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2295, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.6639, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0457, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.2400], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.2400], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.2400], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.2400], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.2400], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4852],
        [ 0.1851],
        [-0.0726],
        [ 0.0998],
        [-0.2672]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4874, 1.3127, 1.3911, 1.4417, 1.6407], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:31:52,049] Trial 11 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 128, 'replay_buffer_size': 1000}. Best is trial 5 with value: 1.4000000000000001.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2404],
        [-0.3634],
        [ 0.2417],
        [-0.6994],
        [ 0.9910]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.7623],
        [ 0.2865],
        [ 0.6483],
        [ 0.1120],
        [ 0.3572]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2201, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1963, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2482, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5961, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3680, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0501, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4041, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2135, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6645, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2935, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8359, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6658, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1698, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7506, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1998, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6742, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8633, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1368, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8156, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1289, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0933], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6711], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2733], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6711], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2733], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.9448],
        [ 1.1545],
        [-0.1927],
        [ 0.3799],
        [-0.4138]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3083, 1.0885, 1.2662, 1.5606, 1.4067], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:31:57,888] Trial 12 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 5 with value: 1.4000000000000001.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9580],
        [ 0.6003],
        [-0.0725],
        [-0.6393],
        [-0.8613]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9384],
        [ 0.8502],
        [-0.3631],
        [ 0.9317],
        [-0.9015]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3548, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2930, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3697, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4421, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3456, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1627, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5489, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2555, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4795, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2498, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0022, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7629, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1600, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5107, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.1698, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.8349, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.9857, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0606, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.5432, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0864, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1667], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.2600], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.9400], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.9400], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.9400], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.8999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2483],
        [ 0.3309],
        [-0.1477],
        [ 0.0483],
        [-0.1238]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3962, 1.2379, 1.3943, 1.4341, 1.3662], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:32:18,179] Trial 13 finished with value: -0.8999999999999999 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 256, 'replay_buffer_size': 50000000}. Best is trial 5 with value: 1.4000000000000001.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.9250],
        [ 0.7585],
        [ 0.7711],
        [-0.6411],
        [ 0.1435]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.5779],
        [ 0.2606],
        [-0.5758],
        [ 0.3231],
        [-0.3866]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4737, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1853, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5687, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4510, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3556, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2492, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4199, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4130, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5380, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3243, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9765, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7048, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2240, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6438, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2862, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8344, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8533, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1255, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6988, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2664, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0600], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.5422], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.2778], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6800], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2778], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0312],
        [ 1.0774],
        [-0.7148],
        [ 0.3997],
        [-0.1440]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5356, 1.1206, 1.6116, 1.4270, 1.3643], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:32:24,111] Trial 14 finished with value: 1.5999999999999999 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7060],
        [ 0.2412],
        [ 0.8783],
        [-0.4038],
        [-0.0819]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5727],
        [ 0.0506],
        [-0.3503],
        [ 0.5812],
        [-0.4266]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3748, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2702, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4067, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5524, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3379, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1398, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4568, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3183, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5819, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2837, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9169, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6338, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2344, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6099, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2323, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6868, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8165, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1478, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6388, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1792, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0778], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7089], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2933], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7089], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2933], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0903],
        [ 0.8655],
        [-0.4103],
        [ 0.1369],
        [-0.2513]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4596, 1.2029, 1.4387, 1.5418, 1.3574], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:32:35,460] Trial 15 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2398],
        [ 0.0236],
        [-0.5560],
        [ 0.7271],
        [ 0.2749]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4150],
        [-0.9386],
        [ 0.6140],
        [-0.9738],
        [ 0.5619]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2660, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1899, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4647, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4634, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3115, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0971, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4089, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4038, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5702, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3142, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8469, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7333, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3135, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7284, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3183, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6947, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9307, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2586, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8247, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3207, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0578], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.5044], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.2378], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6667], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2378], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.9383],
        [ 1.2167],
        [-0.3386],
        [ 0.5933],
        [ 0.0151]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3202, 1.1196, 1.4843, 1.4292, 1.3107], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:32:41,269] Trial 16 finished with value: 1.5999999999999999 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9060],
        [ 0.8349],
        [-0.2015],
        [-0.5896],
        [-0.2404]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.8287],
        [ 0.9323],
        [ 0.5161],
        [-0.9044],
        [-0.5498]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4308, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2537, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4695, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4885, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4257, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1158, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5300, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3351, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5706, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3078, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8778, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7387, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2335, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6326, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2188, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7285, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8697, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1698, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6715, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1629, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0022], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6711], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3022], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6711], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3022], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0499],
        [ 0.9209],
        [-0.4481],
        [ 0.2736],
        [-0.3930]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4331, 1.2517, 1.4705, 1.4879, 1.4266], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:32:47,095] Trial 17 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.3092],
        [-0.4592],
        [-0.4544],
        [ 0.1776],
        [ 0.4470]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9556],
        [-0.3740],
        [ 0.4147],
        [-0.3375],
        [-0.7294]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3005, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2791, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2359, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4566, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3162, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0929, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4203, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1428, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4638, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2503, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9612, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5098, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0837, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4683, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2084, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7239, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6711, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9773, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4765, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1331, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0756], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7667], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3244], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7667], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3244], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8343],
        [ 0.5671],
        [-0.3743],
        [ 0.0287],
        [-0.2650]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3636, 1.2363, 1.2642, 1.4544, 1.3362], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:32:52,969] Trial 18 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.2702],
        [ 0.2399],
        [-0.7599],
        [ 0.6647],
        [-0.5036]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9134],
        [ 0.7684],
        [-0.4344],
        [-0.1616],
        [-0.4361]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6604, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4464, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5048, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8130, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5469, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.4906, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6318, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4387, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8269, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4676, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2884, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8525, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3600, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8435, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3731, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0903, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.0688, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2829, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8598, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2806, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1089], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7356], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2956], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7356], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2956], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.9097],
        [ 0.9931],
        [-0.3541],
        [ 0.0747],
        [-0.4250]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.7595, 1.3383, 1.5434, 1.8049, 1.5932], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:32:58,761] Trial 19 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9398],
        [ 0.4166],
        [ 0.2814],
        [-0.8914],
        [-0.9339]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.4306],
        [ 0.8791],
        [ 0.7330],
        [ 0.4546],
        [-0.9996]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4957, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2784, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5920, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6661, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4565, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2693, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4836, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5279, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7268, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3552, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9951, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7322, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4502, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8003, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2325, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8079, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9019, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3972, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8504, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1487, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0086], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9114], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3057], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9114], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3057], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.7617],
        [ 0.6906],
        [-0.2158],
        [ 0.2041],
        [-0.3409]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5022, 1.2725, 1.5939, 1.6644, 1.4594], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:33:04,562] Trial 20 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 350, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9455],
        [ 0.7501],
        [ 0.0551],
        [ 0.5081],
        [-0.8202]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.8933],
        [-0.6076],
        [-0.7278],
        [ 0.3824],
        [ 0.4309]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5020, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2562, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5099, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6519, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4725, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2503, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5483, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4217, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7077, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3730, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0652, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7632, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3569, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7487, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2999, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9050, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9492, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3008, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7842, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2366, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0489], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6867], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3178], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6867], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3178], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.9362],
        [ 1.0866],
        [-0.3278],
        [ 0.2075],
        [-0.3698]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5478, 1.2030, 1.5259, 1.6417, 1.4905], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:33:15,858] Trial 21 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.3828],
        [-0.4474],
        [-0.4604],
        [-0.8428],
        [-0.8710]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5821],
        [ 0.8380],
        [ 0.4934],
        [ 0.2529],
        [ 0.3090]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4911, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3286, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5131, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5811, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4482, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1887, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5491, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3536, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5395, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3430, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0421, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6560, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2763, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5194, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2920, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7700, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8545, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1327, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4820, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1973, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0311], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7200], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3200], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3200], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3200], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.8999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0468],
        [ 0.7634],
        [-0.5522],
        [-0.1438],
        [-0.3642]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5237, 1.3048, 1.5303, 1.5856, 1.4596], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:33:34,815] Trial 22 finished with value: -0.8999999999999999 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 256, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.7429],
        [ 0.8823],
        [-0.2523],
        [-0.9959],
        [ 0.0897]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7440],
        [ 0.4959],
        [-0.0854],
        [-0.1977],
        [ 0.8377]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3799, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2941, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4094, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5548, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4154, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1720, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4700, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3390, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5763, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3105, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9562, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6524, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2660, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5986, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2016, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6910, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8766, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1762, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6261, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0678, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0978], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6867], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2756], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6867], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2756], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.1698],
        [ 0.9892],
        [-0.3959],
        [ 0.1210],
        [-0.5903]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4943, 1.1974, 1.4481, 1.5430, 1.4732], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:33:46,264] Trial 23 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.8088],
        [-0.5076],
        [ 0.5437],
        [-0.6015],
        [ 0.6603]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2447],
        [-0.4957],
        [ 0.2891],
        [ 0.4582],
        [ 0.3945]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4930, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3023, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5855, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7052, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4849, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3349, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5108, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5334, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7569, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4881, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1041, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8151, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4575, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8324, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4927, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9278, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.0475, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3995, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8901, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4963, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0711], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7622], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2644], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7622], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2644], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8177],
        [ 1.0783],
        [-0.2691],
        [ 0.2676],
        [ 0.0164]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5511, 1.2256, 1.6046, 1.6861, 1.4838], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:33:52,069] Trial 24 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2393],
        [ 0.5586],
        [-0.6507],
        [ 0.5641],
        [ 0.8457]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1113],
        [-0.5937],
        [ 0.4157],
        [-0.9726],
        [-0.3716]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3445, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2503, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4672, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4517, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4306, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1357, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4157, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3629, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4453, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3200, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9185, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5876, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2545, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4387, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2049, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7118, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.7513, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.1513, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.4323, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0954, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1133], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([1.4733], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.7800], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1333], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7800], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3132],
        [ 0.2480],
        [-0.1563],
        [-0.0096],
        [-0.1660]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3800, 1.2222, 1.4849, 1.4528, 1.4494], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:33:57,874] Trial 25 finished with value: 1.5999999999999999 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2148],
        [-0.5000],
        [-0.7918],
        [-0.5792],
        [ 0.7365]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6666],
        [ 0.6314],
        [-0.3551],
        [ 0.5743],
        [ 0.1114]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2147, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0944, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2122, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2882, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2104, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9824, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2138, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1375, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3010, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1395, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7168, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.3503, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.0522, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.3155, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0584, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4513, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.4867, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.9669, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.3301, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.9773, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.2467], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7533], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.2467], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7533], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3557],
        [ 0.1828],
        [-0.1143],
        [ 0.0195],
        [-0.1086]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2503, 1.0761, 1.2236, 1.2863, 1.2213], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:34:03,684] Trial 26 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9759],
        [ 0.9840],
        [ 0.6037],
        [ 0.3630],
        [ 0.0852]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9712],
        [ 0.3823],
        [-0.3874],
        [-0.9129],
        [-0.1400]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2350, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2315, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3301, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4124, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1983, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9913, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3533, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2033, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4040, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1079, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8434, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.4272, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1264, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.3990, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0530, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6017, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.5480, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0006, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.3907, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.9634, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1867], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.2133], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.9667], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.2133], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.9667], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3125],
        [ 0.1562],
        [-0.1626],
        [-0.0107],
        [-0.1159]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2934, 1.2023, 1.3605, 1.4144, 1.2199], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:34:09,498] Trial 27 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 400, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0981],
        [ 0.1364],
        [ 0.2780],
        [-0.1531],
        [ 0.1813]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2131],
        [-0.5868],
        [ 0.3527],
        [ 0.7707],
        [-0.4901]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5856, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5255, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7829, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7188, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5931, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2232, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7893, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6609, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8359, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4964, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8338, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.0727, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.5297, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.9618, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3926, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6441, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.2108, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.4658, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.0231, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.3420, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1800], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([1.6600], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.8933], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0333], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.8933], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.5080],
        [ 0.3697],
        [-0.1711],
        [ 0.1641],
        [-0.1355]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6771, 1.4590, 1.8137, 1.6893, 1.6175], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:34:15,322] Trial 28 finished with value: 1.5999999999999999 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.6082],
        [ 0.3904],
        [-0.8544],
        [ 0.3078],
        [-0.8925]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1994],
        [-0.9744],
        [ 0.0012],
        [-0.9375],
        [ 0.8403]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4393, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4248, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6478, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7024, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4885, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2785, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5924, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5822, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7812, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4095, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0205, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8614, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4768, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9078, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2826, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8242, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.0661, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3967, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.0041, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1862, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0255], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6236], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.1818], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6236], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.1818], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0284],
        [ 1.0722],
        [-0.4199],
        [ 0.5044],
        [-0.5054]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4655, 1.3975, 1.6585, 1.6895, 1.5014], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:34:21,169] Trial 29 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 400, 'high': 550, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0325],
        [-0.7866],
        [ 0.9731],
        [-0.6818],
        [ 0.5899]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2893],
        [-0.7819],
        [-0.4348],
        [-0.6049],
        [-0.0273]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3165, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1275, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4459, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4784, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2817, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0134, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4293, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3272, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5942, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2282, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7830, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6587, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2370, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6822, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1875, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6108, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8301, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1696, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7479, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1571, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0400], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.8714], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3971], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.8714], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3971], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8488],
        [ 0.8451],
        [-0.3323],
        [ 0.3241],
        [-0.1499]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3504, 1.0937, 1.4592, 1.4654, 1.2877], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:34:27,003] Trial 30 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 350, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.5986],
        [ 0.4876],
        [-0.2990],
        [ 0.1623],
        [-0.6442]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9902],
        [-0.9873],
        [ 0.7875],
        [-0.6866],
        [ 0.9574]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3369, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4047, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4505, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6432, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3518, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2021, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5407, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4165, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7063, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2964, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0385, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7057, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3752, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.7828, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2291, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.8109, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.9353, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.3179, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.8892, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.1355, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1667], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.2467], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.2467], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2529],
        [ 0.2551],
        [-0.0637],
        [ 0.1182],
        [-0.1040]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3791, 1.3622, 1.4611, 1.6235, 1.3692], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:34:32,998] Trial 31 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2089],
        [ 0.3578],
        [ 0.8205],
        [ 0.0009],
        [-0.6982]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1203],
        [-0.1371],
        [-0.9380],
        [ 0.2536],
        [ 0.3067]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2227, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1195, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3182, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4170, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2513, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9808, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3684, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2006, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5055, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1686, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8135, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5405, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1193, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5667, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.1114, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6321, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.7272, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0311, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.6331, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0494, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0533], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0067], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.8533], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0067], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.8533], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3023],
        [ 0.3111],
        [-0.1470],
        [ 0.1106],
        [-0.1034]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2388, 1.1029, 1.3261, 1.4111, 1.2568], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:34:38,831] Trial 32 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1956],
        [ 0.2900],
        [ 0.9876],
        [ 0.6311],
        [ 0.0244]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2775],
        [ 0.0659],
        [-0.5317],
        [ 0.2971],
        [ 0.0997]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5708, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1708, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5004, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5272, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4618, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1783, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4140, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2683, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4662, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2626, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9110, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5796, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1102, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4246, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.1269, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7119, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.7030, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.9925, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.3937, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0258, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0400], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0400], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0400], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0400], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0400], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4266],
        [ 0.2643],
        [-0.2523],
        [-0.0663],
        [-0.2165]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5879, 1.1602, 1.5105, 1.5298, 1.4705], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:34:44,663] Trial 33 finished with value: 0.5 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 4.6306e-01],
        [-8.5861e-01],
        [-9.2265e-01],
        [ 3.2592e-04],
        [ 4.4118e-01]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7961],
        [-0.0659],
        [ 0.2609],
        [-0.2443],
        [ 0.0732]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.1707, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0467, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1290, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1886, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1941, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8322, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1909, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0186, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2420, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0168, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6239, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.2797, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.9506, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.2749, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.9078, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3172, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.4104, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.8506, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.3232, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.7472, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1467], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.1467], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.1467], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.1467], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.1467], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4340],
        [ 0.1850],
        [-0.1416],
        [ 0.0684],
        [-0.2273]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2344, 1.0195, 1.1498, 1.1786, 1.2274], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:34:50,532] Trial 34 finished with value: 0.5 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 300, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9013],
        [-0.1275],
        [-0.7730],
        [ 0.3364],
        [ 0.3959]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4060],
        [-0.9698],
        [ 0.9607],
        [-0.8956],
        [ 0.2662]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3634, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2008, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5230, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4902, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3160, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1252, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4758, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4311, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6549, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2708, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8269, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.8201, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3160, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.8612, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2143, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6743, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.9963, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2571, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.9668, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.1853, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0733], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([1.6200], dtype=torch.float64)  Action:  3  Reward Received:  1.0
State:  tensor([2.0600], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7600], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0600], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3468],
        [ 0.4004],
        [-0.1339],
        [ 0.2399],
        [-0.0658]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3888, 1.1715, 1.5329, 1.4726, 1.3208], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:34:56,337] Trial 35 finished with value: 0.5 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.3606],
        [ 0.5541],
        [-0.2875],
        [-0.6745],
        [ 0.6554]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4093],
        [-0.0903],
        [-0.4935],
        [-0.9716],
        [-0.6631]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4980, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3170, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5439, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6517, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5057, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2520, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5736, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4716, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7216, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4696, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9788, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8586, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3913, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7993, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4294, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7999, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.0452, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3387, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.8502, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4032, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1040], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2280], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2280], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.6210],
        [ 0.6479],
        [-0.1826],
        [ 0.1766],
        [-0.0912]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5626, 1.2496, 1.5629, 1.6333, 1.5152], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:34:59,990] Trial 36 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 250, 'minibatch_size': 32, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5024],
        [-0.2886],
        [ 0.0920],
        [ 0.6434],
        [ 0.3554]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0282],
        [-0.0345],
        [ 0.5206],
        [ 0.0372],
        [ 0.2443]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3745, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2333, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4565, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4997, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3047, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1302, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4431, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3170, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5645, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2369, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9174, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6259, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1955, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6209, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1779, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7190, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7962, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0823, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6735, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1228, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0636], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.5564], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2473], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.5564], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2473], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.3303],
        [ 1.1424],
        [-0.7593],
        [ 0.3527],
        [-0.3691]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4592, 1.1606, 1.5048, 1.4773, 1.3282], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:35:18,860] Trial 37 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 300, 'high': 550, 'minibatch_size': 256, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2973],
        [-0.4818],
        [ 0.0968],
        [-0.0653],
        [ 0.1680]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7607],
        [-0.3698],
        [ 0.4215],
        [-0.2293],
        [ 0.9686]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4920, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4903, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6971, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7064, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4777, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3442, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5858, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6246, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7529, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4404, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.1118, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7358, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.5108, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.8259, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3817, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.8513, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.9040, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.3832, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.9078, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.3160, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.2667], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0867], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0867], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0867], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0867], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3520],
        [ 0.2273],
        [-0.1724],
        [ 0.1106],
        [-0.0888]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5859, 1.4297, 1.7430, 1.6769, 1.5014], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:35:24,669] Trial 38 finished with value: -0.5 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.6067],
        [ 0.2748],
        [-0.1684],
        [-0.7757],
        [ 0.3805]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5576],
        [ 0.8152],
        [-0.3286],
        [ 0.0920],
        [-0.1680]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4277, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2499, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5221, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5798, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4216, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2790, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3620, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4695, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6053, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3877, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9577, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6043, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3560, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6604, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3143, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7322, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7743, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2764, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6990, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2628, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1960], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3560], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4440], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3560], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4440], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.5996],
        [ 0.4520],
        [-0.2118],
        [ 0.1028],
        [-0.1369]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5452, 1.1613, 1.5636, 1.5596, 1.4485], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:35:28,298] Trial 39 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 400, 'high': 250, 'minibatch_size': 32, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1172],
        [-0.8867],
        [-0.1684],
        [-0.7957],
        [-0.5389]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5835],
        [ 0.2820],
        [ 0.2584],
        [ 0.0062],
        [-0.0436]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4983, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4123, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5357, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7223, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3674, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3419, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6386, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4667, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7666, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2835, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1557, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9078, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3845, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8194, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1838, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9458, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.2114, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2918, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8789, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0713, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1229], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9200], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3486], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9200], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3486], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.6931],
        [ 1.0024],
        [-0.3060],
        [ 0.1965],
        [-0.3715]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5835, 1.2891, 1.5733, 1.6981, 1.4130], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:35:34,082] Trial 40 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 350, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.0576],
        [ 0.6126],
        [-0.4493],
        [-0.3196],
        [ 0.4899]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.5225],
        [-0.7044],
        [ 0.2170],
        [-0.0780],
        [ 0.4532]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3314, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3231, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5638, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4827, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4216, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1249, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4872, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4838, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5473, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3094, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8947, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6701, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3947, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6192, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1844, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7072, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8191, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3221, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6778, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0826, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0867], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.4956], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.2800], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6711], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2800], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0680],
        [ 0.8487],
        [-0.4135],
        [ 0.3338],
        [-0.5800]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4239, 1.2496, 1.5996, 1.4538, 1.4718], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:35:45,388] Trial 41 finished with value: 1.5999999999999999 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.2227],
        [ 0.7225],
        [ 0.6425],
        [-0.0267],
        [-0.9838]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9440],
        [ 0.7514],
        [-0.6555],
        [-0.1453],
        [ 0.1726]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3706, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2507, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4674, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6003, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3817, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1312, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5071, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3683, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6247, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3222, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8694, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7875, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2600, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6515, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2572, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7016, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9672, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1905, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6686, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2155, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0111], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6756], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2489], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6756], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2489], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0069],
        [ 1.0783],
        [-0.4167],
        [ 0.1029],
        [-0.2501]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3818, 1.2388, 1.4720, 1.5991, 1.3844], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:35:56,621] Trial 42 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.8532],
        [-0.8238],
        [ 0.4121],
        [ 0.7106],
        [ 0.7787]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.4345],
        [-0.8322],
        [-0.5820],
        [ 0.3200],
        [ 0.8381]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3817, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2095, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4124, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5349, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4623, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1665, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4351, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2960, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5903, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3881, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8839, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7314, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1432, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6631, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2908, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7056, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9183, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0468, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7090, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2293, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0244], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7156], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2444], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7156], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2444], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.9784],
        [ 1.0256],
        [-0.5290],
        [ 0.2518],
        [-0.3371]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4057, 1.1844, 1.4253, 1.5288, 1.4705], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:36:08,035] Trial 43 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1072],
        [-0.2995],
        [ 0.3352],
        [ 0.7917],
        [-0.8341]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.8605],
        [-0.6954],
        [-0.0735],
        [-0.6173],
        [-0.0109]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5627, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5626, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7196, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7380, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6177, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2877, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7415, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5964, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7447, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4348, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0909, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8695, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5082, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7495, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3039, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8183, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.0469, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3860, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7562, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1225, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0689], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7667], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3267], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7667], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3267], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0669],
        [ 0.6939],
        [-0.4781],
        [ 0.0261],
        [-0.7096]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6362, 1.5148, 1.7526, 1.7362, 1.6666], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:36:19,292] Trial 44 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1375],
        [-0.9517],
        [ 0.3701],
        [-0.1420],
        [-0.8433]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9745],
        [ 0.0810],
        [-0.4751],
        [-0.7946],
        [-0.0576]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2754, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1959, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3833, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6778, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3449, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1202, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3079, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3034, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6858, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2982, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9176, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4543, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1990, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6964, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2371, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6404, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6544, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0562, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7108, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1536, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0745], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6327], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6327], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6327], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6327], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.1376],
        [ 0.8215],
        [-0.5860],
        [ 0.0592],
        [-0.3428]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3602, 1.1346, 1.4270, 1.6733, 1.3705], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:36:26,185] Trial 45 finished with value: -0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 300, 'high': 550, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.6579],
        [-0.7635],
        [ 0.1397],
        [-0.8335],
        [-0.0733]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9880],
        [ 0.5451],
        [ 0.0316],
        [-0.5367],
        [ 0.6380]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.9180, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8355, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.1119, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.0249, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.9937, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.5488, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.0921, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.9318, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.0527, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.8243, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.2734, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.2835, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.7975, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.0734, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.6980, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.0508, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.4383, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.6889, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.0901, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.5959, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0600], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([1.5267], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.9000], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.9000], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.9000], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4395],
        [ 0.3054],
        [-0.2143],
        [ 0.0330],
        [-0.2016]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.9444, 1.8172, 2.1248, 2.0229, 2.0058], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:36:30,871] Trial 46 finished with value: 1.4000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 32, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.9884],
        [ 0.7021],
        [ 0.2990],
        [ 0.3812],
        [-0.2508]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[0.7053],
        [0.8036],
        [0.7499],
        [0.3765],
        [0.7559]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5790, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4103, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5372, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5409, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5586, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.4107, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5136, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5160, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5807, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4929, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0743, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7203, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4738, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6603, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3614, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8129, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8809, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4410, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7221, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2592, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1089], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.1089], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.1089], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.1089], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.1089], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.3519],
        [ 0.8305],
        [-0.1698],
        [ 0.3198],
        [-0.5283]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.7262, 1.3199, 1.5556, 1.5060, 1.6161], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:36:50,024] Trial 47 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 256, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.8174],
        [-0.2444],
        [-0.5643],
        [-0.7210],
        [-0.2179]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4699],
        [-0.2454],
        [-0.6381],
        [-0.8551],
        [ 0.2726]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4809, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3193, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5226, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6039, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3975, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2748, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4964, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4559, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6484, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3723, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9428, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7817, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3484, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7200, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3317, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7344, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.9608, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2809, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.7650, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3061, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0440], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3480], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4040], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3480], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4040], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.5724],
        [ 0.4920],
        [-0.1854],
        [ 0.1236],
        [-0.0701]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5061, 1.2976, 1.5308, 1.5984, 1.4006], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:37:01,280] Trial 48 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 250, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.2416],
        [ 0.9745],
        [-0.7363],
        [-0.9755],
        [ 0.8458]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6263],
        [-0.2387],
        [ 0.9616],
        [-0.9934],
        [-0.3042]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2815, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1794, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3055, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4920, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3170, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0852, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3349, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2323, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4848, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2714, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8614, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5122, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1489, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4767, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2195, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.5976, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7211, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0505, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4671, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1583, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0178], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7378], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2244], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7378], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2244], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.9499],
        [ 0.7524],
        [-0.3542],
        [-0.0346],
        [-0.2204]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2984, 1.1661, 1.3118, 1.4926, 1.3209], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:37:07,097] Trial 49 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9195],
        [-0.6043],
        [-0.7553],
        [-0.8614],
        [-0.5778]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.5537],
        [ 0.2883],
        [-0.1786],
        [ 0.6999],
        [-0.0910]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3565, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3460, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4068, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4883, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2564, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2003, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5371, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3480, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5312, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1104, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9738, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.8143, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2629, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5935, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.8986, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7746, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.0580, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.1880, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.6482, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.7123, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.2600], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.2467], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7933], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7933], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7933], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.8999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2929],
        [ 0.3584],
        [-0.1101],
        [ 0.0805],
        [-0.2739]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4327, 1.2528, 1.4354, 1.4674, 1.3277], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:37:12,923] Trial 50 finished with value: -0.8999999999999999 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8132],
        [-0.1987],
        [-0.7019],
        [ 0.5217],
        [ 0.4016]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3271],
        [-0.6425],
        [-0.8722],
        [-0.3777],
        [ 0.1381]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3790, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2326, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3867, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3960, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3771, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1076, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4924, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2377, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4677, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2053, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9562, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6374, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1546, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5077, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1095, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7911, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7956, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0639, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5514, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0049, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0244], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6889], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3311], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3311], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3311], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.8999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8848],
        [ 0.8473],
        [-0.4858],
        [ 0.2338],
        [-0.5600]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4006, 1.2119, 1.3986, 1.3903, 1.3907], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:37:25,542] Trial 51 finished with value: -0.8999999999999999 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.0290],
        [ 0.1523],
        [ 0.1534],
        [-0.7592],
        [ 0.9554]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1784],
        [-0.5550],
        [ 0.7429],
        [ 0.6227],
        [ 0.1114]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.0565, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1801, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1198, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2747, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0894, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8620, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3426, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0265, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2972, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0158, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7329, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4504, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9645, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3121, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.9670, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.5029, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6427, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.8541, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3386, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8800, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7578], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3311], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3311], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3311], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.8999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8416],
        [ 0.7033],
        [-0.4039],
        [ 0.0972],
        [-0.3183]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.1406, 1.1097, 1.1602, 1.2650, 1.1212], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:37:38,219] Trial 52 finished with value: -0.8999999999999999 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.8014],
        [-0.2899],
        [-0.3357],
        [-0.5324],
        [-0.4146]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9480],
        [-0.5774],
        [ 0.9303],
        [-0.9733],
        [ 0.7726]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3664, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3185, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5676, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6086, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3724, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1807, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4301, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5032, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6815, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3627, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8835, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6086, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4003, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7982, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3471, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6433, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7529, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3170, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8925, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3345, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0844], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7333], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7333], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7333], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7333], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.1144],
        [ 0.6694],
        [-0.3862],
        [ 0.4376],
        [-0.0585]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4605, 1.2620, 1.6002, 1.5716, 1.3774], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:37:50,896] Trial 53 finished with value: -0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9363],
        [ 0.2932],
        [-0.7460],
        [-0.1565],
        [ 0.9554]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3947],
        [-0.1923],
        [-0.4723],
        [ 0.7483],
        [-0.3460]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4133, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3729, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5252, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6776, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3610, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2160, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5266, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4406, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6653, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3471, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9393, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7422, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3218, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6481, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3276, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7215, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9119, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2284, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6346, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3123, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0889], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6889], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2600], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6889], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2600], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.1530],
        [ 0.8983],
        [-0.4948],
        [-0.0716],
        [-0.0811]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5158, 1.2931, 1.5692, 1.6839, 1.3682], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:38:04,515] Trial 54 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[0.7288],
        [0.0047],
        [0.7091],
        [0.6664],
        [0.8709]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2674],
        [-0.2876],
        [ 0.4254],
        [-0.6641],
        [ 0.5787]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2193, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1765, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3156, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2290, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2303, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0729, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3062, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2568, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2577, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1226, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9265, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4359, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1979, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2865, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0149, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7286, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6114, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1183, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3253, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8693, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0733], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.4644], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.2689], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2689], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2689], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.7485],
        [ 0.6634],
        [-0.3011],
        [ 0.1469],
        [-0.5508]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2742, 1.1278, 1.3377, 1.2182, 1.2707], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:38:17,162] Trial 55 finished with value: 1.4000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8611],
        [-0.3380],
        [ 0.8326],
        [-0.9930],
        [-0.8891]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0747],
        [-0.4548],
        [ 0.1422],
        [ 0.6632],
        [-0.2320]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3466, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2105, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3908, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5038, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3717, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1604, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3828, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3168, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5493, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3005, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8812, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6413, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2058, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6174, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1937, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6834, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8243, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1272, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6656, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1180, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0714], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.8857], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.8857], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8145],
        [ 0.7538],
        [-0.3238],
        [ 0.1987],
        [-0.3115]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4048, 1.1567, 1.4139, 1.4896, 1.3939], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:38:23,072] Trial 56 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 350, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.6435],
        [ 0.3163],
        [ 0.3051],
        [ 0.3514],
        [-0.9469]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4210],
        [ 0.3389],
        [-0.8003],
        [-0.5457],
        [-0.3113]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2840, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2040, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3074, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4581, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2993, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0485, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3951, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2158, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4649, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1618, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9170, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5018, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1646, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4687, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0850, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7129, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6674, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0851, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4745, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.9659, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0467], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6933], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3133], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6933], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3133], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8832],
        [ 0.7166],
        [-0.3438],
        [ 0.0253],
        [-0.5157]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3252, 1.1706, 1.3235, 1.4569, 1.3234], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:38:44,910] Trial 57 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 300, 'high': 450, 'minibatch_size': 256, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.5769],
        [ 0.2992],
        [-0.4979],
        [-0.7757],
        [ 0.7546]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6478],
        [ 0.5949],
        [ 0.6668],
        [-0.5783],
        [-0.5112]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5703, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3125, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3823, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6721, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4293, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3560, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5419, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2978, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6502, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3327, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0421, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8782, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1739, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6181, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1912, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7705, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.1690, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0668, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5904, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0687, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0511], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7644], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2422], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7644], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2422], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.1212],
        [ 1.2008],
        [-0.4424],
        [-0.1146],
        [-0.5055]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6276, 1.2511, 1.4050, 1.6780, 1.4552], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:38:50,740] Trial 58 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8426],
        [ 0.5399],
        [-0.0179],
        [ 0.2860],
        [ 0.1270]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5143],
        [-0.8498],
        [-0.6034],
        [ 0.2317],
        [-0.6212]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.1995, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2101, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3024, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3456, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2550, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0298, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4029, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2292, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3801, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1936, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8732, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5810, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1617, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4120, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1370, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7297, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7442, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0998, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4412, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0850, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0564], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.5800], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2455], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2455], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2455], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.8999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8971],
        [ 1.0199],
        [-0.3870],
        [ 0.1827],
        [-0.3248]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2500, 1.1526, 1.3242, 1.3353, 1.2733], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:38:54,440] Trial 59 finished with value: -0.8999999999999999 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 550, 'minibatch_size': 32, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2044],
        [ 0.7592],
        [-0.3952],
        [ 0.5016],
        [-0.6167]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[0.0261],
        [0.8000],
        [0.5391],
        [0.8653],
        [0.5258]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2926, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1630, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3123, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4081, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2368, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1525, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2902, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2536, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4546, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1892, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9144, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5065, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1538, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5338, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.1085, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7283, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.6755, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0759, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.5956, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0454, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.2067], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0867], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.6733], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0867], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.6733], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3001],
        [ 0.2726],
        [-0.1257],
        [ 0.0997],
        [-0.1018]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3546, 1.1067, 1.3382, 1.3875, 1.2578], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:39:05,725] Trial 60 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0431],
        [-0.0564],
        [-0.4168],
        [ 0.3848],
        [-0.6293]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6570],
        [ 0.8017],
        [-0.3301],
        [-0.1237],
        [-0.6119]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6784, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4148, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6849, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5240, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5660, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3270, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6258, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5899, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6302, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4533, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9881, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8294, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4982, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7326, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3445, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7300, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.9844, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4284, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.8106, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2617, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0960], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.9840], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.5480], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3160], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5480], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.7774],
        [ 0.4669],
        [-0.2103],
        [ 0.2349],
        [-0.2494]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.7530, 1.3700, 1.7051, 1.5015, 1.5899], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:39:11,559] Trial 61 finished with value: 1.5999999999999999 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.6066],
        [-0.2711],
        [ 0.7338],
        [ 0.3161],
        [ 0.7457]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.5570],
        [-0.5702],
        [-0.8293],
        [-0.2478],
        [-0.5189]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2497, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0713, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3346, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4529, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1984, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1328, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2204, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2580, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4434, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1396, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9910, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4013, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1650, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4319, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0683, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8008, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6439, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.0404, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4164, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.9727, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1440], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2960], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4440], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2960], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4440], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3896],
        [ 0.4970],
        [-0.2553],
        [-0.0317],
        [-0.1959]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3058, 0.9997, 1.3713, 1.4574, 1.2266], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:39:17,429] Trial 62 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.5294],
        [-0.8525],
        [-0.2570],
        [ 0.1571],
        [-0.1756]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6680],
        [-0.9850],
        [-0.2654],
        [ 0.3730],
        [ 0.7694]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6141, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5794, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5084, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5017, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.7918, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.4488, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7530, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4658, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5243, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6926, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2156, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9979, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4057, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5561, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5526, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0338, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.1889, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3589, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5810, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4435, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0680], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0680], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0680], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0680], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0680], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4591],
        [ 0.4822],
        [-0.1183],
        [ 0.0627],
        [-0.2755]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6453, 1.5466, 1.5165, 1.4974, 1.8105], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:39:23,286] Trial 63 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5958],
        [-0.8372],
        [-0.6322],
        [-0.6051],
        [-0.1257]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4076],
        [-0.5445],
        [ 0.9699],
        [ 0.6756],
        [ 0.4020]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.8224, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2802, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8720, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9562, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6188, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.5467, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4259, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7624, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.0356, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6093, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9914, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7195, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5417, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.1955, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5903, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7748, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.8340, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4556, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.2578, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.5829, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1480], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2120], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2120], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2120], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2120], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.9845],
        [ 0.5205],
        [-0.3914],
        [ 0.2835],
        [-0.0338]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.9681, 1.2032, 1.9300, 1.9142, 1.6238], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:39:29,164] Trial 64 finished with value: -0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7808],
        [-0.3964],
        [-0.8438],
        [ 0.9694],
        [ 0.3452]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.7136],
        [ 0.4251],
        [ 0.5148],
        [ 0.7122],
        [-0.5707]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4139, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1275, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4140, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4423, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3382, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1924, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3129, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3632, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4110, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2251, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0783, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4085, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3369, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3950, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1667, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8857, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5697, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2927, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.3678, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0684, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0360], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2760], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5560], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2760], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5560], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4260],
        [ 0.3567],
        [-0.0978],
        [-0.0600],
        [-0.2176]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4293, 1.1146, 1.4175, 1.4444, 1.3460], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:39:35,020] Trial 65 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.6022],
        [ 0.5896],
        [-0.5263],
        [ 0.1800],
        [ 0.5779]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.8172],
        [ 0.7928],
        [ 0.8238],
        [ 0.9725],
        [-0.5646]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.8926, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8008, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.1040, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.1020, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.8109, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.6917, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9553, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.9616, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.1528, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.7822, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3242, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.2379, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7013, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.2458, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.7297, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0802, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.4256, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5283, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.3075, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6948, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0867], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.5267], dtype=torch.float64)  Action:  3  Reward Received:  1.0
State:  tensor([0.7156], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2422], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7156], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.2917],
        [ 0.9935],
        [-0.9154],
        [ 0.3268],
        [-0.1845]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([2.0046, 1.7147, 2.1834, 2.0737, 1.8269], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:39:40,921] Trial 66 finished with value: 0.5 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.5287],
        [-0.0855],
        [ 0.1149],
        [ 0.5162],
        [ 0.6808]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4297],
        [-0.7306],
        [ 0.4669],
        [ 0.7821],
        [ 0.5792]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4063, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2157, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3314, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6182, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4498, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1651, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2908, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2627, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6471, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3493, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8794, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.3797, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1813, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6814, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2304, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6159, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.4616, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.1062, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.7130, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.1207, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.2867], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1867], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1867], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1867], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1867], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4160],
        [ 0.1294],
        [-0.1185],
        [ 0.0499],
        [-0.1732]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5256, 1.1786, 1.3654, 1.6039, 1.4994], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:39:46,768] Trial 67 finished with value: -0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5287],
        [ 0.6211],
        [ 0.1029],
        [-0.2475],
        [-0.9955]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.5969],
        [ 0.9871],
        [-0.6639],
        [ 0.5617],
        [-0.3881]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2477, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1493, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2336, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3337, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1904, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0753, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2675, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1628, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3777, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1458, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8052, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4529, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0519, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4468, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0760, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.5514, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6272, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9476, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5116, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0104, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0978], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7622], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2622], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7622], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2622], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0478],
        [ 0.7193],
        [-0.4305],
        [ 0.2678],
        [-0.2708]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3501, 1.0789, 1.2757, 1.3075, 1.2168], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:39:52,615] Trial 68 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 400, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.2078],
        [ 0.9084],
        [-0.9255],
        [ 0.0891],
        [-0.6039]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6280],
        [ 0.8294],
        [-0.0812],
        [-0.7325],
        [ 0.9381]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3720, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1212, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4473, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4404, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3046, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0577, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3970, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2718, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4979, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2403, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8318, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5952, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1457, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5393, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.1941, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6944, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.7158, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0690, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.5644, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.1660, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0933], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([1.6400], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.9933], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0333], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.9933], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3493],
        [ 0.3065],
        [-0.1950],
        [ 0.0640],
        [-0.0714]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4046, 1.0926, 1.4655, 1.4344, 1.3113], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:39:58,486] Trial 69 finished with value: 1.5999999999999999 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7623],
        [-0.2528],
        [ 0.2761],
        [ 0.3184],
        [-0.7580]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8803],
        [ 0.1748],
        [ 0.2014],
        [-0.5803],
        [-0.8228]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3942, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2481, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5257, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5450, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3911, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1000, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4623, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4464, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6317, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3600, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7581, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7111, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3542, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.7325, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3240, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5788, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.8416, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.3058, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.7853, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.3051, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0400], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.8000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0400], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.8000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4203],
        [ 0.3059],
        [-0.1134],
        [ 0.1239],
        [-0.0443]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4363, 1.2175, 1.5371, 1.5326, 1.3955], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:40:04,325] Trial 70 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.7031],
        [ 0.9568],
        [-0.2303],
        [-0.3031],
        [-0.7111]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.8363],
        [ 0.5058],
        [ 0.9388],
        [-0.8676],
        [-0.9180]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5399, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3921, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5983, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6090, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4358, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3258, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5427, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4369, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6766, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3739, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0713, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7216, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2452, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.7569, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3003, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.8320, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.8899, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0648, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.8324, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.2312, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1867], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0600], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7533], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0600], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7533], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3779],
        [ 0.2657],
        [-0.2848],
        [ 0.1193],
        [-0.1092]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6104, 1.3425, 1.6515, 1.5868, 1.4562], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:40:10,175] Trial 71 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1419],
        [ 0.8661],
        [ 0.1266],
        [ 0.0381],
        [-0.7513]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0319],
        [-0.7937],
        [ 0.6544],
        [-0.5331],
        [ 0.6083]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4176, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4342, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5661, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7275, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5040, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2076, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6664, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4751, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7305, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4630, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9909, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.9061, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3812, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.7336, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4206, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7518, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.1705, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2776, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.7370, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.3739, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1400], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1267], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7667], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1267], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7667], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3352],
        [ 0.3706],
        [-0.1452],
        [ 0.0048],
        [-0.0654]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4646, 1.3823, 1.5864, 1.7268, 1.5131], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:40:16,011] Trial 72 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9171],
        [-0.7479],
        [ 0.7897],
        [-0.3792],
        [ 0.0227]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0634],
        [-0.8206],
        [-0.4654],
        [-0.0108],
        [ 0.1078]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5758, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3025, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6375, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6491, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4892, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1862, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5730, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4191, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7213, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3273, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9541, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7342, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2890, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.7643, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2309, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7479, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.8774, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.1733, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.8026, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.1451, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0733], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.9800], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.9800], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4297],
        [ 0.2984],
        [-0.2410],
        [ 0.0796],
        [-0.1786]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6073, 1.2806, 1.6552, 1.6433, 1.5023], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:40:21,902] Trial 73 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0586],
        [-0.2914],
        [-0.0484],
        [-0.2983],
        [ 0.3490]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9321],
        [-0.0227],
        [ 0.2019],
        [-0.3451],
        [ 0.8687]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3662, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3488, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3444, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5471, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3959, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1779, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4888, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2458, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5413, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3661, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9856, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6318, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1451, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5353, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3357, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7712, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.7912, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0328, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.5287, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.3018, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1200], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1000], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7467], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1000], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7467], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3005],
        [ 0.2234],
        [-0.1574],
        [-0.0093],
        [-0.0475]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4022, 1.3220, 1.3633, 1.5482, 1.4016], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:40:27,741] Trial 74 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4877],
        [ 0.3742],
        [-0.6850],
        [-0.3758],
        [ 0.4412]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.7503],
        [-0.5724],
        [ 0.6307],
        [ 0.1730],
        [-0.4843]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2479, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3020, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3495, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4779, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2595, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0553, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5074, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2268, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5567, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1852, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9358, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6348, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1507, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6056, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1391, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7610, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8210, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0395, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6772, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0717, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0514], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9829], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4200], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9829], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4200], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.5228],
        [ 0.5572],
        [-0.3328],
        [ 0.2140],
        [-0.2017]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2748, 1.2734, 1.3666, 1.4669, 1.2699], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:40:46,679] Trial 75 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 350, 'minibatch_size': 256, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.8103],
        [-0.0877],
        [-0.7484],
        [ 0.8512],
        [-0.5811]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6769],
        [-0.3669],
        [-0.5246],
        [ 0.7246],
        [-0.4335]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.1167, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0282, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1991, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2175, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1222, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8354, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1771, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0105, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2816, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.9931, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7095, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2437, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9261, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3103, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.9353, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.5108, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3489, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.7929, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3555, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8441, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0067], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6911], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6911], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6911], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6911], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8852],
        [ 0.4686],
        [-0.5935],
        [ 0.2017],
        [-0.4062]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.1226, 1.0250, 1.2030, 1.2161, 1.1249], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:40:56,779] Trial 76 finished with value: -0.5 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 300, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.6256],
        [-0.1565],
        [-0.8519],
        [ 0.9762],
        [ 0.5425]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4047],
        [-0.9657],
        [-0.1800],
        [-0.2032],
        [ 0.8139]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4488, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2180, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4775, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7354, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3490, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3093, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3838, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3877, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7387, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3029, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0383, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7059, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2133, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7449, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2135, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8490, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.9309, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.0914, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.7493, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.1510, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1560], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3600], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4360], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3600], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4360], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4982],
        [ 0.5921],
        [-0.3206],
        [ 0.0115],
        [-0.1644]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5265, 1.1256, 1.5275, 1.7336, 1.3746], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:41:03,711] Trial 77 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.3890],
        [ 0.6814],
        [ 0.1865],
        [ 0.3858],
        [-0.9191]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.7435],
        [ 0.2200],
        [ 0.1212],
        [ 0.8229],
        [ 0.7422]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2726, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2778, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4237, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5965, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3584, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1079, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4453, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3586, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6140, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3282, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9489, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6069, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2957, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6310, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2991, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7332, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8264, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2104, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6539, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2596, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0800], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.5982], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2382], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.5982], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2382], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0409],
        [ 1.0586],
        [-0.4117],
        [ 0.1108],
        [-0.1906]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3559, 1.1931, 1.4567, 1.5876, 1.3736], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:41:07,480] Trial 78 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 550, 'minibatch_size': 32, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.9214],
        [ 0.4980],
        [-0.6993],
        [ 0.6780],
        [-0.9156]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7680],
        [-0.7986],
        [-0.2042],
        [ 0.5146],
        [ 0.2968]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4136, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3460, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4864, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4897, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3922, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1056, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6238, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2853, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4831, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2698, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8933, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.8152, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1467, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4785, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.1855, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6904, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.9983, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0142, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.4742, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.1048, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0467], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1133], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.9267], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.9267], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.9267], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.8999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3500],
        [ 0.3156],
        [-0.2285],
        [-0.0075],
        [-0.1390]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4299, 1.3313, 1.4970, 1.4900, 1.3987], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:41:13,314] Trial 79 finished with value: -0.8999999999999999 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.7544],
        [-0.8331],
        [ 0.1909],
        [ 0.3490],
        [-0.2888]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9814],
        [-0.3262],
        [-0.6299],
        [ 0.2099],
        [-0.7927]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.1717, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1090, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3109, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3164, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2201, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9247, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2751, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2244, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3779, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1403, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7326, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4043, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1571, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4258, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0783, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.5215, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5462, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0832, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4784, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0101, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0111], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6956], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2711], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6956], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2711], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.9501],
        [ 0.6388],
        [-0.3327],
        [ 0.2366],
        [-0.3069]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.1823, 1.1019, 1.3146, 1.3138, 1.2235], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:41:24,580] Trial 80 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 100, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.6734],
        [-0.1061],
        [ 0.6437],
        [-0.1449],
        [-0.0111]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2570],
        [ 0.0172],
        [ 0.7966],
        [-0.1393],
        [-0.1015]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4638, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3736, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4905, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5208, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4504, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2546, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5200, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4269, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5598, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3794, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0071, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6931, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3517, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6058, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2955, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7551, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.8694, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2751, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6527, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2100, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0360], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2960], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4080], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2960], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4080], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.5625],
        [ 0.3934],
        [-0.1709],
        [ 0.1047],
        [-0.1907]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4841, 1.3595, 1.4966, 1.5170, 1.4572], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:41:30,430] Trial 81 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.8479],
        [-0.9132],
        [ 0.4870],
        [ 0.6310],
        [ 0.1808]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2933],
        [-0.9890],
        [ 0.7103],
        [-0.9427],
        [-0.5872]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6065, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4119, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5629, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6532, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5644, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3385, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6548, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4683, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6136, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3765, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1449, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8303, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3999, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5849, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2407, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9689, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.9899, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3377, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5589, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.1173, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0040], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3080], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5520], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5520], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5520], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.8999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4889],
        [ 0.4432],
        [-0.1726],
        [-0.0724],
        [-0.3428]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6084, 1.4102, 1.5636, 1.6535, 1.5657], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:41:36,246] Trial 82 finished with value: -0.8999999999999999 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.5968],
        [-0.7838],
        [-0.1249],
        [ 0.5205],
        [-0.2540]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.4729],
        [-0.3200],
        [ 0.5014],
        [-0.4284],
        [-0.6363]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2730, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0949, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2760, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4099, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1844, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0783, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2016, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2447, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4395, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1409, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8722, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3145, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2117, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4709, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0950, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6502, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.4361, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1760, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5047, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0455, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1640], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2640], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2640], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2640], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2640], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.5662],
        [ 0.3102],
        [-0.0908],
        [ 0.0862],
        [-0.1262]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3659, 1.0440, 1.2909, 1.3958, 1.2051], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:41:42,254] Trial 83 finished with value: -0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0378],
        [ 0.0313],
        [ 0.4395],
        [ 0.4288],
        [ 0.2471]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9133],
        [ 0.8195],
        [-0.4233],
        [-0.9364],
        [ 0.3216]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4392, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3543, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4814, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4457, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4492, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2128, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4669, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3970, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4570, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3385, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9953, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5751, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3159, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4679, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2322, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6941, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7250, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2036, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4830, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0850, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0360], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.8200], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.4360], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4360], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4360], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.5662],
        [ 0.2817],
        [-0.2111],
        [ 0.0283],
        [-0.2768]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4596, 1.3441, 1.4890, 1.4447, 1.4592], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:41:48,830] Trial 84 finished with value: 1.4000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.6823],
        [ 0.0101],
        [-0.1615],
        [ 0.9560],
        [-0.3539]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.7923],
        [ 0.7822],
        [ 0.6403],
        [ 0.1720],
        [ 0.5600]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3221, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0497, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3006, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4979, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2767, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1360, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1217, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1624, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5460, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1562, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0147, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1686, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0723, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5774, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0776, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8037, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2502, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9155, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6320, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.9410, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0489], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7422], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7422], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7422], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7422], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.7478],
        [ 0.2891],
        [-0.5554],
        [ 0.1934],
        [-0.4843]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3587, 1.0356, 1.3278, 1.4884, 1.3004], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:41:54,628] Trial 85 finished with value: -0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7786],
        [ 0.5825],
        [-0.3992],
        [-0.5954],
        [ 0.8471]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6346],
        [-0.6259],
        [-0.5175],
        [ 0.1792],
        [ 0.9970]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4922, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3422, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4305, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5802, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3805, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3612, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4627, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3497, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6094, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3082, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.1665, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6420, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2297, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6527, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2007, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.9482, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.8429, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0951, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.7014, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0802, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1533], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.3133], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.6733], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.3133], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.6733], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2518],
        [ 0.2318],
        [-0.1553],
        [ 0.0561],
        [-0.1390]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5308, 1.3067, 1.4543, 1.5716, 1.4019], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:42:00,436] Trial 86 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.7421],
        [ 0.2476],
        [-0.0402],
        [-0.6008],
        [ 0.4003]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3662],
        [-0.3056],
        [-0.0186],
        [ 0.8548],
        [-0.5809]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4239, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2621, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5766, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6160, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4126, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1430, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4950, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4560, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6753, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3502, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8648, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7256, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3365, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7340, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2884, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6803, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8785, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2573, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7730, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2474, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0800], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6800], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3067], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6800], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3067], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.2392],
        [ 1.0273],
        [-0.5322],
        [ 0.2616],
        [-0.2753]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5230, 1.1800, 1.6192, 1.5951, 1.4346], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:42:11,928] Trial 87 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.4949],
        [-0.5006],
        [-0.4899],
        [ 0.9238],
        [ 0.7957]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9707],
        [-0.7025],
        [ 0.1086],
        [ 0.7377],
        [-0.6658]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6685, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5357, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8951, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9979, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.8702, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.4220, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7676, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7567, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9755, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.8328, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9808, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.1825, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5091, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9354, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.7660, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6823, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.4632, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3416, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9083, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.7208, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0743], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9429], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2914], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9429], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2914], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.1354],
        [ 1.0678],
        [-0.6372],
        [-0.1031],
        [-0.1720]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.7529, 1.4564, 1.9424, 2.0055, 1.8830], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:42:17,753] Trial 88 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 350, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.8436],
        [ 0.2452],
        [ 0.6595],
        [ 0.4297],
        [ 0.5724]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.3246],
        [-0.4186],
        [ 0.7579],
        [ 0.2222],
        [-0.4175]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2215, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1513, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2303, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3804, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1897, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0714, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2739, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1613, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4107, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1532, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8686, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4395, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0680, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4516, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1039, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6269, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6369, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9568, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5004, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0451, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0511], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7289], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2222], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7289], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2222], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8773],
        [ 0.7166],
        [-0.4036],
        [ 0.1770],
        [-0.2134]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2663, 1.1146, 1.2510, 1.3714, 1.2006], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:42:37,904] Trial 89 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 450, 'minibatch_size': 256, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.6350],
        [ 0.5394],
        [ 0.8657],
        [-0.0957],
        [-0.8954]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.4830],
        [ 0.2691],
        [ 0.9919],
        [-0.4131],
        [-0.2435]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5220, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2321, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5082, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3779, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5361, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2880, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4288, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3417, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4221, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4136, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0899, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5954, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2008, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4596, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3099, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.9424, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.7194, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0959, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.4874, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.2327, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1667], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.1667], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.1667], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.1667], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.1667], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3161],
        [ 0.2658],
        [-0.2249],
        [ 0.0597],
        [-0.1655]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5747, 1.1878, 1.5456, 1.3680, 1.5637], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:42:48,088] Trial 90 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 300, 'high': 150, 'minibatch_size': 128, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.6568],
        [-0.9336],
        [ 0.6510],
        [-0.1795],
        [-0.7114]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3329],
        [ 0.4432],
        [ 0.7927],
        [-0.2003],
        [ 0.1384]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4313, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5048, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4567, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4476, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4351, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2506, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6080, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3680, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4088, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3020, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0474, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7242, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2681, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.3651, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.1522, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7515, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.8932, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.1228, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.3014, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.9341, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.2733], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.8067], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.8067], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.8067], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.8067], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -1.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3388],
        [ 0.1936],
        [-0.1664],
        [-0.0728],
        [-0.2497]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5239, 1.4519, 1.5022, 1.4675, 1.5034], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:42:52,893] Trial 91 finished with value: -1.0 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 32, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.0682],
        [-0.3342],
        [ 0.6420],
        [ 0.0990],
        [ 0.4654]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8733],
        [ 0.7424],
        [ 0.8335],
        [-0.2360],
        [ 0.3368]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6141, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3593, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4999, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4141, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4882, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3012, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5392, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3612, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4390, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2734, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0801, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6664, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2631, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4566, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.1216, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.9201, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.7584, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.1922, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.4694, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0118, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0800], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0800], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0800], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0800], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0800], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3529],
        [ 0.2029],
        [-0.1564],
        [ 0.0281],
        [-0.2423]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6423, 1.3430, 1.5124, 1.4118, 1.5076], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:42:57,773] Trial 92 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 32, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1054],
        [-0.8336],
        [-0.2427],
        [-0.3513],
        [-0.0647]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.5845],
        [ 0.5699],
        [-0.5256],
        [-0.8706],
        [-0.7884]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4481, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4102, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4926, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5164, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5706, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3229, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4907, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4087, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5176, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4715, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.1035, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6320, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2616, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5197, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2978, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.8570, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.7906, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0964, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.5220, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.1027, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.2733], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.2733], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.2733], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.2733], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.2733], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2888],
        [ 0.1859],
        [-0.1936],
        [ 0.0027],
        [-0.2286]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5270, 1.3594, 1.5455, 1.5157, 1.6331], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:43:02,456] Trial 93 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 32, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.4193],
        [ 0.2966],
        [-0.6517],
        [-0.5871],
        [ 0.6082]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1235],
        [ 0.4501],
        [-0.1533],
        [-0.7894],
        [ 0.9230]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4789, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1891, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3873, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5447, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2630, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3614, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3138, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3160, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6057, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2816, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.2207, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.4632, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2306, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6787, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3037, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.0880, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.6040, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.1501, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.7475, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.3246, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0667], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.3067], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.3067], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.3067], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.3067], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1745],
        [ 0.1852],
        [-0.1059],
        [ 0.0905],
        [ 0.0275]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4905, 1.1768, 1.3943, 1.5387, 1.2612], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:43:07,170] Trial 94 finished with value: -0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 32, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4620],
        [-0.1702],
        [-0.3000],
        [ 0.2236],
        [-0.6318]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0873],
        [-0.8622],
        [ 0.6037],
        [ 0.0228],
        [-0.6089]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3943, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2641, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5218, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7066, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4306, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1807, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4425, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3580, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7345, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3473, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0300, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5683, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2425, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7542, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2886, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8545, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7149, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1079, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7772, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2202, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0311], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7556], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7556], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7556], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7556], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.7450],
        [ 0.6222],
        [-0.5713],
        [ 0.0974],
        [-0.2905]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4174, 1.2448, 1.5396, 1.7036, 1.4396], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:43:11,855] Trial 95 finished with value: -0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 32, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7843],
        [ 0.1930],
        [ 0.4816],
        [ 0.5885],
        [ 0.5867]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5925],
        [ 0.6187],
        [-0.3255],
        [ 0.2286],
        [ 0.9537]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3561, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3013, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3144, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5586, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2929, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2110, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4194, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2881, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5437, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2479, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0123, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5810, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2522, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5233, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1862, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7719, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7767, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2087, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4987, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.1115, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1800], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3560], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4720], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3560], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4720], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4968],
        [ 0.4042],
        [-0.0899],
        [-0.0510],
        [-0.1543]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4455, 1.2286, 1.3306, 1.5678, 1.3207], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:43:18,777] Trial 96 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7748],
        [-0.3848],
        [-0.2623],
        [-0.5586],
        [-0.7620]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3604],
        [-0.5391],
        [ 0.1527],
        [ 0.5072],
        [-0.7370]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6973, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6142, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8771, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.0079, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6661, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.4353, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8001, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7578, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.0036, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5428, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1587, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9962, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6319, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9991, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4126, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8879, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.1882, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5087, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9946, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2852, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0836], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.5891], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2473], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.5891], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2473], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.6013],
        [ 1.1355],
        [-0.7290],
        [-0.0264],
        [-0.7537]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.8312, 1.5193, 1.9381, 2.0102, 1.7292], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:43:24,621] Trial 97 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 550, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.4832],
        [ 0.7054],
        [ 0.0586],
        [ 0.0281],
        [-0.4350]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[0.1593],
        [0.7311],
        [0.9195],
        [0.6620],
        [0.9648]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.9518, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7124, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8022, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9968, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.7713, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.7962, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8005, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6637, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9478, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6435, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.5360, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.9478, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4321, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.8658, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4299, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.3015, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.0806, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2232, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.7919, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.2373, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.2800], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1467], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7267], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1467], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7267], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3484],
        [ 0.1972],
        [-0.3102],
        [-0.1098],
        [-0.2861]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([2.0493, 1.6572, 1.8891, 2.0276, 1.8514], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:43:34,764] Trial 98 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 128, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0971],
        [-0.3571],
        [ 0.3496],
        [ 0.8918],
        [-0.0364]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3868],
        [-0.8947],
        [-0.5389],
        [-0.0086],
        [-0.0634]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2417, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1754, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4110, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3283, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2370, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0456, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3687, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3147, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3282, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1241, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8283, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5827, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2080, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3282, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.9990, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6341, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7741, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1127, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3282, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8871, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0311], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.5089], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.2578], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2578], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2578], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-8.6520e-01],
        [ 8.5249e-01],
        [-4.2477e-01],
        [-9.4767e-05],
        [-4.9820e-01]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2686, 1.1489, 1.4242, 1.3283, 1.2525], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:43:41,891] Trial 99 finished with value: 1.4000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.9467],
        [-0.9338],
        [ 0.2187],
        [-0.8978],
        [-0.2162]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7358],
        [ 0.3653],
        [-0.6541],
        [ 0.6625],
        [-0.2002]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4133, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3522, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5495, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5701, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4363, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1558, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6240, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4951, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6156, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4157, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8961, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8981, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4402, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6615, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3950, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7337, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.0693, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4059, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6902, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3821, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0120], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2680], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4880], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4880], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4880], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.8999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.5411],
        [ 0.5709],
        [-0.1143],
        [ 0.0957],
        [-0.0432]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4198, 1.3454, 1.5509, 1.5689, 1.4368], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:43:45,537] Trial 100 finished with value: -0.8999999999999999 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 250, 'minibatch_size': 32, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2124],
        [ 0.0282],
        [ 0.9789],
        [-0.0489],
        [ 0.9763]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6703],
        [-0.8430],
        [-0.7132],
        [-0.3642],
        [ 0.7261]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2205, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1452, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4043, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4669, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3182, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9551, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3430, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2836, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5374, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2849, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7633, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4859, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1964, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5883, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2609, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.5755, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6259, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1110, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6381, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2373, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0222], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7244], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7244], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7244], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7244], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.9184],
        [ 0.6845],
        [-0.4176],
        [ 0.2437],
        [-0.1152]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2409, 1.1300, 1.4135, 1.4615, 1.3207], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:43:56,766] Trial 101 finished with value: -0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.5619],
        [ 0.8667],
        [-0.4578],
        [ 0.9952],
        [ 0.3633]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6554],
        [-0.2661],
        [ 0.4550],
        [ 0.6313],
        [-0.7548]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2634, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2427, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3615, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4254, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3391, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9857, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4893, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2620, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5264, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2846, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7452, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7029, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1759, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6139, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2374, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.5369, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8879, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1013, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6897, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1966, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0533], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7044], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3022], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7044], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3022], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.1158],
        [ 0.9911],
        [-0.3995],
        [ 0.4059],
        [-0.2188]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3229, 1.1898, 1.3828, 1.4038, 1.3507], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:44:07,990] Trial 102 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.8152],
        [-0.7805],
        [ 0.6184],
        [-0.8563],
        [ 0.9264]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9729],
        [ 0.8286],
        [-0.8940],
        [ 0.4834],
        [-0.0066]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6555, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4580, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8383, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8072, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6438, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.4236, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6857, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6805, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8044, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5646, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1709, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9339, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5085, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8014, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4784, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9437, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.1570, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3539, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7987, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4008, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0800], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.5444], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.3022], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7622], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3022], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.5999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0433],
        [ 1.0247],
        [-0.7099],
        [-0.0124],
        [-0.3561]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.7389, 1.3760, 1.8951, 1.8081, 1.6723], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:44:19,226] Trial 103 finished with value: 1.5999999999999999 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.2273],
        [ 0.8900],
        [ 0.7074],
        [-0.7913],
        [-0.5707]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[0.5414],
        [0.9528],
        [0.0565],
        [0.5990],
        [0.7340]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3759, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2147, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4820, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4681, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3795, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0862, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4864, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4168, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4595, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2710, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8931, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6675, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3733, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4537, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1987, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7172, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8326, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3337, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4484, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1327, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.5000], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.3000], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3000], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3000], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.9655],
        [ 0.9056],
        [-0.2175],
        [-0.0289],
        [-0.3617]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3759, 1.2147, 1.4820, 1.4681, 1.3795], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:44:30,682] Trial 104 finished with value: 1.4000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8248],
        [ 0.6165],
        [ 0.2146],
        [ 0.0046],
        [-0.4198]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2797],
        [-0.8483],
        [-0.4167],
        [-0.7451],
        [ 0.5355]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4013, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1904, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5134, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5590, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3485, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1417, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4041, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3969, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5844, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2391, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9076, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5967, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2918, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6074, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1405, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6897, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7761, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1940, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6287, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0487, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0156], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6978], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2644], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6978], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2644], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0429],
        [ 0.8585],
        [-0.4683],
        [ 0.1022],
        [-0.4394]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4175, 1.1770, 1.5207, 1.5574, 1.3553], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:44:41,946] Trial 105 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5820],
        [ 0.4492],
        [-0.3359],
        [ 0.8275],
        [-0.1796]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4242],
        [ 0.1552],
        [-0.0526],
        [-0.9154],
        [ 0.0186]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.7176, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9073, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7857, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6352, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.8567, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.5577, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.0232, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7334, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6239, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.7785, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.3679, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.1608, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.6712, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6105, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.6856, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.1302, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.3333, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.5934, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.5937, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.5693, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.2067], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7400], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7400], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7400], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7400], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -1.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2997],
        [ 0.2174],
        [-0.0981],
        [-0.0212],
        [-0.1467]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.7795, 1.8623, 1.8060, 1.6396, 1.8870], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:44:47,780] Trial 106 finished with value: -1.0 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0351],
        [-0.6957],
        [ 0.5306],
        [ 0.7890],
        [-0.4519]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2872],
        [-0.9505],
        [-0.9756],
        [ 0.6715],
        [ 0.7655]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3078, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2466, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4545, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5582, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2782, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1539, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4090, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4040, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6130, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2581, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8797, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6981, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3142, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7105, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2223, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6625, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9271, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2430, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7878, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1940, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0711], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7511], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2333], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7511], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2333], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.9490],
        [ 1.0007],
        [-0.3110],
        [ 0.3376],
        [-0.1239]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3753, 1.1755, 1.4766, 1.5342, 1.2870], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:44:54,667] Trial 107 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4266],
        [ 0.1612],
        [ 0.4079],
        [ 0.9601],
        [ 0.5422]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8511],
        [ 0.7452],
        [-0.9574],
        [-0.8365],
        [ 0.5349]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3639, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3248, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4664, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7111, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5138, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1468, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4889, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3479, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7078, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4690, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7777, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7678, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1464, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.7021, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3928, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4830, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.9905, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.9855, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.6976, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.3320, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.2133], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1067], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.6800], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1067], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.6800], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4653],
        [ 0.3516],
        [-0.2540],
        [-0.0071],
        [-0.0960]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4632, 1.2498, 1.5206, 1.7126, 1.5343], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:45:04,809] Trial 108 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 128, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.6459],
        [ 0.2981],
        [-0.2223],
        [ 0.7136],
        [-0.5515]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.7774],
        [ 0.7556],
        [-0.2061],
        [ 0.7666],
        [ 0.5505]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5634, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4181, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6414, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7271, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4614, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3588, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5793, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5758, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7937, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3852, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0591, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8154, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4796, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8914, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2735, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8474, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9822, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4117, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9604, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1946, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0743], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9343], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3200], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.9343], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3200], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8326],
        [ 0.6560],
        [-0.2671],
        [ 0.2714],
        [-0.3102]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6252, 1.3693, 1.6613, 1.7069, 1.4844], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:45:23,691] Trial 109 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 350, 'minibatch_size': 256, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.6628],
        [-0.4233],
        [ 0.4757],
        [-0.7525],
        [ 0.9606]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9204],
        [-0.7658],
        [ 0.9875],
        [ 0.3965],
        [-0.5189]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.7497, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6609, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.1346, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.0477, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.7200, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.4585, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8494, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.0248, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.1091, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6407, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1080, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.0762, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8925, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.1830, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5453, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8931, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.2152, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8114, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.2283, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4868, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0244], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.5289], dtype=torch.float64)  Action:  3  Reward Received:  1.0
State:  tensor([0.6978], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6978], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6978], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.2722],
        [ 0.8231],
        [-0.4800],
        [ 0.2682],
        [-0.3463]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.7808, 1.6408, 2.1463, 2.0411, 1.7284], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:45:30,745] Trial 110 finished with value: 0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 300, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1803],
        [-0.8854],
        [ 0.7002],
        [-0.5799],
        [ 0.2417]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2489],
        [-0.5300],
        [ 0.1429],
        [-0.2388],
        [ 0.0608]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4086, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3419, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3426, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3886, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3767, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1468, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5649, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2338, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4053, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2742, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9006, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7746, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1315, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4209, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1777, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6921, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9522, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0448, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4342, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0961, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0311], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0311], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0311], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0311], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0311], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.9982],
        [ 0.8503],
        [-0.4149],
        [ 0.0634],
        [-0.3910]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4396, 1.3155, 1.3555, 1.3866, 1.3889], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:45:42,956] Trial 111 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.5450],
        [-0.1555],
        [ 0.6299],
        [ 0.0825],
        [-0.2965]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3748],
        [-0.9957],
        [ 0.3925],
        [-0.2693],
        [ 0.9295]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5049, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4091, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6042, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7219, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5352, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2525, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6325, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4858, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7384, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4601, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9421, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9071, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3403, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7587, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3678, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7186, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.1049, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2355, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7733, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3013, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0311], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6956], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2444], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6956], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2444], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.1835],
        [ 1.0472],
        [-0.5550],
        [ 0.0773],
        [-0.3521]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5418, 1.3765, 1.6215, 1.7195, 1.5462], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:45:54,878] Trial 112 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1729],
        [-0.5049],
        [ 0.0595],
        [ 0.2358],
        [ 0.4398]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4857],
        [ 0.0602],
        [-0.5227],
        [ 0.9094],
        [ 0.7117]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.0520, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1023, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1065, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1798, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1761, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8794, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2333, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0660, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1986, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0675, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6977, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3712, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0233, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2183, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.9531, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.4664, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5467, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9689, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2435, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8075, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0622], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7711], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2711], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2711], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2711], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.8999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8260],
        [ 0.6269],
        [-0.1942],
        [ 0.0898],
        [-0.5201]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.1034, 1.0633, 1.1186, 1.1742, 1.2085], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:46:07,056] Trial 113 finished with value: -0.8999999999999999 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.3601],
        [-0.2909],
        [-0.9273],
        [ 0.5312],
        [-0.8432]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0713],
        [-0.2870],
        [ 0.1791],
        [ 0.9202],
        [ 0.9774]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3909, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2826, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5275, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6069, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4114, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1092, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5196, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4572, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6677, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3283, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7712, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8040, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3729, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7406, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2287, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.5567, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9846, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3194, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7869, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1654, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0022], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6933], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2356], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6933], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2356], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.2070],
        [ 1.0157],
        [-0.3010],
        [ 0.2604],
        [-0.3559]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3935, 1.2803, 1.5282, 1.6064, 1.4122], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:46:18,216] Trial 114 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.3518],
        [ 0.1164],
        [ 0.3648],
        [-0.4602],
        [ 0.5631]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7687],
        [-0.8069],
        [-0.5204],
        [ 0.8726],
        [-0.7096]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5120, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2491, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4212, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6206, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4567, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3389, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3814, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3442, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6016, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3428, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1621, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5164, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2656, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5821, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2265, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9399, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6863, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1667, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5576, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0802, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1040], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3600], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4840], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3600], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4840], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4555],
        [ 0.3481],
        [-0.2026],
        [-0.0502],
        [-0.2998]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5593, 1.2129, 1.4422, 1.6259, 1.4879], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:46:28,875] Trial 115 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 250, 'minibatch_size': 128, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.9227],
        [-0.2103],
        [ 0.6274],
        [ 0.5219],
        [-0.9609]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0744],
        [-0.7253],
        [ 0.0283],
        [ 0.9928],
        [ 0.0864]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.7629, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5747, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7042, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7108, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6229, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.5718, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7849, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6225, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7382, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5767, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2750, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.1114, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4955, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7808, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5049, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1019, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.3018, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4214, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8056, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4630, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0378], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0378], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0378], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0378], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.0378], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0117],
        [ 1.1129],
        [-0.4329],
        [ 0.1452],
        [-0.2446]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.8011, 1.5327, 1.7206, 1.7053, 1.6321], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:46:36,309] Trial 116 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 100, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8748],
        [-0.2295],
        [-0.2066],
        [ 0.3689],
        [-0.2414]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2579],
        [-0.3368],
        [-0.5613],
        [ 0.5049],
        [ 0.6975]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5601, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4606, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7310, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8812, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4187, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2925, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7372, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6676, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9013, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3397, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9915, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.0483, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.5962, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.9239, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2508, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7379, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.3105, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.5360, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.9429, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.1759, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0600], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0267], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0267], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.7000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4181],
        [ 0.4322],
        [-0.0992],
        [ 0.0314],
        [-0.1234]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5852, 1.4347, 1.7370, 1.8793, 1.4261], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:46:42,112] Trial 117 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.9168],
        [-0.8287],
        [ 0.7782],
        [ 0.9836],
        [ 0.0142]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6930],
        [-0.7046],
        [ 0.0050],
        [-0.6919],
        [ 0.9440]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6002, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4718, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6275, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7367, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4814, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3657, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6157, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5661, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8425, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4446, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1132, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7705, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5001, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9562, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4049, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9092, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8956, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4468, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.0482, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3728, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0711], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6733], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6733], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6733], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6733], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.1475],
        [ 0.7036],
        [-0.3001],
        [ 0.5171],
        [-0.1803]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6818, 1.4218, 1.6488, 1.7000, 1.4943], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:46:52,816] Trial 118 finished with value: -0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.9483],
        [ 0.5130],
        [ 0.3839],
        [-0.5760],
        [ 0.6327]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0680],
        [-0.4663],
        [ 0.2038],
        [-0.2111],
        [-0.4901]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2615, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1589, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3835, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4046, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2565, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9914, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4152, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2526, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4897, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1756, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8139, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5836, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1666, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5456, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1224, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6325, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7556, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.0787, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6028, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0680, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0080], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3120], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5680], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3120], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5680], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4823],
        [ 0.4576],
        [-0.2337],
        [ 0.1520],
        [-0.1446]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2653, 1.1552, 1.3854, 1.4034, 1.2577], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:46:59,728] Trial 119 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2505],
        [-0.3034],
        [ 0.6601],
        [ 0.4687],
        [-0.5970]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.5692],
        [ 0.9817],
        [ 0.3851],
        [-0.2221],
        [-0.9528]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4214, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3202, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4788, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4547, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3886, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1414, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3791, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3544, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4586, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2679, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9525, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4189, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2704, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4612, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1865, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6725, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4777, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1460, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4651, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0659, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0055], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.3800], dtype=torch.float64)  Action:  3  Reward Received:  1.0
State:  tensor([0.6036], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2291], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6036], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.2520],
        [ 0.2633],
        [-0.5563],
        [ 0.0173],
        [-0.5394]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4282, 1.3188, 1.4819, 1.4546, 1.3915], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:47:06,151] Trial 120 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 550, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.3062],
        [-0.9461],
        [-0.2089],
        [-0.1682],
        [-0.3552]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.4766],
        [-0.8484],
        [ 0.2618],
        [-0.9086],
        [-0.9062]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2442, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1988, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3440, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2651, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2857, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9737, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2958, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2862, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2935, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1476, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6920, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3968, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2259, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3231, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0038, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4132, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.4968, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1663, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.3524, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.8615, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1880], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.9800], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.5760], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5760], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5760], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.6971],
        [ 0.2499],
        [-0.1491],
        [ 0.0732],
        [-0.3559]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3752, 1.1519, 1.3721, 1.2513, 1.3526], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:47:11,907] Trial 121 finished with value: 1.4000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.2044],
        [-0.1773],
        [ 0.3769],
        [-0.7472],
        [ 0.3420]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6511],
        [ 0.1178],
        [-0.5209],
        [ 0.2165],
        [ 0.8891]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5047, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2608, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4650, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4384, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3731, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3194, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3375, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4413, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4588, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2651, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1113, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4237, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4146, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4817, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1438, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8825, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5184, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3854, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5069, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0105, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1480], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.1480], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.1480], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.1480], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.1480], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.5719],
        [ 0.2368],
        [-0.0732],
        [ 0.0629],
        [-0.3333]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5894, 1.2257, 1.4758, 1.4291, 1.4225], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:47:17,718] Trial 122 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1761],
        [-0.5605],
        [ 0.9855],
        [ 0.1208],
        [ 0.7213]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9125],
        [ 0.5560],
        [ 0.2334],
        [-0.2134],
        [-0.0765]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3112, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4478, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6444, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6866, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3396, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1129, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5957, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5832, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7096, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3894, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7947, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8329, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4849, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7463, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4694, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5107, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.0446, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3971, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.7791, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.5407, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0880], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3960], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4120], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.3960], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4120], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.6120],
        [ 0.4563],
        [-0.1890],
        [ 0.0707],
        [ 0.1537]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3651, 1.4077, 1.6610, 1.6804, 1.3261], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:47:24,026] Trial 123 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.5492],
        [ 0.8118],
        [-0.5067],
        [ 0.2881],
        [-0.5504]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4703],
        [ 0.6379],
        [-0.5354],
        [-0.3886],
        [ 0.2452]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4151, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3766, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5796, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4454, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5058, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2214, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5627, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5217, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5619, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3775, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9646, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8091, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4449, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7163, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2074, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7784, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.9879, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3892, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.8283, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0841, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1920], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.9080], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.5000], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5000], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5000], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.6292],
        [ 0.6040],
        [-0.1881],
        [ 0.3784],
        [-0.4167]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5359, 1.2607, 1.6157, 1.3727, 1.5858], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:47:29,803] Trial 124 finished with value: 1.4000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.4462],
        [-0.0718],
        [ 0.2473],
        [-0.4491],
        [ 0.9873]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3514],
        [ 0.6322],
        [-0.3857],
        [ 0.9077],
        [ 0.9070]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5757, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4828, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5554, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5732, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6598, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3526, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5821, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4063, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5641, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4604, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.2209, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6407, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3183, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5588, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3428, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.0443, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.7193, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2003, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.5516, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.1850, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0600], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0600], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0600], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0600], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0600], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2408],
        [ 0.1072],
        [-0.1610],
        [-0.0098],
        [-0.2151]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5901, 1.4763, 1.5651, 1.5738, 1.6727], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:47:36,295] Trial 125 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7464],
        [-0.7387],
        [-0.1446],
        [ 0.6261],
        [-0.4558]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0574],
        [-0.0468],
        [-0.0811],
        [-0.9349],
        [-0.2797]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.7187, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5249, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8121, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8603, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6645, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.5298, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7232, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7186, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.0264, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6111, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1917, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.0782, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5511, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.3238, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5154, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0238, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.2545, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4679, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.4715, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4679, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0760], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2680], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2680], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2680], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2680], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.5830],
        [ 0.6121],
        [-0.2888],
        [ 0.5127],
        [-0.1650]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.7630, 1.4784, 1.8341, 1.8213, 1.6771], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:47:39,956] Trial 126 finished with value: -0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 100, 'high': 250, 'minibatch_size': 32, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.4844],
        [ 0.5206],
        [-0.1060],
        [-0.8425],
        [ 0.0423]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5419],
        [-0.4029],
        [-0.0720],
        [-0.3562],
        [ 0.0195]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.1851, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1643, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2487, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3061, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2041, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9628, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3591, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1406, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3698, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1017, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8445, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4627, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0831, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4036, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0471, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6868, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6008, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0064, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4488, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.9745, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0133], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7156], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3267], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7156], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3267], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.7096],
        [ 0.6217],
        [-0.3450],
        [ 0.2031],
        [-0.3271]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.1946, 1.1560, 1.2533, 1.3034, 1.2085], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:47:46,907] Trial 127 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.0420],
        [-0.8882],
        [-0.6699],
        [ 0.1511],
        [ 0.6312]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6296],
        [ 0.3914],
        [-0.6088],
        [-0.8613],
        [ 0.7013]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3455, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2980, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5066, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5163, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2855, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2399, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3571, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3910, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5829, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2633, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.1234, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.4223, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2635, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6565, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2386, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.9657, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.5106, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0909, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.7560, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.2053, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.2600], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1067], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1067], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1067], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.1067], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2057],
        [ 0.1151],
        [-0.2251],
        [ 0.1298],
        [-0.0434]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3990, 1.2681, 1.5651, 1.4826, 1.2968], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:47:58,392] Trial 128 finished with value: -0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 150, 'minibatch_size': 128, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4984],
        [-0.1381],
        [-0.5580],
        [ 0.1955],
        [ 0.9497]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1228],
        [-0.8171],
        [-0.2835],
        [ 0.1619],
        [-0.8048]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.0164, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0163, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1564, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1849, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0937, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9050, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0975, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1049, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2122, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0612, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7371, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2199, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0274, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2533, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0122, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.5244, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3751, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9290, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3054, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.9501, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1089], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7667], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2578], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7667], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2578], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.7480],
        [ 0.5454],
        [-0.3456],
        [ 0.1831],
        [-0.2184]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.0978, 0.9570, 1.1940, 1.1650, 1.1175], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:48:04,274] Trial 129 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1031],
        [ 0.5220],
        [ 0.2375],
        [-0.2956],
        [ 0.4829]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7124],
        [-0.1467],
        [ 0.0901],
        [ 0.5915],
        [ 0.9928]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.7211, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3953, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5216, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4884, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6999, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.4448, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6047, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3936, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4764, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5942, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1150, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8548, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2409, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4620, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4680, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8866, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.0279, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1352, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4521, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3807, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.1240], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.1240], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.1240], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.1240], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.1240], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.7048],
        [ 0.5343],
        [-0.3263],
        [-0.0306],
        [-0.2696]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.8085, 1.3290, 1.5620, 1.4922, 1.7333], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:48:11,406] Trial 130 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1351],
        [ 0.2006],
        [ 0.6610],
        [-0.4947],
        [-0.9255]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2278],
        [ 0.0903],
        [-0.6005],
        [ 0.9051],
        [-0.6833]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4572, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4253, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5660, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6884, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4060, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2553, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5716, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4451, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6870, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3501, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0270, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7371, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3084, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6853, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2868, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8231, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8848, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1863, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6839, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2303, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0067], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6978], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2267], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6978], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2267], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.9175],
        [ 0.6649],
        [-0.5494],
        [-0.0066],
        [-0.2541]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4633, 1.4209, 1.5697, 1.6885, 1.4077], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:48:18,329] Trial 131 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2758],
        [-0.9375],
        [ 0.0082],
        [-0.9365],
        [-0.6823]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3476],
        [-0.6265],
        [-0.7033],
        [-0.9897],
        [ 0.7504]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3150, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2865, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4190, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4358, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4031, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0263, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4869, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3158, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4980, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3066, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7944, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6479, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2330, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5480, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2291, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.5886, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7908, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1594, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5923, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1603, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0244], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7067], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2956], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7067], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2956], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0648],
        [ 0.7393],
        [-0.3806],
        [ 0.2295],
        [-0.3559]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3410, 1.2684, 1.4284, 1.4301, 1.4118], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:48:25,234] Trial 132 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.8208],
        [-0.2612],
        [ 0.3447],
        [ 0.7278],
        [ 0.8296]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6709],
        [ 0.4271],
        [-0.1680],
        [-0.3747],
        [-0.4840]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5002, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4587, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6301, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5573, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5011, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2355, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7222, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5035, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6252, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3638, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0048, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9517, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3931, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6843, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2441, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7717, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.1837, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2816, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7441, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1232, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0444], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.4978], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.2867], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2867], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2867], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0928],
        [ 1.0875],
        [-0.5228],
        [ 0.2801],
        [-0.5669]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5488, 1.4104, 1.6534, 1.5449, 1.5263], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:48:32,156] Trial 133 finished with value: 1.4000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.8412],
        [-0.1538],
        [ 0.9024],
        [-0.5013],
        [ 0.0930]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4186],
        [ 0.4704],
        [-0.7819],
        [ 0.2834],
        [ 0.0065]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4343, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3535, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3544, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5758, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4161, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2868, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5178, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2768, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5781, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3374, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0598, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7707, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1574, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5818, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2162, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8556, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9982, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0500, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5851, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1071, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0622], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7422], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2356], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7422], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2356], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8510],
        [ 0.9481],
        [-0.4476],
        [ 0.0137],
        [-0.4544]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4872, 1.2945, 1.3822, 1.5749, 1.4444], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:48:39,089] Trial 134 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.6023],
        [ 0.7472],
        [ 0.1461],
        [-0.6542],
        [ 0.7491]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1155],
        [ 0.5243],
        [-0.7658],
        [-0.4008],
        [-0.5670]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3001, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2415, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3649, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4493, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2695, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0097, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4316, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2669, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5197, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1784, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7632, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5930, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1837, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5794, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1011, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.4899, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7720, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0915, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6456, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0153, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0111], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7489], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2756], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7489], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2756], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0982],
        [ 0.7191],
        [-0.3705],
        [ 0.2661],
        [-0.3445]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3123, 1.2335, 1.3690, 1.4464, 1.2734], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:48:59,314] Trial 135 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 400, 'high': 450, 'minibatch_size': 256, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.4989],
        [-0.4369],
        [-0.1086],
        [ 0.0935],
        [ 0.6928]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9015],
        [ 0.3865],
        [-0.1480],
        [-0.1523],
        [ 0.1546]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.7213, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5331, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7690, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8448, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6115, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.4511, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8789, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6277, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8656, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5005, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.2319, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.1595, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.5130, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.8825, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4104, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.0569, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.3834, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.4215, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.8960, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.3386, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0333], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0333], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.8467], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.8467], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.8467], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.8999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3322],
        [ 0.4251],
        [-0.1737],
        [ 0.0256],
        [-0.1365]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.7324, 1.5189, 1.7748, 1.8440, 1.6160], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:49:05,136] Trial 136 finished with value: -0.8999999999999999 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.2340],
        [-0.9940],
        [-0.9186],
        [-0.2164],
        [ 0.1622]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5790],
        [ 0.5476],
        [-0.3338],
        [ 0.4402],
        [-0.4370]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2721, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2912, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2549, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3918, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3307, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9919, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4711, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1412, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4576, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2094, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8365, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5708, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0782, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4942, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1421, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.5563, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7507, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9645, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5601, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0208, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0857], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.8886], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4000], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4000], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.4000], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.8999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8916],
        [ 0.5724],
        [-0.3618],
        [ 0.2096],
        [-0.3860]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3485, 1.2421, 1.2859, 1.3738, 1.3638], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:49:16,407] Trial 137 finished with value: -0.8999999999999999 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 300, 'high': 350, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.4450],
        [ 0.7332],
        [-0.5118],
        [ 0.9509],
        [ 0.1887]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0832],
        [-0.8041],
        [-0.2276],
        [-0.0241],
        [-0.4355]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4071, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1727, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2014, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2542, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4276, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2126, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2620, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1402, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2888, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2604, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9963, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3614, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0721, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3273, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0743, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7775, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4618, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0032, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3662, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8861, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0911], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0911], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0911], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0911], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0911], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0940],
        [ 0.5024],
        [-0.3445],
        [ 0.1945],
        [-0.9409]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5068, 1.1269, 1.2328, 1.2365, 1.5134], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:49:20,094] Trial 138 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 32, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[0.1289],
        [0.1165],
        [0.9373],
        [0.1868],
        [0.0702]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5049],
        [ 0.8552],
        [ 0.8289],
        [ 0.4486],
        [-0.5698]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4028, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2400, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4505, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5009, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3726, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1393, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4419, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2958, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5321, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2689, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.9340, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5992, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1754, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5563, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.1881, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7288, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.7565, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0550, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.5805, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.1074, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.3000], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0733], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.9933], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([2.0733], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.9933], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3801],
        [ 0.2913],
        [-0.2230],
        [ 0.0449],
        [-0.1495]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5168, 1.1526, 1.5174, 1.4875, 1.4174], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:49:27,035] Trial 139 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 100, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7760],
        [-0.8299],
        [ 0.2001],
        [ 0.1770],
        [ 0.7807]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9046],
        [-0.5417],
        [-0.6749],
        [-0.9843],
        [ 0.5123]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3013, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2612, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3556, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3370, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3794, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1133, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3973, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2695, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3778, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2944, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9540, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5126, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1964, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4123, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2223, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7374, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6693, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0971, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4592, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1243, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0622], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0622], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0622], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0622], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.0622], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8056],
        [ 0.5830],
        [-0.3693],
        [ 0.1746],
        [-0.3645]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3514, 1.2250, 1.3786, 1.3262, 1.4021], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:49:37,235] Trial 140 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0536],
        [ 0.7868],
        [-0.9925],
        [ 0.2668],
        [ 0.4029]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2541],
        [-0.1860],
        [-0.5958],
        [-0.6524],
        [-0.3556]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3026, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3635, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6622, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7710, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3612, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1090, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5565, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5632, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7667, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2969, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9194, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7455, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4662, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7624, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2340, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.6789, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9852, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3432, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7570, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1542, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0489], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7289], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2600], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7289], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2600], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.9171],
        [ 0.9142],
        [-0.4691],
        [-0.0206],
        [-0.3043]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3474, 1.3188, 1.6851, 1.7721, 1.3761], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:49:48,671] Trial 141 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.2500],
        [-0.6330],
        [-0.8034],
        [ 0.6015],
        [ 0.9360]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3299],
        [ 0.7162],
        [ 0.2175],
        [-0.7827],
        [ 0.4298]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6415, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7020, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8091, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6997, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.7038, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.3898, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9826, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6742, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7412, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6078, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1970, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.1974, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5710, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7730, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5342, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0455, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(2.3662, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4899, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7979, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4764, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0289], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.5311], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.3133], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3133], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3133], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8852],
        [ 0.9864],
        [-0.4740],
        [ 0.1459],
        [-0.3377]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6671, 1.6735, 1.8227, 1.6955, 1.7136], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:49:59,899] Trial 142 finished with value: 1.4000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.6712],
        [ 0.5533],
        [ 0.8148],
        [ 0.3525],
        [-0.9057]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.4459],
        [-0.6972],
        [-0.8534],
        [-0.1704],
        [ 0.4672]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4075, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3705, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5133, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6850, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3953, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.2681, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4633, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4229, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.6989, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3702, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1383, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5497, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3389, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7118, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3467, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9367, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6838, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2083, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.7318, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3104, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0178], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7756], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7756], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7756], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7756], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.6212],
        [ 0.4134],
        [-0.4025],
        [ 0.0618],
        [-0.1121]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4186, 1.3632, 1.5204, 1.6839, 1.3973], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:50:11,156] Trial 143 finished with value: -0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.6572],
        [-0.8304],
        [-0.3529],
        [ 0.2001],
        [ 0.9682]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8573],
        [ 0.3666],
        [ 0.3934],
        [ 0.4447],
        [-0.2507]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3181, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0979, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2223, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3316, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3032, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0163, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3728, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0957, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3394, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1956, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8288, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.5434, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0170, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3443, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1288, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.5878, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7628, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9159, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3505, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0429, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0289], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.6956], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3044], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3044], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.3044], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.8999999999999999
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-1.0954],
        [ 0.9974],
        [-0.4597],
        [ 0.0284],
        [-0.3903]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3498, 1.0691, 1.2356, 1.3308, 1.3144], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:50:22,416] Trial 144 finished with value: -0.8999999999999999 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.8557],
        [ 0.5027],
        [-0.2618],
        [ 0.5005],
        [-0.5030]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.4519],
        [-0.6023],
        [ 0.7108],
        [ 0.1492],
        [ 0.8937]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3601, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3233, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4417, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5061, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4335, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.1814, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4922, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3490, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5200, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3172, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9987, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6648, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2543, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5341, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1983, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7689, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.8820, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1351, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5520, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0487, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0400], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7089], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2422], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7089], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2422], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.8838],
        [ 0.8353],
        [-0.4585],
        [ 0.0686],
        [-0.5752]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3954, 1.2899, 1.4601, 1.5034, 1.4565], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:50:33,852] Trial 145 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.6071],
        [ 0.0524],
        [ 0.0904],
        [-0.0580],
        [ 0.8115]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7589],
        [-0.5759],
        [-0.8300],
        [-0.0341],
        [ 0.0360]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2993, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0746, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2986, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3290, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2389, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0145, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2728, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1715, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3530, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1285, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7756, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4391, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0648, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3730, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0360, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5922, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5667, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.9829, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.3884, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.9649, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0960], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2680], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5680], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([1.2680], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.5680], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.6034],
        [ 0.4199],
        [-0.2693],
        [ 0.0507],
        [-0.2337]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3572, 1.0343, 1.3244, 1.3242, 1.2613], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:50:40,664] Trial 146 finished with value: -0.7000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 250, 'minibatch_size': 64, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0949],
        [-0.5696],
        [ 0.1257],
        [ 0.9565],
        [ 0.7626]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8697],
        [-0.5084],
        [-0.7446],
        [ 0.5487],
        [-0.8325]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.9992, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6420, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8013, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8867, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.7746, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.7504, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.7994, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6813, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.8280, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6248, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.4938, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.9619, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.5576, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.7674, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4703, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.2267, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.1309, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.4288, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.7043, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.3094, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.2133], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.2133], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.2133], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.2133], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.2133], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3928],
        [ 0.2486],
        [-0.1894],
        [-0.0928],
        [-0.2365]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([2.0830, 1.5889, 1.8417, 1.9065, 1.8251], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:50:46,505] Trial 147 finished with value: 0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 150, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[-0.7287],
        [-0.1363],
        [ 0.3049],
        [-0.2329],
        [-0.8256]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.8645],
        [ 0.0299],
        [ 0.4169],
        [ 0.8676],
        [-0.9288]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2594, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1280, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2657, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3886, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3151, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(1.0573, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1994, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1344, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3648, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1937, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9571, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2347, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0694, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3530, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1335, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7377, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3121, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.9269, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3272, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0018, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0556], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7267], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7267], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7267], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([0.7267], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.7774],
        [ 0.2743],
        [-0.5048],
        [-0.0915],
        [-0.4669]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3026, 1.1128, 1.2937, 1.3936, 1.3411], dtype=torch.float64,
       requires_grad=True)
[I 2025-07-10 00:50:57,785] Trial 148 finished with value: -0.5 and parameters: {'lr_begin': 0.05, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 128, 'replay_buffer_size': 50000000}. Best is trial 14 with value: 1.5999999999999999.
Starting Training
layer weights:  Parameter containing:
tensor([[0.3764],
        [0.4297],
        [0.7197],
        [0.5272],
        [0.5186]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6969],
        [ 0.2560],
        [ 0.7385],
        [ 0.2548],
        [-0.7218]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.1447, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1596, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.2857, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2318, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1286, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.9934, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3381, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1923, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2709, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0824, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.8920, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.4577, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1297, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2970, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0515, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 0 		  Q(s,a)=  tensor(0.7293, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.6496, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0292, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3390, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0018, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.0600], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([0.3818], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([0.2527], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2527], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([0.2527], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.7852],
        [ 0.9262],
        [-0.4850],
        [ 0.2026],
        [-0.2396]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.1918, 1.1040, 1.3148, 1.2197, 1.1430], dtype=torch.float64,
       requires_grad=True)
Best Params:  {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 450, 'minibatch_size': 64, 'replay_buffer_size': 1000}
[I 2025-07-10 00:51:04,420] Trial 149 finished with value: 1.4000000000000001 and parameters: {'lr_begin': 0.005, 'target_weight_update_freq': 600, 'high': 550, 'minibatch_size': 64, 'replay_buffer_size': 1000}. Best is trial 14 with value: 1.5999999999999999.
Optimization complete. Data saved to: sqlite:///db.sqlite3
