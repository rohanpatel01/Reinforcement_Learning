Starting Training
layer weights:  Parameter containing:
tensor([[-0.0924,  0.1688, -0.0474,  0.1448,  0.0297, -0.1829,  0.0290, -0.1349,
         -0.0890,  0.1688,  0.0586,  0.1762, -0.0767, -0.0152,  0.0776,  0.0903,
         -0.0558, -0.1053, -0.1017, -0.0660, -0.0452, -0.1577,  0.1676,  0.1006,
         -0.0786],
        [ 0.0482,  0.1458,  0.0217,  0.0446, -0.1973,  0.1241, -0.1995, -0.0704,
         -0.1053,  0.1438,  0.0561, -0.1890,  0.0173,  0.1246,  0.1866,  0.1366,
         -0.0906, -0.1555, -0.1119,  0.0917,  0.1874,  0.1999, -0.1583,  0.1634,
          0.1552],
        [-0.1802,  0.0050, -0.0685,  0.0042, -0.1245,  0.0556, -0.1328, -0.0493,
         -0.1717, -0.1390, -0.1028,  0.1111,  0.1302, -0.0985, -0.1201, -0.0514,
         -0.0027, -0.1648,  0.0257,  0.0938, -0.0908, -0.1419, -0.0871,  0.0623,
          0.0618],
        [-0.1376, -0.1386,  0.0650, -0.1484, -0.0940, -0.0839, -0.0573,  0.0800,
          0.0103,  0.1426,  0.0501,  0.0173,  0.1965, -0.0699,  0.1064,  0.0051,
         -0.1792, -0.0836,  0.1222,  0.1786,  0.1488,  0.1911, -0.0017,  0.1641,
          0.1464],
        [ 0.1838, -0.1127,  0.1658,  0.0860,  0.1671, -0.0312,  0.0133, -0.0921,
          0.0332,  0.0888, -0.1786, -0.0653,  0.1936,  0.1337, -0.1314, -0.0303,
         -0.1158,  0.0424,  0.1580, -0.1526, -0.0026, -0.1116,  0.0671, -0.1908,
         -0.1513]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1262, -0.0856, -0.1219,  0.1996, -0.0702,  0.1104, -0.1385,  0.1004,
         -0.0364, -0.0025,  0.0076,  0.1099, -0.1528,  0.1835,  0.0959,  0.0258,
         -0.0420,  0.1125,  0.1564, -0.1629,  0.1905, -0.1545, -0.0916, -0.1056,
          0.1822],
        [-0.0569,  0.1042,  0.1326,  0.0306, -0.0486,  0.1806,  0.1438,  0.1399,
         -0.0896,  0.0773, -0.1560, -0.0962, -0.1828, -0.0026,  0.1688, -0.1121,
         -0.0135,  0.1272,  0.1227,  0.1841, -0.1373,  0.1684, -0.0336, -0.1801,
         -0.0701],
        [-0.1714, -0.1834, -0.0272,  0.1408,  0.0307,  0.0558, -0.1342,  0.0487,
          0.0798, -0.0090,  0.1035, -0.0970, -0.1382,  0.1093, -0.1687, -0.0634,
         -0.1481, -0.0454, -0.1989,  0.1243, -0.1396,  0.0835, -0.1498,  0.0534,
          0.1369],
        [-0.0849,  0.0585, -0.1652, -0.1264,  0.0744, -0.1409,  0.1044,  0.0829,
         -0.1371, -0.1275, -0.0434, -0.1713,  0.1515,  0.1520, -0.1931, -0.1495,
         -0.0479,  0.0152,  0.0990,  0.1063, -0.1349, -0.0438,  0.1119, -0.0118,
         -0.1192],
        [ 0.1833,  0.1891,  0.1962, -0.0797,  0.1359,  0.0916,  0.0900,  0.0863,
          0.0256, -0.0592, -0.1003, -0.0844, -0.0725, -0.0378,  0.1026, -0.0172,
          0.0513, -0.0911, -0.0374,  0.0287,  0.0897, -0.1371,  0.0583,  0.0513,
         -0.1699]], dtype=torch.float64, requires_grad=True)
Traceback (most recent call last):
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/Atari DQN Project/Linear.py", line 430, in <module>
    main()
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/Atari DQN Project/Linear.py", line 419, in main
    model.train()
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/Atari DQN Project/Q_Learning.py", line 81, in train
    action = self.sample_action(self.env, state, epsilon_scheduler.get_epsilon(self.t - self.config.learning_delay), self.t, "approx")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/Atari DQN Project/Linear.py", line 38, in sample_action
    return self.get_best_action(state, network_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/Atari DQN Project/Linear.py", line 58, in get_best_action
    Q_actions = self.approx_network(state)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/Atari DQN Project/Linear.py", line 152, in forward
    x = self.fc1(x)
        ^^^^^^^^^^^
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/rohanpatel/Desktop/github_repo/Reinforcement Learning/env/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: mat1 and mat2 shapes cannot be multiplied (25x1 and 25x5)
