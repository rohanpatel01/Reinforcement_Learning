layer weights:  Parameter containing:
tensor([[-0.8013],
        [ 0.1719],
        [-0.9966]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1754],
        [-0.5482],
        [ 0.8427]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.4564, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3566, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3765, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5937, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3170, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4374, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7310, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.2774, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4984, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.8683, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.2379, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5593, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 0 		  Q(s,a)=  tensor(1.0056, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 1 		  Q(s,a)=  tensor(0.1983, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 2 		  Q(s,a)=  tensor(0.6203, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 0 		  Q(s,a)=  tensor(1.1429, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 1 		  Q(s,a)=  tensor(0.1587, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 2 		  Q(s,a)=  tensor(0.6812, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0
State:  tensor([1.], dtype=torch.float64)  Action:  0  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  0  Reward Received:  0
State:  tensor([3.], dtype=torch.float64)  Action:  0  Reward Received:  0
State:  tensor([4.], dtype=torch.float64)  Action:  0  Reward Received:  1

Total Reward Received:  1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.1373],
        [-0.0396],
        [ 0.0609]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.4564, 0.3566, 0.3765], dtype=torch.float64, requires_grad=True)
