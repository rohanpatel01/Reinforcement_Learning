Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1268, -0.1811, -0.1121,  0.0391, -0.1644, -0.0491, -0.0762,  0.1567,
         -0.0103, -0.0971,  0.0395, -0.1935,  0.1442,  0.1884,  0.1417, -0.0920,
          0.0282,  0.1447, -0.0362,  0.1401, -0.0788, -0.1511,  0.0153,  0.0323,
          0.1671],
        [ 0.1594,  0.1910,  0.1796,  0.0061, -0.0382,  0.0308,  0.0646,  0.0995,
          0.0668, -0.0023,  0.1620, -0.0291, -0.1777,  0.1759, -0.0074, -0.0365,
         -0.0118,  0.1572,  0.1068, -0.0438,  0.1558, -0.1431, -0.0591,  0.1518,
         -0.0137],
        [-0.0673,  0.1348,  0.0685, -0.1941,  0.1900, -0.0187, -0.0014, -0.1308,
          0.1167,  0.0170, -0.1689, -0.1068,  0.1996,  0.0073,  0.1330, -0.1179,
         -0.1392, -0.1272, -0.0878,  0.1315, -0.0282,  0.1141, -0.0265, -0.1095,
          0.0194]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1171, -0.1923,  0.0889,  0.0057, -0.1151,  0.0283, -0.0025, -0.0090,
         -0.0013,  0.0979, -0.1954, -0.0015,  0.0767, -0.0494, -0.0504, -0.0629,
         -0.1787, -0.0332, -0.1261, -0.0740, -0.1500,  0.1327, -0.1827, -0.1014,
          0.1020],
        [ 0.0802, -0.1548,  0.0292,  0.0620,  0.1869,  0.1851,  0.0389, -0.0337,
         -0.0582, -0.0986, -0.1313,  0.1629, -0.0736, -0.0499, -0.0125, -0.1800,
         -0.1583, -0.1877, -0.1019,  0.0986, -0.0716, -0.1897, -0.1416,  0.0180,
          0.1798],
        [-0.0232,  0.0272,  0.0961, -0.0513, -0.0029,  0.1141,  0.1442, -0.0275,
         -0.0479,  0.0698,  0.1573,  0.1404, -0.1609, -0.1803,  0.0771, -0.0535,
          0.0880,  0.0816,  0.0882,  0.0201, -0.0879, -0.1589,  0.1765, -0.1325,
          0.1571]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6644, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5228, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5243, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7894, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6026, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6246, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.9302, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7149, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7225, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.0714, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.8193, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.8352, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 0 		  Q(s,a)=  tensor(1.1818, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 1 		  Q(s,a)=  tensor(0.9249, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 2 		  Q(s,a)=  tensor(0.9069, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 0 		  Q(s,a)=  tensor(1.3252, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 1 		  Q(s,a)=  tensor(1.0023, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 2 		  Q(s,a)=  tensor(1.0048, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 0 		  Q(s,a)=  tensor(1.5167, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 1 		  Q(s,a)=  tensor(1.1422, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 2 		  Q(s,a)=  tensor(1.1143, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 0 		  Q(s,a)=  tensor(1.6486, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 1 		  Q(s,a)=  tensor(1.2320, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 2 		  Q(s,a)=  tensor(1.2291, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 0 		  Q(s,a)=  tensor(1.8194, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 1 		  Q(s,a)=  tensor(1.3259, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 2 		  Q(s,a)=  tensor(1.3086, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 0 		  Q(s,a)=  tensor(1.9275, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 1 		  Q(s,a)=  tensor(1.4406, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 2 		  Q(s,a)=  tensor(1.4089, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.025642105263157897  Action:  0  Reward Received:  0
State:  0.1293052631578947  Action:  0  Reward Received:  0
State:  0.23844210526315787  Action:  0  Reward Received:  0
State:  0.3376842105263158  Action:  0  Reward Received:  0
State:  0.44421052631578944  Action:  0  Reward Received:  0
State:  0.5523368421052631  Action:  0  Reward Received:  0
State:  0.6589052631578948  Action:  0  Reward Received:  0
State:  0.7650526315789474  Action:  0  Reward Received:  0
State:  0.8670315789473685  Action:  0  Reward Received:  1

Total Reward Received:  1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 5.5577e-01,  1.3535e-01,  1.2189e-01,  3.0849e-04,  4.6617e-01,
         -2.1879e-01,  5.1250e-03, -3.1046e-02,  2.9996e-01, -1.8992e-01,
          6.7491e-02, -5.2536e-03, -8.6043e-02,  6.3494e-02,  1.3287e-01,
          4.3426e-03, -3.2048e-01,  7.4470e-02, -2.5870e-02,  2.0115e-03,
          2.4920e-01,  8.2178e-02, -2.9730e-01,  3.0779e-02,  2.7342e-01],
        [ 1.2400e-01, -6.8420e-02,  1.2965e-01,  7.8820e-02,  2.5845e-01,
          2.6701e-01,  1.8179e-01,  7.0760e-02,  1.2786e-01, -1.0441e-01,
          6.5001e-02,  1.0273e-01,  6.5756e-02, -2.0842e-01,  2.1241e-01,
         -2.0181e-01, -1.1509e-01, -3.3263e-02, -1.0602e-01,  1.2057e-01,
          3.5577e-02, -1.1991e-01, -3.8338e-02, -1.9000e-02,  1.4889e-01],
        [-5.8081e-03,  1.1791e-01,  2.0366e-01, -2.2370e-02,  1.7903e-02,
          1.6992e-01,  3.2340e-02, -1.6098e-01, -8.6901e-02,  8.3185e-02,
          9.7405e-02,  3.8433e-01, -1.0870e-01, -2.0462e-01, -2.5375e-04,
          1.8360e-02,  7.4883e-02, -1.1905e-03,  1.7689e-01,  4.9022e-02,
         -4.6894e-02, -2.0043e-01,  6.6961e-02,  4.1698e-02,  2.2272e-01]],
       dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5937, 0.4821, 0.5011], dtype=torch.float64, requires_grad=True)
Starting Training
layer weights:  Parameter containing:
tensor([[ 2.9311e-02,  1.5894e-01, -5.4558e-02, -1.9030e-01,  7.1507e-03,
          6.5094e-02, -1.5931e-02,  9.9801e-02, -9.7753e-02,  7.0874e-02,
         -6.8831e-04,  9.7546e-02,  1.7519e-01, -1.9035e-01, -1.3174e-01,
         -1.5258e-01,  1.9225e-01,  6.7412e-02,  7.3938e-02,  1.4417e-02,
         -9.5711e-02,  7.0338e-03,  5.0367e-02,  6.5440e-02, -1.5366e-01],
        [ 1.9757e-01,  1.3078e-01, -3.4152e-02, -3.8597e-02,  5.8149e-02,
         -1.4968e-01,  5.6789e-02, -1.0714e-01, -1.6760e-01, -1.0080e-01,
         -1.0689e-01,  3.9880e-02, -1.3069e-01,  5.6868e-02, -7.3135e-02,
         -1.8031e-01, -3.1082e-02, -8.4871e-02, -5.3451e-02, -5.0027e-02,
         -6.6161e-02,  1.7220e-01,  1.4764e-01,  1.6766e-01,  2.5660e-02],
        [-7.6616e-02,  9.6607e-05, -4.9341e-02, -8.2974e-02,  1.0774e-01,
          8.2007e-02, -1.9701e-01,  7.6589e-02,  1.3135e-01,  9.0445e-02,
         -1.0797e-01,  5.3147e-02,  5.1118e-02, -8.1010e-02, -8.7922e-02,
          1.7473e-01, -1.7387e-01,  9.8982e-03,  6.7604e-02, -4.7415e-02,
         -1.5576e-01,  1.8730e-01, -9.3140e-02,  1.3944e-01,  3.6160e-02]],
       dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.0907,  0.1201, -0.1935, -0.1036, -0.0191,  0.1459, -0.0402,  0.1227,
         -0.1852,  0.0148, -0.0203,  0.1827, -0.1900,  0.1836,  0.1389, -0.1632,
         -0.1633, -0.0403, -0.0098, -0.1412, -0.1577,  0.0655,  0.1446, -0.0997,
          0.0073],
        [-0.0945, -0.0403,  0.1083,  0.1456, -0.1035, -0.0304, -0.1687, -0.0471,
          0.1068, -0.0619,  0.1305, -0.0295,  0.0126,  0.1639,  0.0911,  0.1211,
          0.1478,  0.1120,  0.1188, -0.0296, -0.0748, -0.0279,  0.0514,  0.0347,
         -0.0864],
        [ 0.0696,  0.0788, -0.1024, -0.0009, -0.1228,  0.1268,  0.1956,  0.0324,
          0.0871, -0.1129,  0.1296,  0.1926, -0.0125,  0.0796, -0.1969,  0.0985,
          0.1364, -0.0679,  0.1856,  0.0211,  0.0890, -0.0571, -0.0902, -0.1578,
          0.0651]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.4644, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3986, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3928, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5475, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4026, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4289, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6434, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4584, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4357, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7537, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4955, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4375, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 0 		  Q(s,a)=  tensor(0.8754, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 1 		  Q(s,a)=  tensor(0.5457, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 2 		  Q(s,a)=  tensor(0.4622, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 0 		  Q(s,a)=  tensor(0.9992, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 1 		  Q(s,a)=  tensor(0.5279, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 2 		  Q(s,a)=  tensor(0.4307, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 0 		  Q(s,a)=  tensor(1.1178, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 1 		  Q(s,a)=  tensor(0.5809, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 2 		  Q(s,a)=  tensor(0.4457, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 0 		  Q(s,a)=  tensor(1.2288, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 1 		  Q(s,a)=  tensor(0.6204, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 2 		  Q(s,a)=  tensor(0.4532, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 0 		  Q(s,a)=  tensor(1.3871, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 1 		  Q(s,a)=  tensor(0.6608, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 2 		  Q(s,a)=  tensor(0.4811, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 0 		  Q(s,a)=  tensor(1.4745, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 1 		  Q(s,a)=  tensor(0.6797, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 2 		  Q(s,a)=  tensor(0.4912, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.03174736842105263  Action:  0  Reward Received:  0
State:  0.1288842105263158  Action:  0  Reward Received:  0
State:  0.23688421052631584  Action:  0  Reward Received:  0
State:  0.3416  Action:  0  Reward Received:  0
State:  0.44631578947368417  Action:  0  Reward Received:  0
State:  0.5533473684210527  Action:  0  Reward Received:  0
State:  0.6639999999999999  Action:  0  Reward Received:  0
State:  0.7619368421052631  Action:  0  Reward Received:  0
State:  0.864042105263158  Action:  0  Reward Received:  1

Total Reward Received:  1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.1113,  0.0476,  0.2066, -0.1212, -0.1284,  0.1583,  0.1718,  0.4034,
          0.1957,  0.0631,  0.1008,  0.2882,  0.0525, -0.1907,  0.3072,  0.0715,
         -0.1741, -0.0590,  0.1365,  0.0553, -0.2618,  0.2497, -0.3849, -0.2556,
          0.0777],
        [-0.0802,  0.0412, -0.0574,  0.2344,  0.1050, -0.2163, -0.0458,  0.0829,
          0.2050,  0.2104, -0.0249, -0.0158, -0.2036, -0.0765,  0.1633,  0.3423,
          0.0225,  0.1541,  0.1091, -0.0568,  0.0270, -0.2430, -0.1031, -0.1422,
         -0.1357],
        [-0.2043,  0.0873, -0.1758,  0.2254, -0.0844,  0.0655,  0.0686, -0.0329,
          0.0587, -0.0117,  0.1001,  0.2760, -0.0811, -0.0142, -0.4045,  0.0549,
          0.2064,  0.1017,  0.0824, -0.0175, -0.0362, -0.0064, -0.0461, -0.1767,
          0.0545]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3886, 0.3935, 0.3952], dtype=torch.float64, requires_grad=True)
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.1568,  0.0837,  0.1438,  0.1691, -0.1025, -0.1790,  0.1692,  0.1725,
         -0.0366, -0.0538, -0.1906,  0.1865, -0.0935, -0.1097,  0.0356, -0.0582,
         -0.0111, -0.1487,  0.0032,  0.0195, -0.1807, -0.0377,  0.1640,  0.0183,
         -0.1736],
        [-0.0768,  0.1726, -0.0803, -0.0011, -0.1916,  0.0626, -0.0610,  0.1842,
          0.0516, -0.0596,  0.1205, -0.1908, -0.1504, -0.0633,  0.0645,  0.0732,
         -0.0746,  0.0250,  0.0507, -0.0408,  0.1109, -0.0390, -0.1522,  0.0686,
         -0.0023],
        [ 0.0691, -0.1312,  0.0439, -0.0354, -0.0118,  0.1929,  0.1221,  0.0415,
         -0.0273, -0.0915, -0.1159, -0.1983,  0.0600, -0.1341, -0.0100,  0.1827,
         -0.1561, -0.1133,  0.1662, -0.0453, -0.0930, -0.1932, -0.1570, -0.0905,
         -0.1496]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1226, -0.1852, -0.1561,  0.0061, -0.1647,  0.0296,  0.1302, -0.1199,
         -0.1080, -0.1585,  0.1508, -0.1648, -0.0027, -0.1500,  0.1176, -0.0570,
         -0.0051, -0.0310, -0.0994,  0.1414,  0.1707,  0.1247,  0.0502,  0.1685,
          0.1690],
        [ 0.0338,  0.0775,  0.1244, -0.1819, -0.0786, -0.0899,  0.1517,  0.0499,
         -0.1444, -0.1812, -0.1310, -0.0678,  0.0713, -0.0263,  0.0299,  0.0145,
          0.0550, -0.0101,  0.1632, -0.1615, -0.0527,  0.1179, -0.0379, -0.0305,
         -0.0249],
        [-0.1392,  0.1599,  0.0266,  0.1571, -0.0631, -0.1635,  0.0409,  0.0557,
         -0.0969, -0.0773,  0.0367,  0.0477, -0.1111, -0.1892,  0.0314, -0.1960,
         -0.0480,  0.1592,  0.0596, -0.0357, -0.1294,  0.1976, -0.1408,  0.1822,
          0.1717]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5455, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4901, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4633, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6401, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4353, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4988, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7285, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4240, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4888, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.8150, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.4139, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5159, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 0 		  Q(s,a)=  tensor(0.9164, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 1 		  Q(s,a)=  tensor(0.3687, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 2 		  Q(s,a)=  tensor(0.4953, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 0 		  Q(s,a)=  tensor(1.0215, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 1 		  Q(s,a)=  tensor(0.3700, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 2 		  Q(s,a)=  tensor(0.5112, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 0 		  Q(s,a)=  tensor(1.1030, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 1 		  Q(s,a)=  tensor(0.3150, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 2 		  Q(s,a)=  tensor(0.4991, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 0 		  Q(s,a)=  tensor(1.2332, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 1 		  Q(s,a)=  tensor(0.2977, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 2 		  Q(s,a)=  tensor(0.5337, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 0 		  Q(s,a)=  tensor(1.3814, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 1 		  Q(s,a)=  tensor(0.2704, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 2 		  Q(s,a)=  tensor(0.5324, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 0 		  Q(s,a)=  tensor(1.4596, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 1 		  Q(s,a)=  tensor(0.2344, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 2 		  Q(s,a)=  tensor(0.5403, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.0248  Action:  0  Reward Received:  0
State:  0.13452631578947372  Action:  0  Reward Received:  0
State:  0.2375578947368421  Action:  0  Reward Received:  0
State:  0.3399157894736842  Action:  0  Reward Received:  0
State:  0.4461473684210525  Action:  0  Reward Received:  0
State:  0.5521263157894737  Action:  0  Reward Received:  0
State:  0.6567578947368421  Action:  0  Reward Received:  0
State:  0.7578105263157893  Action:  0  Reward Received:  0
State:  0.8664421052631579  Action:  0  Reward Received:  1

Total Reward Received:  1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0038,  0.1641, -0.4827, -0.0248, -0.2016, -0.1181, -0.0769, -0.2705,
         -0.0092,  0.1430,  0.2489, -0.2387,  0.2763, -0.1247,  0.2902, -0.0219,
         -0.0945,  0.2867, -0.1267,  0.6934,  0.0941,  0.0775, -0.0619,  0.2915,
          0.2862],
        [-0.0988, -0.0851,  0.1716, -0.0822, -0.0630,  0.0277,  0.0445,  0.0901,
         -0.1920, -0.0682, -0.1612, -0.1283,  0.1135, -0.1274,  0.0245, -0.0500,
          0.0200,  0.0180,  0.2147, -0.1510, -0.0897,  0.2013,  0.0229, -0.1059,
          0.1882],
        [-0.2212,  0.1269,  0.0113,  0.2529,  0.1162, -0.2320,  0.0530,  0.2245,
         -0.1596, -0.0802,  0.0053, -0.0492, -0.3893, -0.0459,  0.1519,  0.1100,
         -0.2411,  0.1841, -0.0821, -0.0177, -0.0307,  0.1491,  0.1109,  0.1005,
          0.0182]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.4837, 0.4967, 0.4754], dtype=torch.float64, requires_grad=True)
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1799,  0.1487,  0.1075,  0.0406, -0.0716,  0.1564, -0.0164, -0.1115,
          0.0764,  0.0642,  0.0337,  0.0612,  0.0129, -0.1746,  0.0509, -0.1824,
          0.0183, -0.1002,  0.1248, -0.1259,  0.1113,  0.0217,  0.1273,  0.1901,
         -0.1865],
        [-0.0411,  0.1499, -0.0790,  0.1251,  0.1640,  0.1393, -0.1812,  0.1793,
         -0.1125,  0.1742, -0.1318, -0.1961,  0.1119,  0.1034, -0.1279, -0.0207,
         -0.0826, -0.0679, -0.0900, -0.1584,  0.0037, -0.1509, -0.1835, -0.0126,
         -0.1323],
        [ 0.0252, -0.1730,  0.0093, -0.1956, -0.1952, -0.1974, -0.0856,  0.1826,
         -0.0295,  0.0532,  0.1751, -0.1265,  0.0850,  0.0998, -0.1051, -0.1326,
         -0.0184,  0.1688,  0.1668, -0.1889, -0.1797,  0.0038, -0.1650, -0.1853,
         -0.1702]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0584,  0.0875, -0.1393, -0.0329, -0.1735,  0.1130, -0.1622,  0.0016,
         -0.1432, -0.1341, -0.0302, -0.0798, -0.0138,  0.1603, -0.0447,  0.0700,
         -0.0358, -0.0975, -0.1050, -0.1041,  0.0267, -0.1746, -0.0717, -0.1810,
         -0.0844],
        [ 0.0802, -0.1950,  0.1386,  0.0311, -0.0898, -0.1257, -0.1487,  0.1958,
         -0.1639, -0.0189,  0.1677, -0.1345, -0.0404,  0.0826,  0.0833,  0.0772,
          0.1110, -0.1405,  0.0568, -0.1715,  0.1505,  0.1159,  0.0228,  0.0238,
         -0.1307],
        [-0.1470, -0.0558, -0.0534, -0.0717,  0.0369,  0.1175, -0.0755, -0.1590,
         -0.0735, -0.1858, -0.1918, -0.0761,  0.0054,  0.1210,  0.1988,  0.0634,
          0.0244, -0.0833,  0.1858, -0.0576, -0.1869, -0.1486,  0.1131, -0.1170,
         -0.1223]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3810, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2860, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.2969, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4600, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3619, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4139, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5495, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4641, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5170, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6222, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5650, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6128, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 0 		  Q(s,a)=  tensor(0.6954, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 1 		  Q(s,a)=  tensor(0.6208, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 2 		  Q(s,a)=  tensor(0.6610, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 0 		  Q(s,a)=  tensor(0.7737, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 1 		  Q(s,a)=  tensor(0.7141, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 2 		  Q(s,a)=  tensor(0.7321, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 0 		  Q(s,a)=  tensor(0.8700, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 1 		  Q(s,a)=  tensor(0.7819, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 2 		  Q(s,a)=  tensor(0.8220, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 0 		  Q(s,a)=  tensor(0.9960, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 1 		  Q(s,a)=  tensor(0.8483, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 2 		  Q(s,a)=  tensor(0.8957, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 0 		  Q(s,a)=  tensor(1.1601, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 1 		  Q(s,a)=  tensor(0.9488, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 2 		  Q(s,a)=  tensor(0.9885, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 0 		  Q(s,a)=  tensor(1.1954, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 1 		  Q(s,a)=  tensor(1.0514, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 2 		  Q(s,a)=  tensor(1.0884, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.021894736842105265  Action:  0  Reward Received:  0
State:  0.13541052631578948  Action:  0  Reward Received:  0
State:  0.23873684210526314  Action:  0  Reward Received:  0
State:  0.3409684210526316  Action:  0  Reward Received:  0
State:  0.45381052631578944  Action:  0  Reward Received:  0
State:  0.5514105263157895  Action:  0  Reward Received:  0
State:  0.6572631578947369  Action:  0  Reward Received:  0
State:  0.7654736842105262  Action:  0  Reward Received:  0
State:  0.8666526315789472  Action:  0  Reward Received:  1

Total Reward Received:  1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0157,  0.4905, -0.0086,  0.3517,  0.2776,  0.0830,  0.4028,  0.0674,
         -0.0071,  0.0217,  0.0618,  0.0227,  0.0378,  0.4359, -0.2744,  0.5890,
         -0.0343, -0.1795,  0.0364, -0.2825, -0.2911, -0.3888, -0.4690,  0.0706,
         -0.1053],
        [ 0.2049, -0.2594,  0.1509,  0.1036, -0.1374, -0.1197, -0.0921,  0.1688,
         -0.1603,  0.1029,  0.2965,  0.0336, -0.0835,  0.1423,  0.1087,  0.1226,
          0.1603, -0.0996,  0.1751, -0.0945,  0.1600,  0.1630, -0.0562, -0.0258,
         -0.1842],
        [ 0.1367, -0.1000, -0.1857,  0.1972, -0.0521,  0.0077, -0.2568,  0.0162,
         -0.1976,  0.2051, -0.0672,  0.2143,  0.0642,  0.2432,  0.1262,  0.1528,
          0.1231,  0.1824,  0.4056,  0.1954, -0.1657,  0.1362, -0.0632, -0.1740,
         -0.3744]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3145, 0.2780, 0.3220], dtype=torch.float64, requires_grad=True)
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1951,  0.0132,  0.1731, -0.1885, -0.1250, -0.1928,  0.0354,  0.1722,
         -0.1833,  0.0556, -0.1146, -0.1642,  0.1804,  0.0980,  0.0818,  0.0693,
         -0.0835,  0.0221,  0.0423, -0.0205,  0.0659, -0.1891,  0.0057, -0.1114,
         -0.0975],
        [ 0.0993, -0.1798,  0.1187, -0.0896, -0.0453, -0.1576,  0.1546, -0.1547,
         -0.1004,  0.1214,  0.1548,  0.1704, -0.1856,  0.0842, -0.1298, -0.1641,
         -0.0820,  0.1668, -0.0634, -0.1226,  0.1900,  0.0848, -0.0651, -0.1672,
          0.0791],
        [ 0.0310,  0.1626,  0.0127, -0.0866, -0.1497,  0.1689,  0.0791, -0.0881,
         -0.1555, -0.0110,  0.1931,  0.0677, -0.0304,  0.1528,  0.1277, -0.1203,
         -0.0353,  0.0943, -0.0301,  0.0335, -0.1445,  0.0093, -0.0992, -0.0667,
          0.0620]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1048, -0.1723,  0.1979, -0.0567, -0.1568,  0.1645,  0.0848, -0.1623,
         -0.1383,  0.0599,  0.0415,  0.0362,  0.0006,  0.0323,  0.1627,  0.1994,
          0.1444,  0.0392,  0.0081,  0.0400, -0.1363,  0.0803, -0.1921, -0.1176,
         -0.0861],
        [ 0.0712, -0.1282,  0.0730,  0.0889,  0.0951,  0.0740,  0.1884,  0.1890,
          0.1437,  0.0559, -0.0477,  0.1438,  0.0371, -0.1501, -0.1713, -0.0932,
          0.0261, -0.0089, -0.1497, -0.1160, -0.0310,  0.0462,  0.1843, -0.0077,
         -0.1543],
        [-0.0173,  0.1049, -0.0810, -0.1090, -0.0986,  0.0484, -0.1206,  0.1605,
          0.1610,  0.1505, -0.0119, -0.0784,  0.0250, -0.1689,  0.0313,  0.1123,
         -0.0891,  0.1060,  0.0267, -0.0922, -0.1526, -0.0161,  0.1100, -0.0299,
          0.1366]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.3232, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2457, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.2776, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4123, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2881, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4026, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4995, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3326, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5040, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.5964, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3938, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6023, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 0 		  Q(s,a)=  tensor(0.7098, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 1 		  Q(s,a)=  tensor(0.4223, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 2 		  Q(s,a)=  tensor(0.6821, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 0 		  Q(s,a)=  tensor(0.7802, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 1 		  Q(s,a)=  tensor(0.4449, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 2 		  Q(s,a)=  tensor(0.7980, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 0 		  Q(s,a)=  tensor(0.8668, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 1 		  Q(s,a)=  tensor(0.4829, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 2 		  Q(s,a)=  tensor(0.9107, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 0 		  Q(s,a)=  tensor(1.0065, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 1 		  Q(s,a)=  tensor(0.5265, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 2 		  Q(s,a)=  tensor(0.9995, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 0 		  Q(s,a)=  tensor(1.1833, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 1 		  Q(s,a)=  tensor(0.5667, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 2 		  Q(s,a)=  tensor(1.1126, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 0 		  Q(s,a)=  tensor(1.2094, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 1 		  Q(s,a)=  tensor(0.6119, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 2 		  Q(s,a)=  tensor(1.2033, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.024968421052631577  Action:  0  Reward Received:  0
State:  0.13132631578947368  Action:  0  Reward Received:  0
State:  0.2327157894736842  Action:  2  Reward Received:  0
State:  0.2327157894736842  Action:  2  Reward Received:  0
State:  0.2327157894736842  Action:  2  Reward Received:  0
State:  0.2327157894736842  Action:  2  Reward Received:  0
State:  0.2327157894736842  Action:  2  Reward Received:  0
State:  0.2327157894736842  Action:  2  Reward Received:  0
State:  0.2327157894736842  Action:  2  Reward Received:  0

Total Reward Received:  0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.1763, -0.2329,  0.1984,  0.1692, -0.3525,  0.3531,  0.0689,  0.3368,
         -0.0505,  0.3089, -0.0336,  0.1245, -0.5361, -0.1170,  0.1816,  0.2616,
          0.2585,  0.0174, -0.1340,  0.4370, -0.4916,  0.4581, -0.1244, -0.1010,
         -0.2162],
        [ 0.0301, -0.0611,  0.1427, -0.1773,  0.0738, -0.0037, -0.0331,  0.0517,
          0.1710, -0.0388, -0.0802,  0.1041,  0.2363, -0.1133, -0.0653, -0.0734,
          0.2291,  0.0083, -0.0082, -0.1271,  0.1060, -0.1121,  0.1237,  0.1278,
         -0.1490],
        [ 0.0711,  0.0182, -0.0846, -0.0419, -0.0983,  0.0887, -0.2347,  0.1250,
          0.1790,  0.2166, -0.0107, -0.0143,  0.1950, -0.0469,  0.1626,  0.0775,
         -0.0542,  0.0698,  0.0934, -0.0485,  0.0154, -0.0426,  0.0439,  0.0725,
          0.1966]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.2840, 0.2519, 0.2750], dtype=torch.float64, requires_grad=True)
Starting Training
layer weights:  Parameter containing:
tensor([[ 4.0588e-02, -1.4905e-01,  6.1812e-02, -4.2026e-02, -1.3436e-01,
         -7.9725e-03, -1.1969e-01,  9.9157e-02, -1.6813e-01, -9.9285e-02,
          6.0686e-02,  1.1224e-01, -7.1369e-02, -1.2485e-01,  9.9694e-02,
         -1.0030e-01,  1.5178e-01,  3.3108e-02, -1.1671e-01, -1.1738e-01,
          7.4173e-02,  7.4709e-02, -1.2099e-01, -1.1591e-01,  6.7508e-02],
        [-1.1945e-01, -1.2020e-01,  1.6264e-01,  1.7649e-01, -4.6034e-02,
          1.9872e-01,  1.7516e-01,  9.1056e-02,  2.8928e-02, -1.8959e-02,
         -1.1524e-01,  1.9985e-01, -9.9124e-02, -7.9319e-02, -4.1995e-02,
          1.0083e-02, -4.6921e-05, -3.5638e-02,  1.0731e-01, -9.4261e-02,
         -1.3989e-01, -4.4442e-03, -4.0382e-02, -1.4385e-01, -6.2731e-02],
        [ 1.0405e-01,  1.5735e-02,  2.2807e-02,  1.0965e-01, -5.7456e-02,
          1.6005e-01, -1.2117e-01,  1.7491e-01, -1.5336e-01,  1.8270e-01,
          2.2370e-02,  9.5064e-02, -1.8571e-01,  6.1608e-02, -1.5484e-02,
         -1.9348e-01, -1.6016e-01,  1.1570e-03, -1.6034e-01, -6.2879e-02,
          1.1557e-01, -1.8730e-01, -1.9593e-01, -4.9693e-02,  1.3711e-01]],
       dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1043, -0.0227, -0.0887, -0.1424, -0.0865, -0.0716,  0.1854,  0.1588,
          0.1410, -0.1384,  0.1817, -0.1220, -0.1788, -0.0643, -0.0658, -0.1381,
          0.0714,  0.0687, -0.0572, -0.0198, -0.1562,  0.1964, -0.0738, -0.1543,
         -0.1798],
        [ 0.0204,  0.0091, -0.1672,  0.0798,  0.0598, -0.0710, -0.0213, -0.1157,
         -0.1337, -0.1703,  0.1862, -0.0834,  0.0172, -0.0871,  0.1415, -0.0741,
         -0.1951,  0.0135, -0.0645, -0.1800,  0.1955,  0.1793, -0.0711,  0.1793,
         -0.0685],
        [ 0.0798,  0.0366, -0.0145,  0.1363,  0.0477,  0.0613,  0.0752,  0.1754,
         -0.1604, -0.0417,  0.0892, -0.0936,  0.0024,  0.0972,  0.0469, -0.0503,
         -0.1728, -0.1835, -0.0854, -0.0986, -0.1581,  0.1394,  0.1550,  0.0051,
          0.0286]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6161, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6068, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5940, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6303, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5455, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6417, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6382, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5260, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6634, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6556, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5394, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6596, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 0 		  Q(s,a)=  tensor(0.6741, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 1 		  Q(s,a)=  tensor(0.4529, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 2 		  Q(s,a)=  tensor(0.6928, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 0 		  Q(s,a)=  tensor(0.6948, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 1 		  Q(s,a)=  tensor(0.4328, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 2 		  Q(s,a)=  tensor(0.6586, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 0 		  Q(s,a)=  tensor(0.6971, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 1 		  Q(s,a)=  tensor(0.4180, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 2 		  Q(s,a)=  tensor(0.6624, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 0 		  Q(s,a)=  tensor(0.7181, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 1 		  Q(s,a)=  tensor(0.3782, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 2 		  Q(s,a)=  tensor(0.6810, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 0 		  Q(s,a)=  tensor(0.7418, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 1 		  Q(s,a)=  tensor(0.3362, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 2 		  Q(s,a)=  tensor(0.6911, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 0 		  Q(s,a)=  tensor(0.7500, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 1 		  Q(s,a)=  tensor(0.3075, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 2 		  Q(s,a)=  tensor(0.7260, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.023621052631578945  Action:  0  Reward Received:  0
State:  0.1294736842105263  Action:  2  Reward Received:  0
State:  0.1294736842105263  Action:  2  Reward Received:  0
State:  0.1294736842105263  Action:  2  Reward Received:  0
State:  0.1294736842105263  Action:  2  Reward Received:  0
State:  0.1294736842105263  Action:  2  Reward Received:  0
State:  0.1294736842105263  Action:  2  Reward Received:  0
State:  0.1294736842105263  Action:  2  Reward Received:  0
State:  0.1294736842105263  Action:  2  Reward Received:  0

Total Reward Received:  0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0673, -0.1422, -0.1088, -0.0054, -0.1004,  0.0547, -0.0153, -0.0458,
          0.0522,  0.0715,  0.2062,  0.1260, -0.0489, -0.1068, -0.0693,  0.0639,
          0.0679,  0.0765,  0.0732, -0.0369, -0.0403,  0.2277, -0.0090, -0.0932,
         -0.1138],
        [ 0.1045,  0.0904, -0.1732, -0.1612,  0.2649, -0.3681,  0.2930, -0.0045,
          0.0020, -0.2998,  0.1027, -0.0136, -0.0842,  0.2879, -0.0202, -0.3362,
          0.1028,  0.0644, -0.2095, -0.0430,  0.1549, -0.0039,  0.0679,  0.2267,
         -0.3484],
        [-0.0834,  0.0051, -0.0892,  0.1025,  0.1495, -0.0891, -0.0676,  0.0102,
         -0.1868, -0.2838,  0.0314, -0.0391, -0.0736,  0.1526,  0.2356,  0.1109,
         -0.2265, -0.3840, -0.2070, -0.0314,  0.0412,  0.3311,  0.2596,  0.0791,
          0.3340]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6055, 0.6116, 0.6252], dtype=torch.float64, requires_grad=True)
Starting Training
layer weights:  Parameter containing:
tensor([[ 0.0019,  0.0242, -0.1293,  0.1487, -0.0743,  0.0042, -0.0454,  0.1004,
         -0.1235,  0.0230,  0.1775, -0.0656,  0.1376,  0.1773, -0.0331, -0.0544,
         -0.1996,  0.1313, -0.0041,  0.0239, -0.1760,  0.1359,  0.1483,  0.0631,
         -0.0363],
        [-0.1251, -0.0911, -0.0899,  0.1115, -0.1354, -0.0062, -0.1477,  0.1644,
          0.0984,  0.0939,  0.1574, -0.1342, -0.1414,  0.1610, -0.0321,  0.1804,
          0.0156,  0.1297, -0.1687, -0.0269,  0.1324, -0.1207,  0.0470, -0.1219,
          0.1096],
        [-0.1272, -0.1530, -0.0881, -0.1306,  0.1934, -0.0978,  0.1499,  0.0779,
         -0.1256,  0.1004,  0.0554, -0.1751,  0.1066, -0.1951, -0.0753, -0.1805,
          0.0792, -0.1469,  0.0701, -0.1782, -0.0355, -0.0796, -0.0061, -0.0176,
         -0.0562]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1067,  0.0990,  0.1884, -0.1229,  0.0211,  0.0437,  0.1328,  0.0741,
          0.1330,  0.0584,  0.0106,  0.1438,  0.0999,  0.0485, -0.1461,  0.0905,
          0.0357,  0.0049, -0.0345, -0.1709,  0.0887, -0.1906, -0.0374, -0.1847,
          0.1753],
        [-0.1819,  0.0207, -0.1420,  0.1525, -0.1394,  0.1428, -0.1019, -0.1550,
         -0.1811, -0.0412,  0.1259,  0.0749,  0.0301, -0.0219, -0.1784, -0.0020,
          0.1028,  0.1927, -0.0717,  0.0561, -0.1565, -0.0596, -0.0310, -0.0323,
          0.0310],
        [ 0.0555, -0.0002,  0.0878,  0.0519, -0.1643, -0.0414,  0.0313, -0.1585,
          0.1248,  0.0671,  0.0501,  0.0135,  0.1161, -0.1076,  0.0337,  0.1946,
         -0.0287, -0.0690,  0.1913,  0.1427, -0.1983, -0.0612, -0.1658, -0.1856,
         -0.0341]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.2878, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2693, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.2331, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3255, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2962, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.2353, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3730, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3275, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.2171, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.4413, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3556, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.1808, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 0 		  Q(s,a)=  tensor(0.4786, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 1 		  Q(s,a)=  tensor(0.3893, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 2 		  Q(s,a)=  tensor(0.1891, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 0 		  Q(s,a)=  tensor(0.5223, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 1 		  Q(s,a)=  tensor(0.3749, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 2 		  Q(s,a)=  tensor(0.1849, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 0 		  Q(s,a)=  tensor(0.6047, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 1 		  Q(s,a)=  tensor(0.4220, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 2 		  Q(s,a)=  tensor(0.1533, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 0 		  Q(s,a)=  tensor(0.7429, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 1 		  Q(s,a)=  tensor(0.4323, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 2 		  Q(s,a)=  tensor(0.1335, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 0 		  Q(s,a)=  tensor(0.9136, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 1 		  Q(s,a)=  tensor(0.4395, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 2 		  Q(s,a)=  tensor(0.1247, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 0 		  Q(s,a)=  tensor(0.8100, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 1 		  Q(s,a)=  tensor(0.5019, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 2 		  Q(s,a)=  tensor(0.1041, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.025305263157894737  Action:  0  Reward Received:  0
State:  0.12896842105263157  Action:  0  Reward Received:  0
State:  0.24  Action:  0  Reward Received:  0
State:  0.3424842105263158  Action:  0  Reward Received:  0
State:  0.44618947368421047  Action:  0  Reward Received:  0
State:  0.551621052631579  Action:  0  Reward Received:  0
State:  0.656378947368421  Action:  0  Reward Received:  0
State:  0.7656421052631579  Action:  0  Reward Received:  0
State:  0.8648842105263157  Action:  0  Reward Received:  1

Total Reward Received:  1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0098,  0.3063, -0.6930, -0.3368,  0.1532, -0.3015,  0.0341, -0.4493,
          0.0943,  0.3565,  0.2968,  0.2836,  0.6780, -0.2863, -0.3279,  0.5373,
          0.3697, -0.1080,  0.3103, -0.2195,  0.5919,  0.3031, -0.1724, -0.6906,
         -0.0543],
        [-0.2050, -0.0259,  0.0010,  0.0916, -0.2590,  0.3128, -0.0901,  0.0628,
         -0.3258, -0.0381,  0.0570,  0.1876,  0.0395,  0.2915, -0.1856, -0.0611,
          0.0558,  0.3300, -0.0860,  0.0154, -0.1903, -0.0098,  0.1951,  0.0429,
          0.0055],
        [-0.0707, -0.0723,  0.0580,  0.0742, -0.1448,  0.1075, -0.0343, -0.1342,
          0.2140, -0.0204,  0.0033,  0.0446, -0.0624, -0.0748,  0.0014,  0.2926,
         -0.1925, -0.0764,  0.1042,  0.0631, -0.1652, -0.0756, -0.0274, -0.0121,
          0.0624]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.2253, 0.2769, 0.2391], dtype=torch.float64, requires_grad=True)
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0437, -0.1348,  0.0571,  0.1191,  0.0792, -0.0412,  0.1485,  0.1606,
          0.1744,  0.1793,  0.0203,  0.0440, -0.1691,  0.1210,  0.1410, -0.0476,
         -0.0734, -0.0632,  0.1948,  0.1118,  0.0675, -0.0821,  0.0429,  0.1332,
          0.0065],
        [-0.1196, -0.1581, -0.0836, -0.1118,  0.0067, -0.0880, -0.0492,  0.1347,
          0.1397, -0.0063, -0.0566, -0.0256, -0.1037, -0.0784, -0.0866,  0.0832,
          0.0551,  0.0541,  0.0067, -0.1928, -0.0697, -0.1266, -0.1305, -0.0597,
         -0.0540],
        [-0.0132, -0.0100, -0.1384, -0.0809, -0.0664, -0.1840, -0.1659, -0.1597,
          0.0579,  0.0634,  0.1394,  0.1306,  0.1756,  0.1895, -0.1849, -0.1309,
         -0.1702, -0.0548, -0.0600, -0.0402, -0.1974,  0.1453,  0.1252,  0.1419,
         -0.1357]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 1.1357e-01,  3.5399e-02, -1.2495e-01,  8.1072e-02, -1.8943e-01,
          1.4388e-01, -9.7814e-02,  1.3520e-01, -1.8285e-01, -5.6312e-02,
         -1.3339e-01, -1.2656e-02, -4.3823e-02,  1.5712e-01,  5.0747e-02,
          1.2793e-01,  8.0464e-02, -4.9323e-02,  1.4285e-01,  1.0407e-01,
         -9.8385e-02, -1.9352e-01,  1.8794e-01, -4.5447e-02, -9.6444e-02],
        [-1.7154e-02, -8.5865e-02, -1.7446e-01,  1.0716e-01, -1.3341e-01,
         -9.1852e-02,  3.3807e-02,  8.8029e-02,  4.7287e-02, -1.7315e-01,
         -7.1734e-02, -1.3347e-02,  2.0220e-02, -1.5527e-01,  7.0572e-02,
          1.9193e-01,  1.5160e-01,  2.6226e-05, -3.1449e-03, -9.6605e-02,
         -1.0627e-02, -1.8005e-01,  1.6493e-01,  1.3148e-01, -1.6375e-01],
        [-8.8349e-02,  4.8739e-02, -8.2799e-02, -4.2837e-02, -3.7557e-02,
         -6.8950e-02,  1.3982e-01, -1.7389e-01,  3.5863e-03, -1.3475e-01,
          1.6130e-01, -1.0165e-01,  5.5298e-02, -1.2674e-01,  1.7935e-01,
          1.6992e-01, -1.6814e-01, -5.0281e-02, -1.0992e-01, -1.6893e-01,
         -1.8103e-01, -5.1494e-02, -1.9651e-01, -5.7460e-02, -7.2839e-02]],
       dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6599, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5913, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6067, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6733, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5274, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6427, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7120, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5171, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6565, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7296, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5153, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6563, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 0 		  Q(s,a)=  tensor(0.7616, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 1 		  Q(s,a)=  tensor(0.4802, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 2 		  Q(s,a)=  tensor(0.6802, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 0 		  Q(s,a)=  tensor(0.8151, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 1 		  Q(s,a)=  tensor(0.4518, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 2 		  Q(s,a)=  tensor(0.6636, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 0 		  Q(s,a)=  tensor(0.8394, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 1 		  Q(s,a)=  tensor(0.4220, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 2 		  Q(s,a)=  tensor(0.6990, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 0 		  Q(s,a)=  tensor(0.8845, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 1 		  Q(s,a)=  tensor(0.4238, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 2 		  Q(s,a)=  tensor(0.6912, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 0 		  Q(s,a)=  tensor(0.9238, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 1 		  Q(s,a)=  tensor(0.3712, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 2 		  Q(s,a)=  tensor(0.7022, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 0 		  Q(s,a)=  tensor(0.9483, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 1 		  Q(s,a)=  tensor(0.3529, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 2 		  Q(s,a)=  tensor(0.6987, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.02703157894736842  Action:  0  Reward Received:  0
State:  0.1287578947368421  Action:  0  Reward Received:  0
State:  0.230021052631579  Action:  0  Reward Received:  0
State:  0.34311578947368415  Action:  0  Reward Received:  0
State:  0.4397052631578947  Action:  0  Reward Received:  0
State:  0.5542315789473684  Action:  0  Reward Received:  0
State:  0.6594105263157896  Action:  0  Reward Received:  0
State:  0.7629052631578948  Action:  0  Reward Received:  0
State:  0.8713263157894737  Action:  0  Reward Received:  1

Total Reward Received:  1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.1095,  0.1156, -0.1563,  0.1292, -0.0985,  0.0279,  0.1302,  0.2219,
         -0.3329, -0.1700,  0.0138, -0.1906,  0.0107,  0.0027, -0.0214, -0.0990,
          0.0671,  0.2888,  0.0106,  0.1557, -0.0679, -0.0309,  0.2344, -0.0937,
          0.0545],
        [-0.1414, -0.0975, -0.2285,  0.3046,  0.0809, -0.0514,  0.0516,  0.0329,
         -0.0315, -0.0272,  0.0224, -0.1527, -0.1000, -0.2652,  0.1586,  0.0504,
          0.0693,  0.2241,  0.0982, -0.2265, -0.0535,  0.1057,  0.1029,  0.0938,
         -0.2458],
        [ 0.0783,  0.1212, -0.0445, -0.0074, -0.1178, -0.0737,  0.0566, -0.1775,
          0.1917,  0.0456,  0.1316,  0.1777,  0.0621,  0.0005,  0.3228,  0.2540,
          0.0163, -0.2264,  0.0128, -0.1562, -0.1003, -0.1773, -0.2571,  0.0234,
         -0.0620]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6410, 0.5795, 0.6172], dtype=torch.float64, requires_grad=True)
Starting Training
layer weights:  Parameter containing:
tensor([[-0.1444,  0.0254,  0.1713,  0.1587, -0.0033, -0.1092,  0.0852, -0.1137,
          0.1913,  0.1724, -0.0906, -0.0233, -0.0925,  0.0306,  0.0503,  0.1372,
          0.1103, -0.0977,  0.1397,  0.0740,  0.1842,  0.1689,  0.1048,  0.0760,
          0.0469],
        [-0.0530, -0.0902,  0.0599,  0.1215,  0.0500, -0.1788, -0.1268,  0.0075,
          0.0084,  0.1486, -0.0393, -0.0555,  0.1690,  0.1519, -0.1657, -0.1884,
         -0.0887,  0.0858, -0.1150, -0.0941, -0.0874, -0.0360,  0.0086, -0.0343,
         -0.0062],
        [-0.1321, -0.1877, -0.1748,  0.0479, -0.1753, -0.1266, -0.0543,  0.0789,
          0.0459, -0.0592, -0.1981, -0.0106, -0.0817,  0.0303,  0.1079, -0.0202,
          0.1158,  0.0288, -0.1972, -0.1210, -0.0634, -0.0170, -0.1023, -0.1932,
         -0.1904]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1897, -0.1991, -0.1486, -0.0537, -0.0545,  0.1704, -0.1482, -0.1430,
         -0.1942,  0.0440, -0.1124, -0.1631, -0.1909, -0.1472,  0.0331, -0.0289,
         -0.1215, -0.0955, -0.1710, -0.1832, -0.0588, -0.1247, -0.0757,  0.1759,
          0.0446],
        [-0.1099, -0.0967,  0.0180, -0.1873,  0.0570, -0.1553,  0.0761,  0.1914,
          0.1642, -0.0323,  0.0143, -0.1037,  0.1192, -0.1584,  0.1367, -0.1380,
          0.0309,  0.1631, -0.0019,  0.0305, -0.1243,  0.0530,  0.0187,  0.1248,
          0.0846],
        [ 0.0682, -0.1202, -0.1633, -0.0366,  0.0957, -0.1705, -0.1161, -0.1781,
          0.0381,  0.0271,  0.0210, -0.0647,  0.0659, -0.1224, -0.0155, -0.1333,
         -0.1312,  0.1256, -0.1294,  0.1145,  0.0672, -0.0828,  0.0772,  0.0115,
         -0.1045]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.4089, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3994, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3613, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4684, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4436, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3828, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5309, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5138, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4180, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.5705, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5701, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4218, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 0 		  Q(s,a)=  tensor(0.6343, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 1 		  Q(s,a)=  tensor(0.6217, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 2 		  Q(s,a)=  tensor(0.4127, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 0 		  Q(s,a)=  tensor(0.6951, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 1 		  Q(s,a)=  tensor(0.7138, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 2 		  Q(s,a)=  tensor(0.4103, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 0 		  Q(s,a)=  tensor(0.7417, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 1 		  Q(s,a)=  tensor(0.7723, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 2 		  Q(s,a)=  tensor(0.4045, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 0 		  Q(s,a)=  tensor(0.8645, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 1 		  Q(s,a)=  tensor(0.8369, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 2 		  Q(s,a)=  tensor(0.3601, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 0 		  Q(s,a)=  tensor(1.0476, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 1 		  Q(s,a)=  tensor(0.8753, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 2 		  Q(s,a)=  tensor(0.4141, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 0 		  Q(s,a)=  tensor(1.0351, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 1 		  Q(s,a)=  tensor(0.9368, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 2 		  Q(s,a)=  tensor(0.4163, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.02572631578947368  Action:  0  Reward Received:  0
State:  0.13216842105263157  Action:  0  Reward Received:  0
State:  0.23995789473684212  Action:  0  Reward Received:  0
State:  0.3427368421052632  Action:  0  Reward Received:  0
State:  0.4476631578947369  Action:  0  Reward Received:  0
State:  0.5514105263157895  Action:  1  Reward Received:  0
State:  0.4476631578947369  Action:  0  Reward Received:  0
State:  0.5514105263157895  Action:  1  Reward Received:  0
State:  0.4476631578947369  Action:  0  Reward Received:  0

Total Reward Received:  0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.6463, -0.4226, -0.2504, -0.2436, -0.2209,  0.3296, -0.8169, -0.4869,
          0.0320,  0.2208,  0.0383,  0.4865,  0.1956,  0.1393, -0.0877, -0.0846,
          0.1229, -0.1132,  0.0609,  0.2527,  0.2610, -0.1814,  0.4542,  0.1177,
          0.2113],
        [-0.1345, -0.0119,  0.0364, -0.0910,  0.0990, -0.1682,  0.1262,  0.0442,
          0.2174,  0.0589, -0.0287, -0.2076,  0.1205, -0.0617,  0.0694, -0.1284,
          0.0812,  0.1357,  0.1868, -0.1093, -0.0943, -0.0300,  0.1944,  0.1850,
          0.0976],
        [ 0.1237, -0.1629, -0.1717, -0.0809,  0.1969, -0.0647, -0.0451,  0.2330,
         -0.0451, -0.0752,  0.3687,  0.1142,  0.0464, -0.1425,  0.1960, -0.1867,
         -0.0915,  0.2005, -0.3636,  0.4152, -0.0788,  0.0589, -0.3362,  0.1231,
         -0.2111]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.3707, 0.3762, 0.3959], dtype=torch.float64, requires_grad=True)
Starting Training
layer weights:  Parameter containing:
tensor([[-0.0256,  0.1483, -0.1758, -0.0743,  0.0120, -0.0953,  0.0980,  0.0119,
         -0.1443,  0.1737,  0.1044,  0.0911, -0.0586,  0.1388,  0.1759,  0.0837,
          0.1489, -0.1191, -0.1394,  0.0350,  0.0237,  0.0167, -0.0696,  0.0785,
          0.1728],
        [ 0.0513,  0.0968, -0.0651, -0.1025, -0.1370, -0.1233,  0.0504, -0.1112,
         -0.1227, -0.0656, -0.0974, -0.0801,  0.1362,  0.1921, -0.0950, -0.1737,
         -0.0223, -0.0139, -0.0825, -0.0991, -0.0018,  0.1706, -0.1504, -0.1782,
         -0.1477],
        [ 0.1686, -0.1477, -0.1323, -0.1081, -0.1323, -0.0620, -0.1131,  0.0429,
          0.1992, -0.1021,  0.1144, -0.1001,  0.0811,  0.1013, -0.1989, -0.0016,
          0.1503, -0.1626,  0.1206,  0.1578, -0.1686,  0.1143, -0.0233,  0.1026,
         -0.0523]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1819,  0.1914, -0.1266,  0.1693,  0.0490, -0.0715, -0.0125, -0.0246,
         -0.1044,  0.0731, -0.1689,  0.0179,  0.0765, -0.0055, -0.1105, -0.0222,
         -0.0427, -0.0569,  0.0890, -0.1995,  0.1550, -0.0148,  0.0535, -0.0448,
         -0.0173],
        [-0.1594, -0.1892,  0.0265, -0.1750,  0.1263,  0.0511,  0.1685,  0.0488,
          0.1261, -0.1863,  0.0056,  0.0754,  0.1900, -0.1244,  0.0061, -0.1614,
          0.1332, -0.0824,  0.0772,  0.1185,  0.1303,  0.0039, -0.0969, -0.0598,
         -0.0777],
        [ 0.1014,  0.1052,  0.1481, -0.1449, -0.0494,  0.1959, -0.1682, -0.0477,
         -0.0804,  0.1754,  0.0888, -0.0533, -0.0340,  0.1033, -0.1987,  0.0291,
         -0.0033, -0.0832, -0.1728, -0.1470,  0.1571,  0.1447, -0.1983, -0.1483,
          0.1632]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5615, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5188, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4781, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6031, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5384, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5224, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6578, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5738, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5109, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7205, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6060, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5298, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 0 		  Q(s,a)=  tensor(0.7757, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 1 		  Q(s,a)=  tensor(0.6516, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 2 		  Q(s,a)=  tensor(0.4943, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 0 		  Q(s,a)=  tensor(0.8217, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 1 		  Q(s,a)=  tensor(0.7004, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 2 		  Q(s,a)=  tensor(0.4961, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 0 		  Q(s,a)=  tensor(0.9129, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 1 		  Q(s,a)=  tensor(0.6701, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 2 		  Q(s,a)=  tensor(0.5201, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 0 		  Q(s,a)=  tensor(1.0285, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 1 		  Q(s,a)=  tensor(0.7589, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 2 		  Q(s,a)=  tensor(0.4685, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 0 		  Q(s,a)=  tensor(1.1162, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 1 		  Q(s,a)=  tensor(0.7708, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 2 		  Q(s,a)=  tensor(0.4756, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 0 		  Q(s,a)=  tensor(1.1071, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 1 		  Q(s,a)=  tensor(0.8064, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 2 		  Q(s,a)=  tensor(0.4824, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.028673684210526313  Action:  0  Reward Received:  0
State:  0.136  Action:  0  Reward Received:  0
State:  0.23692631578947368  Action:  0  Reward Received:  0
State:  0.3372631578947368  Action:  0  Reward Received:  0
State:  0.4485052631578947  Action:  0  Reward Received:  0
State:  0.5563789473684211  Action:  0  Reward Received:  0
State:  0.6552  Action:  0  Reward Received:  0
State:  0.7624842105263159  Action:  0  Reward Received:  0
State:  0.8611368421052632  Action:  0  Reward Received:  1

Total Reward Received:  1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0763,  0.5154, -0.1911,  0.1215,  0.2762,  0.2963, -0.2059, -0.0363,
         -0.0382, -0.0554, -0.4360, -0.2921,  0.1387,  0.0426, -0.2891, -0.1826,
          0.1091, -0.3259,  0.0934,  0.1572,  0.5375, -0.1690,  0.2861,  0.3020,
          0.0647],
        [-0.0865, -0.2454,  0.0806, -0.0779, -0.0071, -0.1170,  0.0085,  0.1046,
          0.2096, -0.2264, -0.1526,  0.3076, -0.0827, -0.2353,  0.1555, -0.1778,
          0.2471,  0.0098,  0.2070,  0.0362,  0.2285,  0.0595,  0.0789,  0.1686,
         -0.1826],
        [ 0.0696, -0.0341,  0.0053, -0.0620, -0.0841,  0.0544,  0.0426, -0.2073,
          0.0595,  0.1820,  0.2986,  0.0604, -0.0611,  0.1022, -0.1855,  0.4720,
          0.2671, -0.0293, -0.1699, -0.1524, -0.1181,  0.0312, -0.2925, -0.4113,
          0.1387]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5095, 0.5069, 0.5039], dtype=torch.float64, requires_grad=True)
