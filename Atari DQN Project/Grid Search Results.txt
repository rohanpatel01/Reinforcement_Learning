Starting Training
layer weights:  Parameter containing:
tensor([[ 0.0948,  0.1167, -0.0510,  0.1485,  0.0047, -0.0771, -0.0439,  0.0714,
          0.1225, -0.1084,  0.1588, -0.1428,  0.1914,  0.1202,  0.1574, -0.0219,
          0.1194,  0.0676, -0.1966, -0.0099, -0.0035,  0.1602, -0.0976, -0.1926,
         -0.0243],
        [-0.1362,  0.1411, -0.1138,  0.0662, -0.1539, -0.0966,  0.1382, -0.1216,
         -0.1409, -0.1996, -0.0423,  0.0401, -0.0854,  0.0503, -0.0490, -0.1826,
         -0.1207, -0.1661,  0.0718,  0.1950,  0.1424, -0.0920,  0.1065,  0.0757,
         -0.1222],
        [-0.1025,  0.0143,  0.0336, -0.1685,  0.0382, -0.1833, -0.0953, -0.1502,
         -0.1388,  0.0679, -0.0770,  0.1077, -0.1892, -0.0597,  0.1291, -0.0263,
          0.0371,  0.1233,  0.1244,  0.1896,  0.0725, -0.1645, -0.1992, -0.0792,
          0.1853]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.1296, -0.1268,  0.1697,  0.0566,  0.0153, -0.1214, -0.0272,  0.1875,
          0.1103,  0.0711, -0.0173, -0.1193, -0.1676, -0.1806,  0.1068, -0.0435,
         -0.1371,  0.1929,  0.0820,  0.0142,  0.1029, -0.0270, -0.0331,  0.1248,
          0.0603],
        [-0.1607, -0.0099,  0.0953,  0.1210,  0.1165, -0.0889,  0.0405,  0.1499,
         -0.0668, -0.0828,  0.0646, -0.0064,  0.1160, -0.0686, -0.1840, -0.0993,
          0.0694,  0.1129, -0.1794,  0.0941, -0.0933,  0.1675,  0.1558, -0.1281,
          0.0297],
        [ 0.1258,  0.1728, -0.1757,  0.0294,  0.1094,  0.0418, -0.0004,  0.0232,
         -0.0267, -0.0817,  0.1637, -0.0686, -0.1869,  0.1096, -0.0593,  0.1417,
         -0.1225,  0.0811,  0.0765, -0.1312, -0.0047, -0.1728, -0.1179, -0.0545,
          0.1842]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5169, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4330, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4491, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5447, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4907, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5411, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5881, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5404, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6215, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.6218, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6227, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6605, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 0 		  Q(s,a)=  tensor(0.6698, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 1 		  Q(s,a)=  tensor(0.6092, dtype=torch.float64, grad_fn=<SelectBackward0>)
4 		 2 		  Q(s,a)=  tensor(0.7463, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 0 		  Q(s,a)=  tensor(0.7481, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 1 		  Q(s,a)=  tensor(0.6508, dtype=torch.float64, grad_fn=<SelectBackward0>)
5 		 2 		  Q(s,a)=  tensor(0.8137, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 0 		  Q(s,a)=  tensor(0.7454, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 1 		  Q(s,a)=  tensor(0.7105, dtype=torch.float64, grad_fn=<SelectBackward0>)
6 		 2 		  Q(s,a)=  tensor(0.8972, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 0 		  Q(s,a)=  tensor(0.8113, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 1 		  Q(s,a)=  tensor(0.7565, dtype=torch.float64, grad_fn=<SelectBackward0>)
7 		 2 		  Q(s,a)=  tensor(0.9248, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 0 		  Q(s,a)=  tensor(0.9434, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 1 		  Q(s,a)=  tensor(0.7914, dtype=torch.float64, grad_fn=<SelectBackward0>)
8 		 2 		  Q(s,a)=  tensor(0.9432, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 0 		  Q(s,a)=  tensor(0.9093, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 1 		  Q(s,a)=  tensor(0.8419, dtype=torch.float64, grad_fn=<SelectBackward0>)
9 		 2 		  Q(s,a)=  tensor(1.0672, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  0.04618181818181819  Action:  0  Reward Received:  0
State:  0.22967272727272728  Action:  0  Reward Received:  0
State:  0.41643636363636366  Action:  2  Reward Received:  0
State:  0.41643636363636366  Action:  2  Reward Received:  0
State:  0.41643636363636366  Action:  2  Reward Received:  0
State:  0.41643636363636366  Action:  2  Reward Received:  0
State:  0.41643636363636366  Action:  2  Reward Received:  0
State:  0.41643636363636366  Action:  2  Reward Received:  0
State:  0.41643636363636366  Action:  2  Reward Received:  0

Total Reward Received:  0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0101, -0.1048,  0.1416,  0.0415,  0.0854, -0.0792, -0.0105, -0.0710,
          0.1586,  0.2629, -0.2390, -0.2320, -0.1114, -0.1639,  0.1896, -0.2600,
          0.0999,  0.0514, -0.1048,  0.1282,  0.0206,  0.0754,  0.0972,  0.3574,
         -0.0588],
        [-0.0633, -0.1420,  0.0681,  0.1950,  0.1287, -0.0404, -0.0885,  0.1087,
          0.0220, -0.1996,  0.0561,  0.0223,  0.2138, -0.0267, -0.1275, -0.0265,
         -0.0764,  0.2209, -0.1427,  0.1582, -0.1600,  0.2004,  0.0533, -0.2044,
          0.0888],
        [ 0.0392,  0.2900, -0.2248, -0.0604, -0.0322, -0.0472, -0.0009,  0.0392,
         -0.0025,  0.0285,  0.1614,  0.0712, -0.0758,  0.0078, -0.1346,  0.2652,
         -0.0676,  0.1093,  0.1554, -0.0602,  0.0183, -0.2136, -0.2008,  0.0512,
          0.2493]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.4781, 0.4351, 0.4554], dtype=torch.float64, requires_grad=True)
