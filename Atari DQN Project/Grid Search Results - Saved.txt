Starting Training
layer weights:  Parameter containing:
tensor([[-0.7381],
        [-0.9970],
        [ 0.0802],
        [-0.9681],
        [ 0.4016]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6687],
        [ 0.0517],
        [-0.0816],
        [-0.9051],
        [ 0.1382]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(2.9989, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2688, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(3.2905, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.9706, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.9853, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(2.9785, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.3708, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(2.3696, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.4297, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(2.1565, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(2.9581, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(3.4728, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.4487, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.8889, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(2.3276, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(2.9378, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(4.5748, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5278, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(3.3480, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.4988, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0

Total Reward Received:  4.1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0204],
        [ 1.1020],
        [-0.9209],
        [ 0.4592],
        [ 0.1712]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([2.9989, 1.2688, 3.2905, 1.9706, 1.9853], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1000
Num nsteps_train %:  0.5

layer weights:  Parameter containing:
tensor([[ 0.2529],
        [-0.0261],
        [ 0.0287],
        [ 0.0748],
        [-0.0914]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.2638],
        [ 0.6374],
        [-0.2914],
        [-0.8184],
        [ 0.9371]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6200, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3754, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3749, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.2627, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5889, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4469, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4226, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3490, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.2562, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4326, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2738, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.4699, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3232, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.2498, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2762, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1007, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5171, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2974, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.2433, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1199, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1731],
        [ 0.0472],
        [-0.0258],
        [-0.0065],
        [-0.1563]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6200, 0.3754, 0.3749, 0.2627, 0.5889], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1000
Num nsteps_train %:  1.0

layer weights:  Parameter containing:
tensor([[ 0.9782],
        [-0.3389],
        [-0.7932],
        [-0.4983],
        [-0.0732]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9856],
        [-0.1209],
        [ 0.4189],
        [-0.7325],
        [ 0.4539]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5346, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2882, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3503, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.2976, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5271, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4648, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2807, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3107, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.3701, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4067, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3951, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.2733, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.2711, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4425, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2863, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3253, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.2658, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2315, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5150, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1659, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0698],
        [-0.0075],
        [-0.0396],
        [ 0.0725],
        [-0.1204]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5346, 0.2882, 0.3503, 0.2976, 0.5271], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1000
Num nsteps_train %:  1.5

layer weights:  Parameter containing:
tensor([[ 0.0247],
        [-0.0994],
        [-0.6703],
        [ 0.6259],
        [ 0.0613]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1327],
        [ 0.1549],
        [-0.8983],
        [ 0.1753],
        [-0.1346]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5159, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1941, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.9066, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5608, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5278, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.3939, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6486, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.6153, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6555, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.6512, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.2719, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.1030, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.3241, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.7503, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.7747, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.1499, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.5575, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.0328, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.8450, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.8982, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.6
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1220],
        [ 0.4545],
        [-0.2913],
        [ 0.0947],
        [ 0.1235]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5159, 1.1941, 1.9066, 1.5608, 1.5278], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1000
Num nsteps_train %:  2.0

layer weights:  Parameter containing:
tensor([[-0.1871],
        [ 0.3478],
        [ 0.6047],
        [-0.4810],
        [-0.7837]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0893],
        [-0.1860],
        [-0.6363],
        [-0.5732],
        [ 0.5130]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3583, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2696, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5760, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3463, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3930, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.1806, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.4012, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3495, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4200, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2508, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.0029, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.5328, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.1231, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.4937, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.1087, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.8251, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.6644, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.8967, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.5673, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.9665, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.6
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1777],
        [ 0.1316],
        [-0.2264],
        [ 0.0737],
        [-0.1422]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3583, 1.2696, 1.5760, 1.3463, 1.3930], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1000
Num nsteps_train %:  2.5

layer weights:  Parameter containing:
tensor([[-0.1004],
        [ 0.6208],
        [-0.4199],
        [ 0.1060],
        [-0.7578]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.3901],
        [ 0.7942],
        [-0.2011],
        [ 0.4405],
        [ 0.7229]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6903, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4953, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6687, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6296, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6917, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5287, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5961, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5886, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6270, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6183, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3672, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6970, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5085, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6244, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5450, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2056, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7978, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4284, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6218, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4716, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1616],
        [ 0.1008],
        [-0.0801],
        [-0.0026],
        [-0.0734]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6903, 0.4953, 0.6687, 0.6296, 0.6917], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1000
Num nsteps_train %:  3.0

layer weights:  Parameter containing:
tensor([[ 0.2677],
        [-0.8374],
        [ 0.9996],
        [-0.6033],
        [-0.9172]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2142],
        [ 0.8071],
        [-0.3736],
        [-0.7796],
        [ 0.5643]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7566, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6294, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.8653, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8036, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7180, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5832, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.8089, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.7962, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.8873, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6538, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4098, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.9884, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7272, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.9709, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5895, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2365, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.1680, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6581, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.0545, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5253, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.6
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1734],
        [ 0.1795],
        [-0.0691],
        [ 0.0836],
        [-0.0642]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7566, 0.6294, 0.8653, 0.8036, 0.7180], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1000
Num nsteps_train %:  3.5

layer weights:  Parameter containing:
tensor([[ 0.2283],
        [-0.0563],
        [-0.7667],
        [-0.4141],
        [ 0.9892]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0178],
        [-0.5843],
        [ 0.8587],
        [-0.1716],
        [ 0.1245]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6893, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4509, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4996, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5412, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6847, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4992, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5181, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4413, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5615, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5388, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3090, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5854, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3830, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5818, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3928, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1189, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6526, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3248, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6021, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2469, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1901],
        [ 0.0672],
        [-0.0583],
        [ 0.0203],
        [-0.1459]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6893, 0.4509, 0.4996, 0.5412, 0.6847], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1000
Num nsteps_train %:  4.0

layer weights:  Parameter containing:
tensor([[-0.1775],
        [-0.2516],
        [ 0.1943],
        [-0.2893],
        [ 0.8387]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3338],
        [-0.9636],
        [ 0.4656],
        [-0.0977],
        [-0.4281]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4537, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2405, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5485, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4684, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3391, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.2902, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.4844, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4549, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5351, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3126, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.1268, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.7283, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.3613, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.6017, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.2862, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.9633, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.9721, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.2677, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.6684, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.2597, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.6
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1635],
        [ 0.2439],
        [-0.0936],
        [ 0.0667],
        [-0.0265]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4537, 1.2405, 1.5485, 1.4684, 1.3391], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1000
Num nsteps_train %:  4.5

layer weights:  Parameter containing:
tensor([[-0.3969],
        [-0.0530],
        [-0.7007],
        [-0.7916],
        [-0.1481]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.7629],
        [-0.9013],
        [-0.0314],
        [-0.9103],
        [-0.9307]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.8216, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6854, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.8176, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8009, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7486, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7228, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7683, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.7956, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.8432, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6762, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6241, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.8512, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7736, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.8854, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6039, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.5253, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.9341, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7515, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.9276, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5315, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0988],
        [ 0.0829],
        [-0.0220],
        [ 0.0422],
        [-0.0724]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.8216, 0.6854, 0.8176, 0.8009, 0.7486], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1000
Num nsteps_train %:  5.0

layer weights:  Parameter containing:
tensor([[-0.3997],
        [ 0.2727],
        [ 0.9246],
        [-0.4971],
        [-0.3685]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4052],
        [-0.2974],
        [-0.4833],
        [-0.9207],
        [-0.1844]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4912, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0091, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.9497, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5910, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5288, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.4665, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5676, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.5640, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6233, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4686, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.4418, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.1261, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.1784, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.6555, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.4083, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.4171, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.6845, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7927, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.6878, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.3480, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.6
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0247],
        [ 0.5585],
        [-0.3857],
        [ 0.0323],
        [-0.0603]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4912, 1.0091, 1.9497, 1.5910, 1.5288], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1000
Num nsteps_train %:  2.0

layer weights:  Parameter containing:
tensor([[ 0.8045],
        [ 0.7079],
        [ 0.5715],
        [-0.6817],
        [ 0.2694]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8235],
        [ 0.4252],
        [ 0.0670],
        [-0.1038],
        [ 0.6962]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6728, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5769, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5912, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6446, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6887, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5632, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6230, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5866, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6317, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5864, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4536, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6691, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5820, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6188, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4840, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3441, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7152, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5773, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6059, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3817, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1096],
        [ 0.0461],
        [-0.0046],
        [-0.0129],
        [-0.1023]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6728, 0.5769, 0.5912, 0.6446, 0.6887], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1000
Num nsteps_train %:  3.0

layer weights:  Parameter containing:
tensor([[ 0.1992],
        [ 0.1566],
        [-0.1405],
        [-0.5503],
        [-0.4213]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.5226],
        [-0.6492],
        [-0.8680],
        [-0.5966],
        [ 0.2160]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6024, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4922, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5246, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5526, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6109, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4792, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5382, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5128, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5403, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5662, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3561, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5841, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5010, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5280, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.5214, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2329, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6300, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4892, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5157, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4767, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1232],
        [ 0.0459],
        [-0.0118],
        [-0.0123],
        [-0.0448]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6024, 0.4922, 0.5246, 0.5526, 0.6109], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1000
Num nsteps_train %:  4.0

layer weights:  Parameter containing:
tensor([[ 0.9076],
        [ 0.5670],
        [ 0.4294],
        [ 0.8387],
        [-0.7277]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6537],
        [-0.6651],
        [-0.7921],
        [-0.4906],
        [ 0.7033]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5653, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2274, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.2727, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.2717, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5237, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4885, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2731, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.2753, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.2595, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4254, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4117, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3188, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.2779, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.2473, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3272, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3349, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3645, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2805, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.2351, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2289, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0768],
        [ 0.0457],
        [ 0.0026],
        [-0.0122],
        [-0.0983]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5653, 0.2274, 0.2727, 0.2717, 0.5237], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1500
Num nsteps_train %:  0.5

layer weights:  Parameter containing:
tensor([[-0.4425],
        [ 0.9838],
        [-0.1287],
        [-0.7414],
        [ 0.3439]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5269],
        [-0.3987],
        [-0.7365],
        [-0.8889],
        [-0.4506]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5412, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0462, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7929, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4338, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3992, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.3929, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.4308, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4382, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5636, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2928, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.2446, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.8153, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0835, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.6935, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.1864, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.0963, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.1998, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7288, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.8233, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.0801, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.6
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1483],
        [ 0.3845],
        [-0.3547],
        [ 0.1298],
        [-0.1064]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5412, 1.0462, 1.7929, 1.4338, 1.3992], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1500
Num nsteps_train %:  1.0

layer weights:  Parameter containing:
tensor([[-0.6129],
        [ 0.7250],
        [ 0.3343],
        [-0.4361],
        [-0.5706]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8789],
        [-0.8845],
        [-0.5115],
        [ 0.5587],
        [-0.0100]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5227, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3929, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4500, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5325, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5052, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3667, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4983, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4678, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5686, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4722, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2108, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6036, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4856, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6047, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4392, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.0548, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7089, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5034, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6408, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4062, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  -0.7000000000000001
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1559],
        [ 0.1053],
        [ 0.0178],
        [ 0.0361],
        [-0.0330]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5227, 0.3929, 0.4500, 0.5325, 0.5052], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1500
Num nsteps_train %:  1.5

layer weights:  Parameter containing:
tensor([[ 0.6912],
        [-0.5628],
        [-0.0481],
        [-0.2505],
        [ 0.3300]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.4581],
        [-0.5868],
        [ 0.7601],
        [ 0.8052],
        [-0.1575]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2354, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.9418, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3537, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1228, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.1302, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0823, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.0710, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2008, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.2349, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0738, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.9293, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.2002, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0479, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.3469, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0174, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7762, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.3293, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.8950, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.4590, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.9611, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  3  Reward Received:  1.0
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  0.7
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1531],
        [ 0.1292],
        [-0.1529],
        [ 0.1120],
        [-0.0564]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2354, 0.9418, 1.3537, 1.1228, 1.1302], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1500
Num nsteps_train %:  2.0

layer weights:  Parameter containing:
tensor([[-0.3290],
        [ 0.3400],
        [ 0.8163],
        [ 0.2976],
        [-0.7704]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9508],
        [ 0.4951],
        [-0.5040],
        [-0.7831],
        [ 0.8055]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3089, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.9806, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.4780, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1694, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.2352, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.1584, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.3341, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2916, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.2483, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.1626, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.0080, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.6875, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.1052, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.3271, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0900, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.8576, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.0409, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.9189, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.4060, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.0173, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1504],
        [ 0.3534],
        [-0.1864],
        [ 0.0788],
        [-0.0726]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3089, 0.9806, 1.4780, 1.1694, 1.2352], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1500
Num nsteps_train %:  2.5

layer weights:  Parameter containing:
tensor([[ 0.3082],
        [-0.0031],
        [-0.5340],
        [-0.8696],
        [ 0.7295]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.7735],
        [-0.7079],
        [-0.4819],
        [ 0.3655],
        [ 0.7597]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5972, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4624, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5275, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4678, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5769, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4820, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4966, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5238, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4471, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4671, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3669, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5308, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5202, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.4264, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3574, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2517, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.5650, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5165, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.4057, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2476, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1152],
        [ 0.0342],
        [-0.0037],
        [-0.0207],
        [-0.1098]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5972, 0.4624, 0.5275, 0.4678, 0.5769], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1500
Num nsteps_train %:  3.0

layer weights:  Parameter containing:
tensor([[-0.8687],
        [-0.0217],
        [ 0.4479],
        [ 0.2573],
        [-0.6079]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9140],
        [ 0.7000],
        [-0.6678],
        [ 0.9634],
        [ 0.5978]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.8691, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.7983, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.8985, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.8879, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8722, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7257, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.9133, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.8102, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.9143, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.8179, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5823, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.0283, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.7218, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.9407, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.7637, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.4390, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.1433, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6334, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.9671, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.7095, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.6
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1434],
        [ 0.1150],
        [-0.0884],
        [ 0.0264],
        [-0.0542]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.8691, 0.7983, 0.8985, 0.8879, 0.8722], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1500
Num nsteps_train %:  3.5

layer weights:  Parameter containing:
tensor([[-0.0989],
        [-0.8898],
        [-0.8534],
        [-0.4918],
        [ 0.1796]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.3243],
        [-0.9530],
        [-0.6918],
        [ 0.2079],
        [-0.4359]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5819, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4707, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5705, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5925, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6231, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4723, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5866, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5330, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6036, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5325, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3627, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7026, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4955, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6146, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4419, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2530, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.8185, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4581, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6256, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3512, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1096],
        [ 0.1160],
        [-0.0375],
        [ 0.0110],
        [-0.0906]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5819, 0.4707, 0.5705, 0.5925, 0.6231], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1500
Num nsteps_train %:  4.0

layer weights:  Parameter containing:
tensor([[ 0.2131],
        [ 0.5811],
        [ 0.2024],
        [ 0.0028],
        [-0.3657]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2185],
        [ 0.1832],
        [-0.7818],
        [ 0.3497],
        [ 0.8325]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7229, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5900, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6860, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6788, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6549, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.6297, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6652, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6563, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6751, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5690, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5366, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7405, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6266, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6714, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4831, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.4434, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.8157, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5969, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6678, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3973, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0932],
        [ 0.0752],
        [-0.0297],
        [-0.0037],
        [-0.0859]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7229, 0.5900, 0.6860, 0.6788, 0.6549], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1500
Num nsteps_train %:  4.5

layer weights:  Parameter containing:
tensor([[-0.2295],
        [ 0.4149],
        [-0.6959],
        [-0.6758],
        [ 0.6250]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.3362],
        [-0.2807],
        [-0.7660],
        [-0.3091],
        [-0.1283]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.9760, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.9646, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0060, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.0365, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0063, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8200, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.0590, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.9267, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.0522, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.9705, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.6640, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.1534, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.8473, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.0678, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.9348, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.5080, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.2478, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7680, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.0835, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.8990, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  -0.9
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1560],
        [ 0.0944],
        [-0.0793],
        [ 0.0156],
        [-0.0358]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.9760, 0.9646, 1.0060, 1.0365, 1.0063], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1500
Num nsteps_train %:  5.0

layer weights:  Parameter containing:
tensor([[ 0.4402],
        [-0.6686],
        [ 0.3037],
        [-0.0615],
        [ 0.9150]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6710],
        [ 0.2608],
        [ 0.0217],
        [ 0.6261],
        [ 0.5827]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2990, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0900, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.5775, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2729, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3749, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.1892, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.4307, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3953, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4109, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.3267, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.0794, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.7714, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2131, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.5488, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.2785, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.9697, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.1121, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.0309, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.6868, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.2303, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1098],
        [ 0.3407],
        [-0.1822],
        [ 0.1380],
        [-0.0482]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2990, 1.0900, 1.5775, 1.2729, 1.3749], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1500
Num nsteps_train %:  2.0

layer weights:  Parameter containing:
tensor([[-0.6636],
        [-0.9896],
        [-0.3411],
        [-0.8211],
        [-0.9214]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.8351],
        [-0.8919],
        [-0.6220],
        [ 0.5485],
        [ 0.7609]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6354, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5641, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5951, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5861, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6273, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5130, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6330, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5068, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5826, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5218, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3906, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7020, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4186, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5791, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4164, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2682, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7710, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3303, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5756, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3109, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1224],
        [ 0.0690],
        [-0.0883],
        [-0.0035],
        [-0.1055]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6354, 0.5641, 0.5951, 0.5861, 0.6273], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1500
Num nsteps_train %:  3.0

layer weights:  Parameter containing:
tensor([[-0.6086],
        [-0.5502],
        [ 0.1718],
        [-0.6290],
        [ 0.3771]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7639],
        [-0.5370],
        [ 0.8062],
        [ 0.7395],
        [-0.1137]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7572, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6208, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.7434, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7257, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7528, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5947, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7130, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6744, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.7540, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6861, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4323, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.8052, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6054, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.7823, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6194, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2698, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.8974, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5363, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.8105, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.5526, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1625],
        [ 0.0922],
        [-0.0690],
        [ 0.0283],
        [-0.0667]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7572, 0.6208, 0.7434, 0.7257, 0.7528], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  1500
Num nsteps_train %:  4.0

layer weights:  Parameter containing:
tensor([[-0.9658],
        [-0.4678],
        [ 0.1174],
        [ 0.7199],
        [-0.8295]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8877],
        [ 0.1103],
        [-0.7048],
        [ 0.2451],
        [-0.9341]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5203, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.2352, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.2561, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.2194, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5113, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4340, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2011, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.2384, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.2392, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4004, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3476, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.1670, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.2207, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.2590, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.2894, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2613, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.1330, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2030, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.2789, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.1784, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0863],
        [-0.0341],
        [-0.0177],
        [ 0.0198],
        [-0.1110]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5203, 0.2352, 0.2561, 0.2194, 0.5113], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2000
Num nsteps_train %:  0.5

layer weights:  Parameter containing:
tensor([[-0.9828],
        [ 0.8800],
        [ 0.8942],
        [ 0.1601],
        [-0.6760]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4093],
        [-0.4568],
        [ 0.2867],
        [-0.9617],
        [ 0.4161]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5196, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3944, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3988, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4062, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5417, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4226, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4861, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3723, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.3961, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4661, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3256, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5778, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3457, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.3860, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3904, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2286, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6694, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3191, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.3759, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3147, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0970],
        [ 0.0917],
        [-0.0266],
        [-0.0101],
        [-0.0757]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5196, 0.3944, 0.3988, 0.4062, 0.5417], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2000
Num nsteps_train %:  1.0

layer weights:  Parameter containing:
tensor([[ 0.3591],
        [-0.2486],
        [-0.0233],
        [-0.2230],
        [-0.7825]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8338],
        [ 0.7066],
        [ 0.2956],
        [-0.1391],
        [-0.4720]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6740, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1956, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7723, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3972, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.5465, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.4485, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6199, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4114, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5667, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4505, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.2231, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.0441, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0506, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.7363, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.3545, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.9976, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.4684, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6898, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.9059, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.2585, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2254],
        [ 0.4242],
        [-0.3608],
        [ 0.1696],
        [-0.0960]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6740, 1.1956, 1.7723, 1.3972, 1.5465], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2000
Num nsteps_train %:  1.5

layer weights:  Parameter containing:
tensor([[-0.4093],
        [ 0.5215],
        [-0.0193],
        [-0.7789],
        [-0.0916]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8649],
        [ 0.2659],
        [ 0.0277],
        [-0.9531],
        [ 0.2276]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.8661, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3719, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.0294, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5693, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.8853, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.6410, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.8339, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.6189, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.7045, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.7451, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.4159, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.2960, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2084, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.8398, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.6048, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.1908, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.7580, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7978, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.9750, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.4646, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2251],
        [ 0.4621],
        [-0.4105],
        [ 0.1352],
        [-0.1402]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.8661, 1.3719, 2.0294, 1.5693, 1.8853], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2000
Num nsteps_train %:  2.0

layer weights:  Parameter containing:
tensor([[ 0.4554],
        [ 0.0116],
        [-0.1727],
        [ 0.9435],
        [-0.9724]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6507],
        [-0.8473],
        [-0.8640],
        [-0.9208],
        [ 0.9491]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5466, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3971, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4566, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4775, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5643, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3637, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4974, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4029, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5037, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4541, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.1808, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5977, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3492, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5299, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3439, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(-0.0021, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6980, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2956, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5560, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2338, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1829],
        [ 0.1003],
        [-0.0537],
        [ 0.0262],
        [-0.1102]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5466, 0.3971, 0.4566, 0.4775, 0.5643], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2000
Num nsteps_train %:  2.5

layer weights:  Parameter containing:
tensor([[-0.4984],
        [ 0.9925],
        [ 0.9999],
        [ 0.5025],
        [-0.0210]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.4961],
        [ 0.8877],
        [ 0.4029],
        [-0.3127],
        [ 0.2369]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6451, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5519, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6977, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.6384, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6333, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4710, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6492, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.6612, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6347, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5639, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.2970, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7464, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6246, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6310, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4944, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1229, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.8436, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5881, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6273, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4249, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1741],
        [ 0.0972],
        [-0.0366],
        [-0.0037],
        [-0.0695]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6451, 0.5519, 0.6977, 0.6384, 0.6333], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2000
Num nsteps_train %:  3.0

layer weights:  Parameter containing:
tensor([[ 0.1628],
        [-0.7185],
        [ 0.6193],
        [ 0.6268],
        [-0.6860]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5842],
        [ 0.6982],
        [ 0.8164],
        [ 0.7663],
        [ 0.7597]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7221, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.6360, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.7788, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.7430, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7463, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5584, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.7558, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.7190, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.8043, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.7055, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3947, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.8756, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6591, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.8657, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.6647, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2310, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.9954, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5993, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.9270, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.6239, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.6
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1637],
        [ 0.1198],
        [-0.0599],
        [ 0.0613],
        [-0.0408]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7221, 0.6360, 0.7788, 0.7430, 0.7463], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2000
Num nsteps_train %:  3.5

layer weights:  Parameter containing:
tensor([[-0.0690],
        [ 0.4285],
        [-0.0244],
        [ 0.9251],
        [-0.6348]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4561],
        [-0.1115],
        [-0.7550],
        [-0.5650],
        [ 0.5075]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6575, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5417, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6001, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5398, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6536, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5395, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.6229, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5768, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5534, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5706, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4215, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.7041, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.5535, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5670, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4877, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3035, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.7853, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5302, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5805, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4047, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1180],
        [ 0.0812],
        [-0.0233],
        [ 0.0136],
        [-0.0830]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6575, 0.5417, 0.6001, 0.5398, 0.6536], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2000
Num nsteps_train %:  4.0

layer weights:  Parameter containing:
tensor([[-0.9727],
        [-0.6054],
        [-0.8974],
        [ 0.8673],
        [ 0.2094]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[0.0479],
        [0.6953],
        [0.8380],
        [0.6475],
        [0.8835]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.8990, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.8562, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.0365, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.9954, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.8781, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.7202, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.0155, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.9499, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.0683, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.8675, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5415, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.1747, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.8632, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.1412, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.8570, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3627, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.3340, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7766, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.2140, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.8464, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.6
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1788],
        [ 0.1593],
        [-0.0867],
        [ 0.0729],
        [-0.0106]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.8990, 0.8562, 1.0365, 0.9954, 0.8781], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2000
Num nsteps_train %:  4.5

layer weights:  Parameter containing:
tensor([[ 0.0453],
        [ 0.2422],
        [-0.6157],
        [-0.8867],
        [-0.8439]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4987],
        [ 0.8702],
        [-0.9261],
        [ 0.0096],
        [ 0.5939]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.7256, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5220, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.6038, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5924, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.7686, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5695, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5776, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5206, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.6064, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.6266, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4134, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.6332, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4374, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.6204, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4846, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2573, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6888, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3542, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.6344, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3425, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1561],
        [ 0.0556],
        [-0.0832],
        [ 0.0140],
        [-0.1420]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.7256, 0.5220, 0.6038, 0.5924, 0.7686], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2000
Num nsteps_train %:  5.0

layer weights:  Parameter containing:
tensor([[-0.3266],
        [ 0.7969],
        [ 0.1705],
        [-0.1269],
        [ 0.7261]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.4236],
        [ 0.2007],
        [ 0.5675],
        [ 0.0166],
        [-0.6145]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.2944, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1626, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7569, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4848, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4553, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.1548, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5428, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.5064, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5150, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4364, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.0152, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.9231, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2559, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.5452, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.4175, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.8756, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.3034, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.0053, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.5754, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.3987, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1396],
        [ 0.3803],
        [-0.2505],
        [ 0.0302],
        [-0.0189]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.2944, 1.1626, 1.7569, 1.4848, 1.4553], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2000
Num nsteps_train %:  2.0

layer weights:  Parameter containing:
tensor([[ 0.8639],
        [ 0.1476],
        [ 0.7205],
        [-0.5051],
        [-0.7578]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6783],
        [-0.8800],
        [ 0.7251],
        [-0.7860],
        [ 0.0264]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5796, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3476, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3647, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.3970, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5667, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4526, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.4390, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3065, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.3907, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.4513, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3257, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5305, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.2483, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.3844, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.3358, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.1988, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6219, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.1901, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.3780, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2204, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1269],
        [ 0.0914],
        [-0.0582],
        [-0.0063],
        [-0.1155]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5796, 0.3476, 0.3647, 0.3970, 0.5667], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2000
Num nsteps_train %:  3.0

layer weights:  Parameter containing:
tensor([[-0.2843],
        [ 0.2807],
        [ 0.7310],
        [-0.6553],
        [ 0.6444]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1611],
        [-0.3794],
        [ 0.7099],
        [ 0.0955],
        [-0.9754]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.3999, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1395, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6618, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5162, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3257, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.1859, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.4805, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4806, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5032, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2895, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.9718, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.8216, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2993, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.4903, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.2533, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7578, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.1627, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.1181, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.4773, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.2171, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.6
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2140],
        [ 0.3411],
        [-0.1812],
        [-0.0129],
        [-0.0362]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.3999, 1.1395, 1.6618, 1.5162, 1.3257], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2000
Num nsteps_train %:  4.0

layer weights:  Parameter containing:
tensor([[ 0.2374],
        [-0.8849],
        [-0.3419],
        [ 0.3426],
        [ 0.6337]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.4685],
        [ 0.6559],
        [ 0.0874],
        [ 0.0392],
        [-0.1092]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5247, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(-0.0476, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.0355, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(-0.0622, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5215, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.0956, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(-0.0048, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.0105, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(-0.0735, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(-0.2006, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(-0.3335, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.0379, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(-0.0145, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(-0.0849, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(-0.9228, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(-0.7626, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.0807, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(-0.0395, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(-0.0962, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(-1.6450, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.4291],
        [ 0.0428],
        [-0.0250],
        [-0.0114],
        [-0.7222]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([ 0.5247, -0.0476,  0.0355, -0.0622,  0.5215], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2500
Num nsteps_train %:  0.5

layer weights:  Parameter containing:
tensor([[ 0.9019],
        [-0.4400],
        [-0.7723],
        [ 0.1797],
        [-0.7862]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.9965],
        [ 0.3188],
        [-0.8528],
        [ 0.9672],
        [ 0.0103]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5239, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.1512, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3178, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.2114, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5271, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.4271, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.1736, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3392, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.2020, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.2179, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.3303, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.1961, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3606, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.1927, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(-0.0912, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.2336, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.2186, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3820, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.1833, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(-0.4004, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0968],
        [ 0.0225],
        [ 0.0214],
        [-0.0093],
        [-0.3091]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5239, 0.1512, 0.3178, 0.2114, 0.5271], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2500
Num nsteps_train %:  1.0

layer weights:  Parameter containing:
tensor([[-0.3388],
        [ 0.9527],
        [ 0.7305],
        [-0.1743],
        [ 0.5772]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0157],
        [-0.0221],
        [-0.4652],
        [-0.6799],
        [ 0.9322]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5417, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3048, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3027, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.2558, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5682, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3213, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.2836, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.2712, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.2031, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.2558, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.1008, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.2624, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.2397, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.1503, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(-0.0566, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(-0.1196, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.2411, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2082, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.0976, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(-0.3690, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2204],
        [-0.0212],
        [-0.0315],
        [-0.0528],
        [-0.3124]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5417, 0.3048, 0.3027, 0.2558, 0.5682], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2500
Num nsteps_train %:  1.5

layer weights:  Parameter containing:
tensor([[ 0.1854],
        [ 0.6161],
        [-0.8554],
        [-0.1802],
        [-0.4746]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7664],
        [-0.1117],
        [-0.1016],
        [ 0.8860],
        [-0.3068]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.7124, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0713, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.0050, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5221, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.7244, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.6347, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5766, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4446, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5475, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.5372, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.5569, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.0818, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.8842, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.5728, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.3500, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.4792, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.5871, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3237, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.5982, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.1628, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0

Total Reward Received:  4.1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0777],
        [ 0.5053],
        [-0.5604],
        [ 0.0253],
        [-0.1872]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.7124, 1.0713, 2.0050, 1.5221, 1.7244], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2500
Num nsteps_train %:  2.0

layer weights:  Parameter containing:
tensor([[ 0.7904],
        [-0.3355],
        [-0.2322],
        [ 0.1276],
        [-0.4425]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6981],
        [-0.3858],
        [-0.6090],
        [-0.9654],
        [ 0.3196]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6774, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0950, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7063, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.2902, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4232, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.4923, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.4905, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4774, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.3892, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2826, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.3071, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.8860, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2486, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.4882, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.1421, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.1219, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.2816, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.0197, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.5871, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.0015, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0

Total Reward Received:  4.1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1852],
        [ 0.3955],
        [-0.2289],
        [ 0.0990],
        [-0.1406]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6774, 1.0950, 1.7063, 1.2902, 1.4232], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2500
Num nsteps_train %:  2.5

layer weights:  Parameter containing:
tensor([[-0.9845],
        [-0.8611],
        [-0.5065],
        [-0.2144],
        [ 0.1326]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1923],
        [-0.6683],
        [-0.2517],
        [ 0.7011],
        [ 0.1197]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5887, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.1428, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8462, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5153, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3523, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.3170, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5917, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.5991, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4905, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4074, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.0454, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.0406, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.3521, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.4657, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.4626, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7737, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.4895, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.1050, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.4409, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.5177, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2717],
        [ 0.4489],
        [-0.2471],
        [-0.0248],
        [ 0.0551]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5887, 1.1428, 1.8462, 1.5153, 1.3523], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2500
Num nsteps_train %:  3.0

layer weights:  Parameter containing:
tensor([[ 0.9318],
        [-0.4558],
        [ 0.6711],
        [ 0.0767],
        [-0.0826]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.2076],
        [ 0.5719],
        [ 0.8119],
        [ 0.5406],
        [-0.0421]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.0566, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.8749, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.1827, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.0375, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0068, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.8947, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.0669, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.0471, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.0364, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.9613, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.7327, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.2588, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.9114, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.0354, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.9158, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.5708, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.4507, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.7757, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.0343, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.8703, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1619],
        [ 0.1919],
        [-0.1357],
        [-0.0010],
        [-0.0455]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.0566, 0.8749, 1.1827, 1.0375, 1.0068], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2500
Num nsteps_train %:  3.5

layer weights:  Parameter containing:
tensor([[ 0.2117],
        [ 0.4710],
        [-0.6061],
        [ 0.5882],
        [ 0.8439]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.8405],
        [-0.8978],
        [ 0.4383],
        [-0.8959],
        [ 0.7358]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6294, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.3461, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(2.0622, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5770, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6312, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.3294, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.7794, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.8611, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.8500, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.7067, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.0293, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.2127, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.6601, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(2.1230, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.7821, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7293, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.6460, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.4591, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.3960, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.8576, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.3000],
        [ 0.4333],
        [-0.2010],
        [ 0.2730],
        [ 0.0755]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6294, 1.3461, 2.0622, 1.5770, 1.6312], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2500
Num nsteps_train %:  4.0

layer weights:  Parameter containing:
tensor([[-0.6931],
        [-0.0961],
        [-0.1103],
        [-0.8628],
        [-0.6194]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6313],
        [ 0.2772],
        [ 0.3811],
        [ 0.5894],
        [-0.0343]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6559, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.5104, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.5556, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.5437, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6440, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5517, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5408, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.5220, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.5476, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5267, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.4475, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5712, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.4885, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5514, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4093, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.3434, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6015, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4550, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5553, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.2919, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1042],
        [ 0.0304],
        [-0.0335],
        [ 0.0039],
        [-0.1174]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6559, 0.5104, 0.5556, 0.5437, 0.6440], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2500
Num nsteps_train %:  4.5

layer weights:  Parameter containing:
tensor([[ 0.2839],
        [ 0.5117],
        [ 0.1878],
        [ 0.6164],
        [-0.9812]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3776],
        [-0.1081],
        [ 0.7990],
        [ 0.3783],
        [ 0.2015]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.6916, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.4675, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.4854, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.4704, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.6657, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.5972, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.5238, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.4426, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.4868, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.5442, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.5028, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.5800, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.3998, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.5033, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.4226, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.4084, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.6363, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.3570, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.5197, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.3011, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0944],
        [ 0.0562],
        [-0.0428],
        [ 0.0165],
        [-0.1215]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.6916, 0.4675, 0.4854, 0.4704, 0.6657], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2500
Num nsteps_train %:  5.0

layer weights:  Parameter containing:
tensor([[-0.6802],
        [ 0.3348],
        [-0.5819],
        [ 0.7567],
        [-0.0860]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.8620],
        [ 0.6451],
        [-0.2449],
        [ 0.3436],
        [-0.2484]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6681, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0493, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.9534, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3298, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.6380, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.6418, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5725, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4604, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6075, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2272, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.6154, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(2.0958, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.9673, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.8851, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(0.8163, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.5891, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.6190, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4742, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(2.1627, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.4055, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0

Total Reward Received:  4.1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.0263],
        [ 0.5232],
        [-0.4931],
        [ 0.2776],
        [-0.4108]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6681, 1.0493, 1.9534, 1.3298, 1.6380], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2500
Num nsteps_train %:  2.0

layer weights:  Parameter containing:
tensor([[-0.7576],
        [-0.8836],
        [-0.5145],
        [ 0.1889],
        [ 0.1691]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9264],
        [-0.2781],
        [ 0.5793],
        [ 0.8083],
        [-0.2064]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6221, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2228, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.9018, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4740, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3824, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.4816, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6050, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.5538, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6314, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4407, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.3411, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.9871, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.2059, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.7889, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.4990, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.2006, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.3693, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.8579, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.9463, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.5573, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.6
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1405],
        [ 0.3822],
        [-0.3480],
        [ 0.1574],
        [ 0.0583]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6221, 1.2228, 1.9018, 1.4740, 1.3824], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2500
Num nsteps_train %:  3.0

layer weights:  Parameter containing:
tensor([[-0.6885],
        [ 0.6364],
        [ 0.1387],
        [ 0.8070],
        [-0.0240]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.9356],
        [-0.6595],
        [ 0.1348],
        [ 0.7924],
        [ 0.2621]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5540, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.2965, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8265, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.5198, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4462, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.3665, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.6446, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.6573, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.6154, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4575, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.1791, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.9927, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.4881, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.7109, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.4687, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.9917, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.3408, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.3189, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.8065, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.4799, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1874],
        [ 0.3481],
        [-0.1692],
        [ 0.0956],
        [ 0.0112]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5540, 1.2965, 1.8265, 1.5198, 1.4462], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  2500
Num nsteps_train %:  4.0

layer weights:  Parameter containing:
tensor([[-0.3148],
        [-0.5249],
        [ 0.2356],
        [-0.6398],
        [-0.6343]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.7175],
        [ 0.7651],
        [-0.5202],
        [-0.7835],
        [ 0.9413]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5568, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.1349, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.3525, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.3793, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5202, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3556, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.1970, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.3090, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.3535, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.1732, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.1543, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.2591, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.2654, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.3277, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(-0.1739, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(-0.0470, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3212, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.2218, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.3018, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(-0.5209, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  0  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2013],
        [ 0.0621],
        [-0.0436],
        [-0.0258],
        [-0.3471]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5568, 0.1349, 0.3525, 0.3793, 0.5202], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  3000
Num nsteps_train %:  0.5

layer weights:  Parameter containing:
tensor([[ 0.1807],
        [ 0.0128],
        [-0.6068],
        [ 0.0249],
        [ 0.3439]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.3370],
        [ 0.3496],
        [-0.6255],
        [-0.0076],
        [-0.7756]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(3.6912, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.9032, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(4.0942, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(2.1668, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(2.0323, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(3.7280, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(2.8990, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(2.3636, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(2.6780, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(2.1072, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(3.7647, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(3.8948, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.6329, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(3.1893, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(2.1821, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(3.8015, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(4.8906, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(-1.0977, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(3.7005, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(2.2569, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0

Total Reward Received:  4.1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.0368],
        [ 0.9958],
        [-1.7306],
        [ 0.5112],
        [ 0.0749]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([3.6912, 1.9032, 4.0942, 2.1668, 2.0323], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  3000
Num nsteps_train %:  1.0

layer weights:  Parameter containing:
tensor([[-0.0482],
        [-0.7142],
        [-0.0351],
        [ 0.1160],
        [ 0.0477]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.7266],
        [-0.7732],
        [-0.2609],
        [-0.5880],
        [ 0.6232]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4373, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.7398, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.7306, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4238, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4184, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.6076, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.3003, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.3601, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.3734, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.2205, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.7779, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.8607, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.9897, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.3230, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0225, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.9482, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.4212, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.6193, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.2726, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.8246, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0

Total Reward Received:  4.1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[ 0.1703],
        [ 0.5605],
        [-0.3704],
        [-0.0504],
        [-0.1979]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4373, 0.7398, 1.7306, 1.4238, 1.4184], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  3000
Num nsteps_train %:  1.5

layer weights:  Parameter containing:
tensor([[ 0.9377],
        [-0.6786],
        [ 0.7374],
        [-0.6038],
        [ 0.7357]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.5343],
        [-0.6051],
        [-0.8338],
        [-0.3537],
        [ 0.2420]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(0.5417, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.3146, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(0.2487, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(0.1725, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(0.5481, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(0.3463, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(0.3187, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(0.2227, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(0.1789, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(0.2524, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.1508, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(0.3228, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.1968, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(0.1852, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(-0.0432, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(-0.0446, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(0.3269, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.1708, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(0.1916, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(-0.3388, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  4  Reward Received:  0.1

Total Reward Received:  0.5
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1954],
        [ 0.0041],
        [-0.0259],
        [ 0.0063],
        [-0.2956]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([0.5417, 0.3146, 0.2487, 0.1725, 0.5481], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  3000
Num nsteps_train %:  2.0

layer weights:  Parameter containing:
tensor([[ 0.6599],
        [ 0.0182],
        [ 0.0145],
        [ 0.6538],
        [-0.5614]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0831],
        [ 0.8923],
        [-0.5345],
        [ 0.4533],
        [-0.3002]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.6919, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.9084, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.9318, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3484, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3856, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.5605, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.3888, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.4261, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.4837, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4173, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.4291, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.8692, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.9203, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.6190, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.4490, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(1.2977, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.3496, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.4145, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.7543, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.4807, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  0  Reward Received:  0.1
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0

Total Reward Received:  4.1
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1314],
        [ 0.4804],
        [-0.5058],
        [ 0.1353],
        [ 0.0317]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.6919, 0.9084, 1.9318, 1.3484, 1.3856], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  3000
Num nsteps_train %:  2.5

layer weights:  Parameter containing:
tensor([[-0.2454],
        [ 0.6552],
        [ 0.5628],
        [ 0.5841],
        [ 0.9245]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[ 0.6128],
        [ 0.9098],
        [-0.4309],
        [ 0.2528],
        [-0.1242]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.4796, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.9944, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.6543, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.3467, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.3615, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.3024, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.3916, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.2852, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.3457, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.1987, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.1252, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.7888, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(0.9162, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.3447, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0359, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.9480, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.1861, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.5471, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.3437, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(0.8732, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2

Total Reward Received:  1.4
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1772],
        [ 0.3972],
        [-0.3691],
        [-0.0010],
        [-0.1628]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.4796, 0.9944, 1.6543, 1.3467, 1.3615], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  3000
Num nsteps_train %:  3.0

layer weights:  Parameter containing:
tensor([[-0.2780],
        [-0.5479],
        [ 0.5519],
        [ 0.7982],
        [-0.0179]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.0063],
        [-0.6507],
        [-0.1091],
        [ 0.1199],
        [-0.6855]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.5594, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(1.0575, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.8976, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.4598, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.4980, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.3551, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.5140, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.6144, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.5231, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.4511, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(1.1508, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.9705, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.3311, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.5863, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.4042, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.9465, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(2.4270, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(1.0479, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.6495, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.3573, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  2  Reward Received:  0

Total Reward Received:  4.0
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.2043],
        [ 0.4565],
        [-0.2832],
        [ 0.0632],
        [-0.0469]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.5594, 1.0575, 1.8976, 1.4598, 1.4980], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  3000
Num nsteps_train %:  3.5

layer weights:  Parameter containing:
tensor([[-0.5463],
        [-0.2186],
        [-0.0438],
        [-0.4259],
        [ 0.5145]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.1246],
        [-0.7508],
        [-0.5987],
        [ 0.3148],
        [ 0.1012]], dtype=torch.float64, requires_grad=True)
Summary AFTER training

==============================================
Summary: 
State	Action	Next State	Reward
0 		 0 		  Q(s,a)=  tensor(1.1518, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 1 		  Q(s,a)=  tensor(0.8966, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 2 		  Q(s,a)=  tensor(1.3504, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 3 		  Q(s,a)=  tensor(1.1966, dtype=torch.float64, grad_fn=<SelectBackward0>)
0 		 4 		  Q(s,a)=  tensor(1.0324, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 0 		  Q(s,a)=  tensor(1.0263, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 1 		  Q(s,a)=  tensor(1.1108, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 2 		  Q(s,a)=  tensor(1.1926, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 3 		  Q(s,a)=  tensor(1.2557, dtype=torch.float64, grad_fn=<SelectBackward0>)
1 		 4 		  Q(s,a)=  tensor(1.0534, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 0 		  Q(s,a)=  tensor(0.9008, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 1 		  Q(s,a)=  tensor(1.3249, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 2 		  Q(s,a)=  tensor(1.0349, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 3 		  Q(s,a)=  tensor(1.3147, dtype=torch.float64, grad_fn=<SelectBackward0>)
2 		 4 		  Q(s,a)=  tensor(1.0743, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 0 		  Q(s,a)=  tensor(0.7752, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 1 		  Q(s,a)=  tensor(1.5391, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 2 		  Q(s,a)=  tensor(0.8771, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 3 		  Q(s,a)=  tensor(1.3738, dtype=torch.float64, grad_fn=<SelectBackward0>)
3 		 4 		  Q(s,a)=  tensor(1.0953, dtype=torch.float64, grad_fn=<SelectBackward0>)
==============================================

Best trajectory: 
Best trajectory from Test Environment
State:  tensor([0.], dtype=torch.float64)  Action:  2  Reward Received:  0
State:  tensor([2.], dtype=torch.float64)  Action:  1  Reward Received:  2.0
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1
State:  tensor([3.], dtype=torch.float64)  Action:  1  Reward Received:  -0.2
State:  tensor([1.], dtype=torch.float64)  Action:  3  Reward Received:  -0.1

Total Reward Received:  1.6
Taking a look at model parameters to see if weights are changing
Parameter containing:
tensor([[-0.1255],
        [ 0.2141],
        [-0.1578],
        [ 0.0591],
        [ 0.0209]], dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor([1.1518, 0.8966, 1.3504, 1.1966, 1.0324], dtype=torch.float64,
       requires_grad=True)
Configs:
Num episodes:  3000
Num nsteps_train %:  4.0

layer weights:  Parameter containing:
tensor([[-0.8697],
        [ 0.6617],
        [-0.7940],
        [ 0.7697],
        [-0.5095]], dtype=torch.float64, requires_grad=True)
layer weights:  Parameter containing:
tensor([[-0.6938],
        [-0.7610],
        [-0.6983],
        [ 0.0382],
        [-0.8831]], dtype=torch.float64, requires_grad=True)
